{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "b4e51815-3605-4c9a-84e8-67af8eb88d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the necessary libraries\n",
    "import pandas as pd \n",
    "import numpy as  np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#import all the necessary libraries to build a neural network classifier from tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential                   \n",
    "from tensorflow.keras.layers import Dense                        #import fully connected neural net layers\n",
    "from tensorflow.keras.optimizers import Adam                     #choose adam as the optimization algorithm\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy      #cost function needed for softmax classification if the labels are \n",
    "                                                                 #one hot encoded\n",
    "\n",
    "from tensorflow.keras.regularizers import l2                     #add regularization in order to avoid overfitting \n",
    "\n",
    "from sklearn.model_selection import train_test_split             #to split the training set into train and test set \n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report  #to proceed with error analysis on our predictions\n",
    "\n",
    "\n",
    "import seaborn as sns                                                 #to visualize confusion matrices as a heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dropout              #import long-short term memory neural net layers\n",
    "                                                               #and dropout regularization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3fccc-5a2d-43ab-8684-aad3205344dc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "59a12369-b923-430e-b738-85f471c9c144",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  vectorize_string_csv_column_TF_idf(string_column):  \n",
    "                                                #create a function that receives a column of strings from a csv file \n",
    "                                                  #and converts each entry into a unique tfidf vector depending on the \n",
    "                                                  #unique vocabulary\n",
    "                                                   \n",
    "                                                 #necessary to convert a text input into a numeric vector to be used \n",
    "                                                 #as input to the classifier\n",
    "\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()  \n",
    "                                      #create a TF_idf_vectorizer model that will receive the entire csv column\n",
    "                                      #and will eventually turn each entry into a numeric vector\n",
    "                                      #the length of each vector will be the number of the unique words in the vocabulary\n",
    "                                      #if this length=N the resulting vectors for each entry will be of dimension (N,)\n",
    "    \n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(string_column)  \n",
    "                                                    #apply the created model to the column title of the csv\n",
    "\n",
    "\n",
    "    tfidf_vectors = tfidf_matrix.toarray()          #each entry of the tfidf_vectors is a numeric vector\n",
    "                                                    #corresponding to a title entry\n",
    "                                                    #convert it to an array format for ease of handling\n",
    "\n",
    "    Vocabulary=tfidf_vectorizer.get_feature_names_out()  #export the unique vocabulary out of the created vectorizer model\n",
    "\n",
    "    return tfidf_vectors, Vocabulary   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b84b5f-6a74-4941-8ca1-4cf1d2527555",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0dc21fb0-aada-457a-873f-ed74b965cac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that finds all the unique labels in a column \n",
    "def find_unique_column_labels(column):\n",
    "     \n",
    "    unique_labels_list=[]               #create a list that will contain all the unique labels found in the input column\n",
    "\n",
    "    for i in column : #search the entire column\n",
    "        if i not in unique_labels_list:   #if the element i is not found in the unique list \n",
    "            unique_labels_list.append(i)  #append it to the list\n",
    "    \n",
    "    return  unique_labels_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33171087-9178-441a-8c52-83e23f6d5f79",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "95257d90-e92c-4246-a32a-1f57781112a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_labels(unique_labels_of_a_column):\n",
    "    #create a dictionary for these categories that coresponds each one into a one hot numpy vector\n",
    "    #this is necessary in order to use a softmax classifier \n",
    "\n",
    "    number_of_classes=len(unique_labels_of_a_column)  #find the number of classes/possible labels from the vector that contains the unique labels\n",
    "\n",
    "                                               #create a numpy I matrix I lxl where l is the number of classes \n",
    "                                               #each row of the I matrix will correspond to a one hot encoding for each label\n",
    "    I=np.eye(number_of_classes)\n",
    "\n",
    "    #create a dictionary to correspond each class name with it's one hot encoded label\n",
    "\n",
    "    dict_labels={} #initialize an empty dictionary where the keys will be the labels and the values will be their one hot encoding\n",
    "\n",
    "\n",
    "    for i in range(number_of_classes):\n",
    "        dict_labels[unique_labels_of_a_column[i]]=I[i,:]\n",
    "\n",
    "    return dict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8246df-f347-45f0-b092-4d60a0dd47a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "7cc4cf3c-fbb7-4fbb-acc7-ed0f31a1c77b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Y_label(initial_label_column,dict_labels):\n",
    "    #create the Y part of the dataset by receiving  a column  and the dict_labels corresponding to that column\n",
    "\n",
    "\n",
    "\n",
    "    #len(dict_labels['biological'])\n",
    "\n",
    "    Y1={}   #initialize an empty dictionary\n",
    "    count=0 #and a count variable\n",
    "    for i in initial_label_column:    #search through the hazard_category column of the data frame\n",
    "        for j in dict_labels.keys():  #and through all the keys of the labels dictionary with keys all the unique labels \n",
    "                                  #and values their one hot encoded representation\n",
    "            if i==j:                   #if you find a match\n",
    "                Y1[count]=dict_labels[j]   #assign the category with it's one hot encoding\n",
    "                count+=1\n",
    "    #Y\n",
    "    #now the dictionary above should be turned into a numpy matrix with its elements being column vectors\n",
    "\n",
    "    # Convert dictionary values to a numpy matrix\n",
    "    matrix = np.array([v for v in Y1.values()]).T\n",
    "\n",
    "    matrix.shape #Nxm format\n",
    "    return matrix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab36b65-9c25-4f68-a179-a8e1c7a2602f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e7ca31c2-63b1-4080-9e7a-70d977b96eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_dense_nn(X_train,Y_train,X_test,Y_test,hu_1,hu_2,number_of_classes,a_epochs,a_batch_size):\n",
    "    #define a function that trains a fully connected dense neural net to fit the pre-processed data\n",
    "    #X_train are the training data\n",
    "    #Y_train are the training labels \n",
    "    #X_test,Y_test are the data we are going to use for validation\n",
    "    #hu_1 is the number of hidden units in layer 1\n",
    "    #hu_2 is the number of hidden units in layer 2\n",
    "    #number of classes is self explanatory and it is required for the softmax classification layer\n",
    "    #a_epochs is the number of training epochs \n",
    "    #and a_batch size is the number of batches that we use to split the training data\n",
    "    #the activation function is chosen as the relu activation f(z)=max(z,0)\n",
    "    #and the regularization is set to zero after manual tuning\n",
    "    \n",
    "    model = Sequential([                                          #model architecture    \n",
    "    Dense(hu_1, activation='relu', input_shape=(X_train.shape[1],),kernel_regularizer=l2(0)),  \n",
    "    Dense(hu_2, activation='relu',kernel_regularizer=l2(0)),                      \n",
    "    Dense(number_of_classes, activation='softmax',kernel_regularizer=l2(0))         \n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),           #for training the model the Adam optimization algorithm has been chosen \n",
    "                                                                  #with initial learning rate=0.001\n",
    "        loss=CategoricalCrossentropy(),                            #since the output labels are one-hot encoded we have chosen the \n",
    "                                                                 #Categorical cross entropy function as a loss function\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    model.fit(     #the method fit is used to actually train the model using the X_train and Y_train \n",
    "    X_train, Y_train,    #as training data and X_test,Y_test as validation data \n",
    "    validation_data=(X_test, Y_test),\n",
    "    epochs=a_epochs,          #the number of epochs chosen for the training are 30\n",
    "    batch_size=a_batch_size,     #the training itself is mini batch gradient-descent with adam and the training data -set is split into \n",
    "    verbose=1          #32 batches    \n",
    "    )\n",
    "\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, Y_test, verbose=1)   #we now evaluate the models performance on the\n",
    "                                                                       #X_test and Y_test data for which we now the labels \n",
    "                                                                       #but these labels are unkown to the model \n",
    "                                                                       #using only the input data X_test it predicts the output \n",
    "                                                                       #labels and then compares them with the actual Y_test labels\n",
    "\n",
    "    print(\"Test Loss:\", test_loss)                           #depict the final value of the cost function for the test set                                     \n",
    "    print(\"Test Accuracy:\", test_accuracy)                  #depict the accuracy of predictions in the test set\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81a219e-789b-44ef-96b6-057aff6905c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "426585ab-d9f9-42ff-a0e2-5f9562e95a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap_and_error_report(model,X_test,Y_test,num_of_classes):  #input the X_test \n",
    "                                                                    #and Y_test\n",
    "                                                                    #make predictions on X_test with the model\n",
    "                                                                    #compare the model predictions with \n",
    "                                                                    #the actual Y_test and create an error analysis\n",
    "\n",
    "    \n",
    "    Y_predict_initial=model.predict(X_test) #we predict the model output for the X_test \n",
    "        #and we are going to compare with the actual lables from Y_test\n",
    "\n",
    "    #Each entry y in Y_predict_initial is a vector of  outputs= number of classes, containing the probabilities that show \n",
    "    #how likely it is for the model to assign an entry x of x_test to a specific class.\n",
    "    #For the entry x with prediction y if y[0] is the highest amongst the elements of y, x will be assigned to \n",
    "    #class 0. If y[1] is the highest then x will be assigned to class_1 and so on\n",
    "\n",
    "    #As a result, we need to find the index of  maximum element of each y in Y_predict \n",
    "    #that will show us in which class x corresponds to:\n",
    "    Y_intermediate = np.argmax(Y_predict_initial, axis=1) #find the index of the maximum element for each y in Y_predict_initial\n",
    "    \n",
    "    #we are also going to convert Y_test from a one hot encoding to \n",
    "    #the number of class this one hot encoding represents and compare it with \n",
    "    #the predicted class stored in Y_intermediate\n",
    "\n",
    "    Y_true = np.argmax(Y_test, axis=1)  #due to the fact that we have a one hot encoding, finding the index of the max \n",
    "                                    #element will lead directly to the number of class it represents\n",
    "\n",
    "    #\n",
    "    #we are going to find out where Y_true matches our prediction in Y_intermediate \n",
    "    #and we are going to display a confusion matrix of the true vs the prediction\n",
    "\n",
    "    \n",
    "    conf_mat=confusion_matrix(Y_true,Y_intermediate)\n",
    "    \n",
    "\n",
    "   # sns.heatmap(conf_mat,annot=True, fmt='d', cmap='Blues')\n",
    "   # plt.xlabel('Predicted Class')\n",
    "   # plt.ylabel('True Class')\n",
    "   # plt.title('Confusion Matrix')\n",
    "   # plt.show()\n",
    "\n",
    "    print(conf_mat.shape)\n",
    "    report = classification_report(Y_true, Y_intermediate, target_names=[f\"Class {i}\" for i in range(conf_mat.shape[0])]) #i in range number of classes\n",
    "    print(\"\\n\",report)\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4ccfa0-fd03-4213-8f15-925951588d7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "8ab2069a-7062-4fb4-b18c-88507be57de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a function that inputs data X\n",
    "#the model that makes predictions and the \n",
    "#dictionary with the labels and their one hot encoding. \n",
    "#It computes the numeric predictions for X and turns them into the text of the label \n",
    "#they correspond to\n",
    "\n",
    "def from_pred_and_dictionary_to_labels(X,model,one_hot_dictionary):\n",
    "    \n",
    "        #predicted output\n",
    "    Y_pr=model.predict(X)\n",
    "\n",
    "        #create a new dictionary with the key being the index/label of the one_hot_dictionary\n",
    "        #and the value of this new  dictionary being the phrase of the one hot encoding \n",
    "\n",
    "    new_dict={}\n",
    "\n",
    "    for key,value in one_hot_dictionary.items():   \n",
    "        convert_one_hot_encode_to_number=np.argmax(value)    #get the number representation of the one hot encoding\n",
    "        label=key                                            #get the phrase of the one hot encoding \n",
    "        new_dict[convert_one_hot_encode_to_number]=label     #store them as number-> phrase\n",
    "\n",
    "    #use the newly created dictionary to map the predictions into the labels\n",
    "    predictions_text_format=[]\n",
    "\n",
    "    for i in Y_pr:\n",
    "        chosen_label=np.argmax(i) #loop through the predictions of the model and choose to which label the prediction is assigned\n",
    "        predictions_text_format.append(new_dict[chosen_label]) #get the phrase corresponding to that label and append it to a list\n",
    "        \n",
    "    return predictions_text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df3ea60-789f-423b-919a-be56fe8e6647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "755d30f8-fa74-49a4-8259-346b8635dbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv incidents_train into a data frame\n",
    "\n",
    "df=pd.read_csv('incidents_train.csv')\n",
    "#df.head()\n",
    "\n",
    "#isolate the title category which is to be used as the input X to a classifier\n",
    "title=df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66a223fe-fd47-4a41-9335-18750ce8fa64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "48802827-a696-47f4-a17f-0388c76bebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,unique_title_voc=vectorize_string_csv_column_TF_idf(title)\n",
    "#X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172b3eea-dd1f-44c4-b335-896990831578",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "3b6beee6-d42a-44e8-93fa-07766078b4b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5082, 128)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hazard=df['hazard']  #get the hazard category for the data frame refering to the training set\n",
    "unique_hazard_labels=find_unique_column_labels(hazard) #find the unique labels of product category\n",
    "\n",
    "\n",
    "one_hot_encode_hazard=one_hot_encode_labels(unique_hazard_labels)#one hot encode these labels and get a \n",
    "#dictionary with the key being the label and the value being its one hot encoding\n",
    "#one_hot_encode_hazard_category\n",
    "\n",
    "Y_hazard_transposed=create_Y_label(hazard,one_hot_encode_hazard)\n",
    "#create the Y part of the data-set but in a transposed format\n",
    "\n",
    "Y_hazard=Y_hazard_transposed.T\n",
    "Y_hazard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb4c4a7-23c6-4951-94e0-fe4745195bb3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "94d92158-051c-4e2e-a71c-8244e31da82f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data intro train and test set\n",
    "#using the processed title data as X\n",
    "#and hazard category as Y\n",
    "X_train, X_test, Y_hazard_train, Y_hazard_test = train_test_split(X, Y_hazard, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "43c86345-e75b-4946-b6e0-7f9d8c142177",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.1777 - loss: 4.1353 - val_accuracy: 0.3746 - val_loss: 2.8878\n",
      "Epoch 2/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.4294 - loss: 2.4668 - val_accuracy: 0.4789 - val_loss: 2.3555\n",
      "Epoch 3/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.6345 - loss: 1.6112 - val_accuracy: 0.5506 - val_loss: 2.1160\n",
      "Epoch 4/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.7685 - loss: 1.0957 - val_accuracy: 0.5624 - val_loss: 2.0551\n",
      "Epoch 5/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.8424 - loss: 0.7576 - val_accuracy: 0.5811 - val_loss: 2.0348\n",
      "Epoch 6/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9109 - loss: 0.4782 - val_accuracy: 0.5801 - val_loss: 2.0359\n",
      "Epoch 7/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 23ms/step - accuracy: 0.9489 - loss: 0.2832 - val_accuracy: 0.5890 - val_loss: 2.0953\n",
      "Epoch 8/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 22ms/step - accuracy: 0.9754 - loss: 0.1752 - val_accuracy: 0.5919 - val_loss: 2.1660\n",
      "Epoch 9/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9896 - loss: 0.0965 - val_accuracy: 0.5890 - val_loss: 2.2281\n",
      "Epoch 10/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9940 - loss: 0.0641 - val_accuracy: 0.5949 - val_loss: 2.2799\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6090 - loss: 2.2144\n",
      "Test Loss: 2.279886245727539\n",
      "Test Accuracy: 0.5948868989944458\n"
     ]
    }
   ],
   "source": [
    "#128 is the number of unique classes in hazard\n",
    "model=train_and_eval_dense_nn(X_train,Y_hazard_train,X_test,Y_hazard_test,256,120,128,10,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "7f8ceb9c-8823-4fb2-b501-ba3cf15cfd69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
      "(111, 111)\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.77      0.83      0.80       139\n",
      "     Class 1       0.00      0.00      0.00         2\n",
      "     Class 2       0.48      0.64      0.55        39\n",
      "     Class 3       0.81      0.81      0.81        47\n",
      "     Class 4       0.00      0.00      0.00         1\n",
      "     Class 5       0.67      0.86      0.75       121\n",
      "     Class 6       0.11      0.25      0.15         4\n",
      "     Class 7       0.00      0.00      0.00         3\n",
      "     Class 8       0.00      0.00      0.00         4\n",
      "     Class 9       0.28      0.32      0.30        34\n",
      "    Class 10       0.57      0.64      0.61        36\n",
      "    Class 11       0.55      0.78      0.64       108\n",
      "    Class 12       0.69      0.43      0.53        21\n",
      "    Class 13       0.60      0.75      0.67         4\n",
      "    Class 14       1.00      0.33      0.50         6\n",
      "    Class 15       0.77      0.51      0.61        45\n",
      "    Class 16       0.00      0.00      0.00         2\n",
      "    Class 17       0.40      0.50      0.44         4\n",
      "    Class 18       0.00      0.00      0.00         0\n",
      "    Class 19       0.67      0.57      0.62        42\n",
      "    Class 20       0.00      0.00      0.00         1\n",
      "    Class 21       0.55      0.43      0.48        14\n",
      "    Class 22       1.00      0.50      0.67         4\n",
      "    Class 23       0.60      0.38      0.46         8\n",
      "    Class 24       0.00      0.00      0.00         5\n",
      "    Class 25       1.00      0.33      0.50         3\n",
      "    Class 26       0.00      0.00      0.00         0\n",
      "    Class 27       0.00      0.00      0.00         2\n",
      "    Class 28       0.20      0.25      0.22         4\n",
      "    Class 29       1.00      1.00      1.00         1\n",
      "    Class 30       0.00      0.00      0.00         1\n",
      "    Class 31       0.53      0.47      0.49        43\n",
      "    Class 32       0.37      0.30      0.33        23\n",
      "    Class 33       0.00      0.00      0.00         3\n",
      "    Class 34       0.10      0.20      0.13         5\n",
      "    Class 35       1.00      0.50      0.67         2\n",
      "    Class 36       1.00      0.67      0.80         6\n",
      "    Class 37       0.40      0.20      0.27        10\n",
      "    Class 38       0.75      0.67      0.71        18\n",
      "    Class 39       0.00      0.00      0.00         1\n",
      "    Class 40       0.33      0.25      0.29         8\n",
      "    Class 41       1.00      0.33      0.50         3\n",
      "    Class 42       0.60      0.50      0.55         6\n",
      "    Class 43       1.00      0.43      0.60         7\n",
      "    Class 44       0.00      0.00      0.00         0\n",
      "    Class 45       0.50      0.33      0.40         3\n",
      "    Class 46       0.00      0.00      0.00         2\n",
      "    Class 47       0.00      0.00      0.00         1\n",
      "    Class 48       0.00      0.00      0.00         2\n",
      "    Class 49       0.50      1.00      0.67         1\n",
      "    Class 50       1.00      0.50      0.67         2\n",
      "    Class 51       0.00      0.00      0.00         2\n",
      "    Class 52       0.00      0.00      0.00         4\n",
      "    Class 53       0.50      0.17      0.25         6\n",
      "    Class 54       1.00      1.00      1.00         1\n",
      "    Class 55       0.00      0.00      0.00         1\n",
      "    Class 56       0.50      0.40      0.44         5\n",
      "    Class 57       0.08      0.33      0.13         3\n",
      "    Class 58       0.00      0.00      0.00         1\n",
      "    Class 59       0.22      0.20      0.21        10\n",
      "    Class 60       0.00      0.00      0.00         1\n",
      "    Class 61       0.83      0.71      0.77         7\n",
      "    Class 62       0.83      0.94      0.88        16\n",
      "    Class 63       0.14      0.50      0.22         2\n",
      "    Class 64       0.00      0.00      0.00         1\n",
      "    Class 65       0.00      0.00      0.00         3\n",
      "    Class 66       0.00      0.00      0.00         1\n",
      "    Class 67       0.00      0.00      0.00         2\n",
      "    Class 68       1.00      1.00      1.00         1\n",
      "    Class 69       0.00      0.00      0.00         3\n",
      "    Class 70       1.00      1.00      1.00         2\n",
      "    Class 71       0.75      0.67      0.71         9\n",
      "    Class 72       0.00      0.00      0.00         1\n",
      "    Class 73       0.00      0.00      0.00         1\n",
      "    Class 74       0.50      0.38      0.43         8\n",
      "    Class 75       0.00      0.00      0.00         4\n",
      "    Class 76       0.00      0.00      0.00         1\n",
      "    Class 77       0.00      0.00      0.00         1\n",
      "    Class 78       0.00      0.00      0.00         3\n",
      "    Class 79       0.00      0.00      0.00         1\n",
      "    Class 80       0.00      0.00      0.00         4\n",
      "    Class 81       0.50      1.00      0.67         1\n",
      "    Class 82       1.00      0.67      0.80         3\n",
      "    Class 83       0.00      0.00      0.00         2\n",
      "    Class 84       0.00      0.00      0.00         2\n",
      "    Class 85       0.00      0.00      0.00         3\n",
      "    Class 86       0.00      0.00      0.00         2\n",
      "    Class 87       0.00      0.00      0.00         1\n",
      "    Class 88       0.00      0.00      0.00         2\n",
      "    Class 89       0.00      0.00      0.00         1\n",
      "    Class 90       0.00      0.00      0.00         2\n",
      "    Class 91       1.00      0.60      0.75         5\n",
      "    Class 92       0.00      0.00      0.00         2\n",
      "    Class 93       0.00      0.00      0.00         1\n",
      "    Class 94       0.25      1.00      0.40         1\n",
      "    Class 95       1.00      1.00      1.00         1\n",
      "    Class 96       0.00      0.00      0.00         1\n",
      "    Class 97       0.00      0.00      0.00         1\n",
      "    Class 98       0.00      0.00      0.00         1\n",
      "    Class 99       1.00      1.00      1.00         2\n",
      "   Class 100       0.80      0.80      0.80         5\n",
      "   Class 101       1.00      1.00      1.00         1\n",
      "   Class 102       0.00      0.00      0.00         1\n",
      "   Class 103       1.00      1.00      1.00         9\n",
      "   Class 104       0.00      0.00      0.00         1\n",
      "   Class 105       0.00      0.00      0.00         3\n",
      "   Class 106       0.00      0.00      0.00         2\n",
      "   Class 107       1.00      1.00      1.00         1\n",
      "   Class 108       0.00      0.00      0.00         1\n",
      "   Class 109       1.00      1.00      1.00         1\n",
      "   Class 110       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.59      1017\n",
      "   macro avg       0.36      0.32      0.32      1017\n",
      "weighted avg       0.58      0.59      0.58      1017\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the heatmap and error report is on the artificial tets set , as a result \n",
    "#all the number of classes in hazard might not be included in the test set\n",
    "#so the report for the f1 score includes only the classes that are found in the artificial test set\n",
    "create_heatmap_and_error_report(model,X_test,Y_hazard_test,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "4f0f8891-95b6-44f8-af8e-f451333f4234",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the validation_file for hazard category and create a prediction vector\n",
    "\n",
    "#load the csv incidents_val into a data frame\n",
    "\n",
    "df_validation=pd.read_csv('incidents_val.csv')\n",
    "#df.head()\n",
    "\n",
    "#isolate the title category which is to be used as the input X to a classifier\n",
    "title_validation=df_validation['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "353197c2-5c31-4353-963b-46ce3faddca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565, 7372)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#follow the same steps as before to vectorize the title_validation \n",
    "#into a form that can be loaded into the trained model\n",
    "\n",
    "#to avoid having different number of features for the train set and the validation set with the tfidf vectorizer we are \n",
    "#going to use the tfidf vectorizer fitted for the train set to the validation set \n",
    "#the function we created previously will not suffise because it will create a unique representation based on the validation set \n",
    "#with number of features different than the ones in the training set\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()  #create a tfidfvectorizer object\n",
    "\n",
    "\n",
    "tfidf_vectorizer.fit(title)        #fit it for the title of the training set \n",
    "                                                   \n",
    "X_center=tfidf_vectorizer.transform(title)      #transform the title of the training set into a format where each entry is a \n",
    "                                                #numeric vector of shape (m,n) where m \n",
    "                                                #is the number of examples in the given set and n is the number of features\n",
    "\n",
    "X_center_format=X_center.toarray()             #follow this conversion to get an array form\n",
    "                                               #\n",
    "\n",
    "\n",
    "X_val_0=tfidf_vectorizer.transform(title_validation)   #use the vectorizer that is fitted to the training set to transform the input \n",
    "                                                       #from the validation set. the result will be a numeric vector of shape (m_val,n)\n",
    "                                                       #where m_val is the number of entries in the validation set and \n",
    "                                                       #n is the number of features which is the same as the number of features \n",
    "                                                       # used for training\n",
    "\n",
    "X_val_1=X_val_0.toarray()\n",
    "\n",
    "X_val_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "0833fb3d-08f7-41a9-b535-9991265e6dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n"
     ]
    }
   ],
   "source": [
    "#predictions in text format from the trained dense model named model\n",
    "\n",
    "H_val_prediction=from_pred_and_dictionary_to_labels(X_val_1,model,one_hot_encode_hazard)  #use the aforementioned function\n",
    "                                                       #to predict the Hazard  labels for the validation set using the \n",
    "                                                       #validation title. This function also receives as input the dictionary with \n",
    "                                                       #the labels and their one hot encoding in order to convert the predictions back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a31a53ef-40ef-4756-a571-06db9de6c903",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "f73b686c-550d-44b0-9a77-e7d3b9fa4d49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are going to follow the exact same procedure and train a seperate model to predict \n",
    "#the product  first for the training set and then for the validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "a0f31549-65fc-49c1-8498-3dec90c62b4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5082, 1022)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product=df['product'] #get the product category for the data frame refering to the training set\n",
    "unique_product_labels=find_unique_column_labels(product) #find the unique labels of product category\n",
    "\n",
    "one_hot_encode_product=one_hot_encode_labels(unique_product_labels) #one hot encode these labels and get a \n",
    "#dictionary with the key being the label and the value being its one hot encoding\n",
    "\n",
    "\n",
    "Y_product_transposed=create_Y_label(product,one_hot_encode_product)\n",
    "#create the Y part of the data set\n",
    "\n",
    "Y_product=Y_product_transposed.T\n",
    "Y_product.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a95d88-e81e-4f86-93b6-6e62df72e8f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "a77f7fe4-9bb8-4cf9-a313-491445e9cc54",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data intro train and test set\n",
    "#using the processed title data as X\n",
    "#and product category as Y\n",
    "X_train, X_test, Y_product_train, Y_product_test = train_test_split(X, Y_product, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edda851c-f397-454d-9846-330f010954bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "64676df7-6160-4c1a-9f70-76fbaba2a463",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.0344 - loss: 6.6708 - val_accuracy: 0.0659 - val_loss: 6.0547\n",
      "Epoch 2/15\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.0588 - loss: 5.6696 - val_accuracy: 0.0846 - val_loss: 5.8744\n",
      "Epoch 3/15\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.1015 - loss: 5.1193 - val_accuracy: 0.1504 - val_loss: 5.7652\n",
      "Epoch 4/15\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.2377 - loss: 4.3486 - val_accuracy: 0.2409 - val_loss: 5.6202\n",
      "Epoch 5/15\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.3920 - loss: 3.5048 - val_accuracy: 0.2822 - val_loss: 5.6845\n",
      "Epoch 6/15\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.5528 - loss: 2.6193 - val_accuracy: 0.3215 - val_loss: 5.9198\n",
      "Epoch 7/15\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.6689 - loss: 1.9968 - val_accuracy: 0.3382 - val_loss: 6.3004\n",
      "Epoch 8/15\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.7646 - loss: 1.4412 - val_accuracy: 0.3363 - val_loss: 6.7710\n",
      "Epoch 9/15\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.8341 - loss: 0.9796 - val_accuracy: 0.3510 - val_loss: 7.2533\n",
      "Epoch 10/15\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.8765 - loss: 0.6702 - val_accuracy: 0.3540 - val_loss: 7.7160\n",
      "Epoch 11/15\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.9249 - loss: 0.4333 - val_accuracy: 0.3422 - val_loss: 8.1781\n",
      "Epoch 12/15\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9637 - loss: 0.2688 - val_accuracy: 0.3481 - val_loss: 8.8601\n",
      "Epoch 13/15\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9784 - loss: 0.1593 - val_accuracy: 0.3432 - val_loss: 9.2144\n",
      "Epoch 14/15\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9841 - loss: 0.0917 - val_accuracy: 0.3451 - val_loss: 9.7910\n",
      "Epoch 15/15\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.9857 - loss: 0.0628 - val_accuracy: 0.3491 - val_loss: 9.7590\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.3574 - loss: 9.4924\n",
      "Test Loss: 9.759013175964355\n",
      "Test Accuracy: 0.3490658700466156\n"
     ]
    }
   ],
   "source": [
    "#train a different dense net than before in order to predict the product-category column of the data frame\n",
    "#22 is the number of unique categories for product-category\n",
    "model_2=train_and_eval_dense_nn(X_train,Y_product_train,X_test,Y_product_test,256,128,Y_product_train.shape[1],15,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "e92eeb7f-1bed-4e8c-b34b-3355faebc954",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1017, 1022)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_product_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "1cac06d1-66a7-40e5-9128-7f6ed099c545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "(538, 538)\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.00      0.00      0.00         4\n",
      "     Class 1       0.50      0.29      0.36         7\n",
      "     Class 2       0.20      0.43      0.27         7\n",
      "     Class 3       0.20      1.00      0.33         1\n",
      "     Class 4       0.47      0.78      0.58         9\n",
      "     Class 5       0.00      0.00      0.00         7\n",
      "     Class 6       0.00      0.00      0.00         1\n",
      "     Class 7       0.00      0.00      0.00         1\n",
      "     Class 8       0.28      0.48      0.35        23\n",
      "     Class 9       0.00      0.00      0.00         1\n",
      "    Class 10       0.33      0.25      0.29         4\n",
      "    Class 11       0.14      0.33      0.20         3\n",
      "    Class 12       0.25      0.50      0.33         2\n",
      "    Class 13       0.50      0.50      0.50         2\n",
      "    Class 14       0.38      0.38      0.38         8\n",
      "    Class 15       1.00      0.33      0.50         3\n",
      "    Class 16       0.55      0.65      0.59        17\n",
      "    Class 17       0.25      0.25      0.25         8\n",
      "    Class 18       0.00      0.00      0.00         1\n",
      "    Class 19       0.17      0.11      0.13         9\n",
      "    Class 20       0.91      0.80      0.85        50\n",
      "    Class 21       1.00      0.50      0.67         2\n",
      "    Class 22       0.40      0.33      0.36         6\n",
      "    Class 23       0.00      0.00      0.00         1\n",
      "    Class 24       1.00      0.67      0.80         3\n",
      "    Class 25       1.00      1.00      1.00         2\n",
      "    Class 26       0.50      0.69      0.58        13\n",
      "    Class 27       0.29      0.50      0.37        10\n",
      "    Class 28       0.00      0.00      0.00         0\n",
      "    Class 29       0.00      0.00      0.00         2\n",
      "    Class 30       0.00      0.00      0.00         1\n",
      "    Class 31       0.00      0.00      0.00         1\n",
      "    Class 32       0.00      0.00      0.00         2\n",
      "    Class 33       1.00      0.20      0.33         5\n",
      "    Class 34       0.57      0.80      0.67         5\n",
      "    Class 35       0.35      0.82      0.49        11\n",
      "    Class 36       0.00      0.00      0.00         1\n",
      "    Class 37       0.00      0.00      0.00         1\n",
      "    Class 38       0.67      0.57      0.62         7\n",
      "    Class 39       0.50      1.00      0.67         1\n",
      "    Class 40       0.50      0.33      0.40         3\n",
      "    Class 41       0.00      0.00      0.00         2\n",
      "    Class 42       0.06      0.14      0.09        14\n",
      "    Class 43       0.33      0.20      0.25         5\n",
      "    Class 44       0.50      0.20      0.29         5\n",
      "    Class 45       0.00      0.00      0.00         2\n",
      "    Class 46       0.60      0.60      0.60         5\n",
      "    Class 47       1.00      0.40      0.57         5\n",
      "    Class 48       0.00      0.00      0.00         8\n",
      "    Class 49       1.00      1.00      1.00         2\n",
      "    Class 50       0.00      0.00      0.00         2\n",
      "    Class 51       1.00      0.62      0.77         8\n",
      "    Class 52       0.00      0.00      0.00         1\n",
      "    Class 53       1.00      1.00      1.00         1\n",
      "    Class 54       1.00      1.00      1.00         1\n",
      "    Class 55       0.00      0.00      0.00         1\n",
      "    Class 56       0.50      0.17      0.25         6\n",
      "    Class 57       0.00      0.00      0.00         4\n",
      "    Class 58       0.00      0.00      0.00         3\n",
      "    Class 59       0.00      0.00      0.00         1\n",
      "    Class 60       0.62      0.62      0.62         8\n",
      "    Class 61       0.33      1.00      0.50         1\n",
      "    Class 62       0.00      0.00      0.00         0\n",
      "    Class 63       0.67      0.50      0.57         4\n",
      "    Class 64       0.83      0.71      0.77         7\n",
      "    Class 65       0.27      0.50      0.35         6\n",
      "    Class 66       1.00      1.00      1.00         1\n",
      "    Class 67       0.17      1.00      0.29         1\n",
      "    Class 68       0.00      0.00      0.00         2\n",
      "    Class 69       1.00      1.00      1.00         1\n",
      "    Class 70       0.00      0.00      0.00         2\n",
      "    Class 71       0.00      0.00      0.00         2\n",
      "    Class 72       0.50      0.20      0.29         5\n",
      "    Class 73       1.00      1.00      1.00         1\n",
      "    Class 74       0.00      0.00      0.00         1\n",
      "    Class 75       0.00      0.00      0.00         2\n",
      "    Class 76       0.00      0.00      0.00         1\n",
      "    Class 77       0.00      0.00      0.00         0\n",
      "    Class 78       0.00      0.00      0.00         2\n",
      "    Class 79       1.00      0.50      0.67         4\n",
      "    Class 80       0.33      1.00      0.50         1\n",
      "    Class 81       1.00      1.00      1.00         1\n",
      "    Class 82       0.89      0.89      0.89         9\n",
      "    Class 83       0.00      0.00      0.00         1\n",
      "    Class 84       0.33      0.67      0.44         3\n",
      "    Class 85       1.00      0.50      0.67         2\n",
      "    Class 86       0.00      0.00      0.00         1\n",
      "    Class 87       0.00      0.00      0.00         1\n",
      "    Class 88       0.14      0.50      0.22         2\n",
      "    Class 89       1.00      1.00      1.00         1\n",
      "    Class 90       0.50      0.25      0.33         4\n",
      "    Class 91       0.00      0.00      0.00         1\n",
      "    Class 92       0.00      0.00      0.00         1\n",
      "    Class 93       0.00      0.00      0.00         0\n",
      "    Class 94       0.00      0.00      0.00         1\n",
      "    Class 95       1.00      0.33      0.50         3\n",
      "    Class 96       0.18      0.42      0.25        12\n",
      "    Class 97       0.00      0.00      0.00         0\n",
      "    Class 98       0.00      0.00      0.00         1\n",
      "    Class 99       0.00      0.00      0.00         0\n",
      "   Class 100       0.00      0.00      0.00         1\n",
      "   Class 101       1.00      1.00      1.00         1\n",
      "   Class 102       1.00      1.00      1.00         3\n",
      "   Class 103       0.00      0.00      0.00         1\n",
      "   Class 104       1.00      0.71      0.83         7\n",
      "   Class 105       0.00      0.00      0.00         2\n",
      "   Class 106       0.00      0.00      0.00         3\n",
      "   Class 107       0.75      0.75      0.75         4\n",
      "   Class 108       0.00      0.00      0.00         2\n",
      "   Class 109       0.00      0.00      0.00         3\n",
      "   Class 110       1.00      1.00      1.00         1\n",
      "   Class 111       1.00      0.40      0.57         5\n",
      "   Class 112       0.00      0.00      0.00         1\n",
      "   Class 113       0.67      0.40      0.50         5\n",
      "   Class 114       0.00      0.00      0.00         1\n",
      "   Class 115       0.50      0.50      0.50         2\n",
      "   Class 116       0.00      0.00      0.00         0\n",
      "   Class 117       0.00      0.00      0.00         0\n",
      "   Class 118       0.00      0.00      0.00         1\n",
      "   Class 119       0.00      0.00      0.00         0\n",
      "   Class 120       0.00      0.00      0.00         2\n",
      "   Class 121       1.00      1.00      1.00         1\n",
      "   Class 122       0.00      0.00      0.00         0\n",
      "   Class 123       0.00      0.00      0.00         2\n",
      "   Class 124       0.50      0.29      0.36         7\n",
      "   Class 125       0.00      0.00      0.00         1\n",
      "   Class 126       0.25      0.50      0.33         6\n",
      "   Class 127       0.00      0.00      0.00         0\n",
      "   Class 128       0.00      0.00      0.00         2\n",
      "   Class 129       0.00      0.00      0.00         0\n",
      "   Class 130       0.00      0.00      0.00         1\n",
      "   Class 131       0.00      0.00      0.00         1\n",
      "   Class 132       0.00      0.00      0.00         0\n",
      "   Class 133       0.33      0.50      0.40         2\n",
      "   Class 134       0.50      0.67      0.57         3\n",
      "   Class 135       0.60      0.43      0.50         7\n",
      "   Class 136       0.00      0.00      0.00         1\n",
      "   Class 137       0.00      0.00      0.00         0\n",
      "   Class 138       0.75      0.86      0.80         7\n",
      "   Class 139       0.40      1.00      0.57         2\n",
      "   Class 140       0.20      0.50      0.29         2\n",
      "   Class 141       0.50      0.56      0.53         9\n",
      "   Class 142       1.00      0.50      0.67         2\n",
      "   Class 143       0.00      0.00      0.00         1\n",
      "   Class 144       0.00      0.00      0.00         3\n",
      "   Class 145       0.00      0.00      0.00         0\n",
      "   Class 146       0.50      0.25      0.33         4\n",
      "   Class 147       0.00      0.00      0.00         2\n",
      "   Class 148       0.00      0.00      0.00         1\n",
      "   Class 149       0.50      1.00      0.67         1\n",
      "   Class 150       0.00      0.00      0.00         1\n",
      "   Class 151       0.00      0.00      0.00         6\n",
      "   Class 152       0.00      0.00      0.00         1\n",
      "   Class 153       0.10      0.25      0.14         4\n",
      "   Class 154       0.00      0.00      0.00         1\n",
      "   Class 155       0.00      0.00      0.00         0\n",
      "   Class 156       1.00      0.50      0.67         4\n",
      "   Class 157       0.80      0.44      0.57         9\n",
      "   Class 158       0.00      0.00      0.00         2\n",
      "   Class 159       1.00      1.00      1.00         1\n",
      "   Class 160       1.00      1.00      1.00         1\n",
      "   Class 161       0.00      0.00      0.00         0\n",
      "   Class 162       0.00      0.00      0.00         1\n",
      "   Class 163       0.00      0.00      0.00         0\n",
      "   Class 164       0.00      0.00      0.00         1\n",
      "   Class 165       0.00      0.00      0.00         1\n",
      "   Class 166       0.00      0.00      0.00         5\n",
      "   Class 167       0.17      0.33      0.22         3\n",
      "   Class 168       0.18      0.33      0.24         6\n",
      "   Class 169       0.50      1.00      0.67         1\n",
      "   Class 170       0.00      0.00      0.00         2\n",
      "   Class 171       0.00      0.00      0.00         1\n",
      "   Class 172       0.00      0.00      0.00         1\n",
      "   Class 173       0.00      0.00      0.00         1\n",
      "   Class 174       0.00      0.00      0.00         3\n",
      "   Class 175       1.00      0.50      0.67         2\n",
      "   Class 176       0.00      0.00      0.00         1\n",
      "   Class 177       0.00      0.00      0.00         3\n",
      "   Class 178       0.00      0.00      0.00         3\n",
      "   Class 179       1.00      0.67      0.80         3\n",
      "   Class 180       0.00      0.00      0.00         1\n",
      "   Class 181       0.00      0.00      0.00         4\n",
      "   Class 182       0.00      0.00      0.00         0\n",
      "   Class 183       0.50      0.67      0.57         6\n",
      "   Class 184       0.00      0.00      0.00         0\n",
      "   Class 185       0.00      0.00      0.00         3\n",
      "   Class 186       0.00      0.00      0.00         0\n",
      "   Class 187       0.00      0.00      0.00         1\n",
      "   Class 188       0.00      0.00      0.00         1\n",
      "   Class 189       0.00      0.00      0.00         4\n",
      "   Class 190       0.00      0.00      0.00         1\n",
      "   Class 191       0.00      0.00      0.00         0\n",
      "   Class 192       0.00      0.00      0.00         0\n",
      "   Class 193       0.00      0.00      0.00         1\n",
      "   Class 194       1.00      0.50      0.67         4\n",
      "   Class 195       0.00      0.00      0.00         2\n",
      "   Class 196       0.00      0.00      0.00         2\n",
      "   Class 197       0.00      0.00      0.00         1\n",
      "   Class 198       0.60      0.60      0.60         5\n",
      "   Class 199       0.00      0.00      0.00         1\n",
      "   Class 200       1.00      0.50      0.67         2\n",
      "   Class 201       0.00      0.00      0.00         1\n",
      "   Class 202       0.00      0.00      0.00         2\n",
      "   Class 203       0.00      0.00      0.00         2\n",
      "   Class 204       0.75      0.86      0.80         7\n",
      "   Class 205       1.00      1.00      1.00         1\n",
      "   Class 206       0.00      0.00      0.00         0\n",
      "   Class 207       0.00      0.00      0.00         2\n",
      "   Class 208       0.00      0.00      0.00         1\n",
      "   Class 209       0.06      0.50      0.11         2\n",
      "   Class 210       0.00      0.00      0.00         1\n",
      "   Class 211       1.00      1.00      1.00         1\n",
      "   Class 212       0.00      0.00      0.00         1\n",
      "   Class 213       0.00      0.00      0.00         1\n",
      "   Class 214       0.00      0.00      0.00         1\n",
      "   Class 215       0.00      0.00      0.00         1\n",
      "   Class 216       0.00      0.00      0.00         1\n",
      "   Class 217       0.00      0.00      0.00         2\n",
      "   Class 218       0.00      0.00      0.00         0\n",
      "   Class 219       0.00      0.00      0.00         0\n",
      "   Class 220       0.00      0.00      0.00         3\n",
      "   Class 221       0.29      0.40      0.33         5\n",
      "   Class 222       0.00      0.00      0.00         2\n",
      "   Class 223       0.00      0.00      0.00         4\n",
      "   Class 224       0.00      0.00      0.00         0\n",
      "   Class 225       0.00      0.00      0.00         1\n",
      "   Class 226       0.00      0.00      0.00         1\n",
      "   Class 227       0.50      1.00      0.67         1\n",
      "   Class 228       0.50      0.33      0.40         3\n",
      "   Class 229       0.50      1.00      0.67         1\n",
      "   Class 230       0.00      0.00      0.00         0\n",
      "   Class 231       0.00      0.00      0.00         0\n",
      "   Class 232       0.00      0.00      0.00         2\n",
      "   Class 233       0.00      0.00      0.00         1\n",
      "   Class 234       0.00      0.00      0.00         0\n",
      "   Class 235       0.00      0.00      0.00         0\n",
      "   Class 236       0.00      0.00      0.00         0\n",
      "   Class 237       0.00      0.00      0.00         1\n",
      "   Class 238       0.00      0.00      0.00         0\n",
      "   Class 239       1.00      0.25      0.40         4\n",
      "   Class 240       0.25      0.33      0.29         3\n",
      "   Class 241       1.00      0.50      0.67         2\n",
      "   Class 242       0.00      0.00      0.00         2\n",
      "   Class 243       0.00      0.00      0.00         1\n",
      "   Class 244       0.00      0.00      0.00         1\n",
      "   Class 245       1.00      1.00      1.00         1\n",
      "   Class 246       0.00      0.00      0.00         1\n",
      "   Class 247       0.00      0.00      0.00         2\n",
      "   Class 248       0.00      0.00      0.00         0\n",
      "   Class 249       0.00      0.00      0.00         0\n",
      "   Class 250       1.00      1.00      1.00         1\n",
      "   Class 251       0.80      1.00      0.89         4\n",
      "   Class 252       0.00      0.00      0.00         2\n",
      "   Class 253       0.00      0.00      0.00         2\n",
      "   Class 254       0.00      0.00      0.00         0\n",
      "   Class 255       0.00      0.00      0.00         1\n",
      "   Class 256       0.50      1.00      0.67         1\n",
      "   Class 257       0.00      0.00      0.00         2\n",
      "   Class 258       0.00      0.00      0.00         0\n",
      "   Class 259       0.00      0.00      0.00         0\n",
      "   Class 260       0.00      0.00      0.00         1\n",
      "   Class 261       0.00      0.00      0.00         1\n",
      "   Class 262       0.00      0.00      0.00         1\n",
      "   Class 263       0.00      0.00      0.00         0\n",
      "   Class 264       0.00      0.00      0.00         1\n",
      "   Class 265       0.00      0.00      0.00         1\n",
      "   Class 266       0.00      0.00      0.00         2\n",
      "   Class 267       1.00      1.00      1.00         1\n",
      "   Class 268       0.00      0.00      0.00         1\n",
      "   Class 269       0.00      0.00      0.00         1\n",
      "   Class 270       0.00      0.00      0.00         0\n",
      "   Class 271       0.00      0.00      0.00         0\n",
      "   Class 272       0.00      0.00      0.00         1\n",
      "   Class 273       0.00      0.00      0.00         1\n",
      "   Class 274       0.67      1.00      0.80         4\n",
      "   Class 275       0.00      0.00      0.00         1\n",
      "   Class 276       0.00      0.00      0.00         0\n",
      "   Class 277       0.00      0.00      0.00         1\n",
      "   Class 278       0.00      0.00      0.00         3\n",
      "   Class 279       1.00      1.00      1.00         1\n",
      "   Class 280       0.00      0.00      0.00         2\n",
      "   Class 281       0.00      0.00      0.00         1\n",
      "   Class 282       0.00      0.00      0.00         2\n",
      "   Class 283       0.25      0.33      0.29         3\n",
      "   Class 284       0.00      0.00      0.00         1\n",
      "   Class 285       0.00      0.00      0.00         1\n",
      "   Class 286       0.00      0.00      0.00         1\n",
      "   Class 287       0.00      0.00      0.00         1\n",
      "   Class 288       0.00      0.00      0.00         1\n",
      "   Class 289       0.00      0.00      0.00         1\n",
      "   Class 290       0.00      0.00      0.00         2\n",
      "   Class 291       0.00      0.00      0.00         2\n",
      "   Class 292       0.00      0.00      0.00         1\n",
      "   Class 293       0.00      0.00      0.00         0\n",
      "   Class 294       0.00      0.00      0.00         1\n",
      "   Class 295       0.67      1.00      0.80         2\n",
      "   Class 296       0.50      0.33      0.40         3\n",
      "   Class 297       0.00      0.00      0.00         1\n",
      "   Class 298       0.00      0.00      0.00         1\n",
      "   Class 299       0.00      0.00      0.00         0\n",
      "   Class 300       0.00      0.00      0.00         1\n",
      "   Class 301       0.00      0.00      0.00         1\n",
      "   Class 302       1.00      0.75      0.86         4\n",
      "   Class 303       0.00      0.00      0.00         1\n",
      "   Class 304       0.00      0.00      0.00         0\n",
      "   Class 305       0.67      1.00      0.80         2\n",
      "   Class 306       0.00      0.00      0.00         1\n",
      "   Class 307       0.00      0.00      0.00         1\n",
      "   Class 308       0.00      0.00      0.00         1\n",
      "   Class 309       0.00      0.00      0.00         0\n",
      "   Class 310       0.00      0.00      0.00         1\n",
      "   Class 311       0.00      0.00      0.00         2\n",
      "   Class 312       0.00      0.00      0.00         1\n",
      "   Class 313       0.00      0.00      0.00         0\n",
      "   Class 314       0.00      0.00      0.00         0\n",
      "   Class 315       0.00      0.00      0.00         0\n",
      "   Class 316       0.00      0.00      0.00         1\n",
      "   Class 317       0.00      0.00      0.00         2\n",
      "   Class 318       0.00      0.00      0.00         1\n",
      "   Class 319       1.00      1.00      1.00         1\n",
      "   Class 320       0.00      0.00      0.00         1\n",
      "   Class 321       0.00      0.00      0.00         2\n",
      "   Class 322       0.00      0.00      0.00         1\n",
      "   Class 323       0.00      0.00      0.00         1\n",
      "   Class 324       0.00      0.00      0.00         1\n",
      "   Class 325       0.00      0.00      0.00         0\n",
      "   Class 326       0.00      0.00      0.00         0\n",
      "   Class 327       0.50      1.00      0.67         1\n",
      "   Class 328       0.00      0.00      0.00         1\n",
      "   Class 329       0.00      0.00      0.00         1\n",
      "   Class 330       0.33      1.00      0.50         1\n",
      "   Class 331       0.50      0.50      0.50         2\n",
      "   Class 332       0.00      0.00      0.00         1\n",
      "   Class 333       0.00      0.00      0.00         2\n",
      "   Class 334       0.00      0.00      0.00         1\n",
      "   Class 335       0.00      0.00      0.00         1\n",
      "   Class 336       0.00      0.00      0.00         2\n",
      "   Class 337       0.00      0.00      0.00         1\n",
      "   Class 338       0.00      0.00      0.00         1\n",
      "   Class 339       0.00      0.00      0.00         1\n",
      "   Class 340       0.00      0.00      0.00         1\n",
      "   Class 341       0.50      1.00      0.67         1\n",
      "   Class 342       0.00      0.00      0.00         1\n",
      "   Class 343       0.00      0.00      0.00         1\n",
      "   Class 344       0.00      0.00      0.00         1\n",
      "   Class 345       0.00      0.00      0.00         1\n",
      "   Class 346       0.00      0.00      0.00         1\n",
      "   Class 347       0.50      1.00      0.67         1\n",
      "   Class 348       0.00      0.00      0.00         0\n",
      "   Class 349       0.00      0.00      0.00         1\n",
      "   Class 350       0.00      0.00      0.00         1\n",
      "   Class 351       1.00      0.50      0.67         2\n",
      "   Class 352       1.00      1.00      1.00         1\n",
      "   Class 353       0.00      0.00      0.00         1\n",
      "   Class 354       0.00      0.00      0.00         3\n",
      "   Class 355       0.00      0.00      0.00         1\n",
      "   Class 356       0.00      0.00      0.00         1\n",
      "   Class 357       0.00      0.00      0.00         1\n",
      "   Class 358       0.00      0.00      0.00         1\n",
      "   Class 359       0.00      0.00      0.00         1\n",
      "   Class 360       0.00      0.00      0.00         1\n",
      "   Class 361       0.00      0.00      0.00         1\n",
      "   Class 362       0.25      0.50      0.33         4\n",
      "   Class 363       0.00      0.00      0.00         1\n",
      "   Class 364       0.00      0.00      0.00         0\n",
      "   Class 365       0.00      0.00      0.00         1\n",
      "   Class 366       0.00      0.00      0.00         1\n",
      "   Class 367       0.00      0.00      0.00         1\n",
      "   Class 368       0.00      0.00      0.00         1\n",
      "   Class 369       0.00      0.00      0.00         1\n",
      "   Class 370       0.00      0.00      0.00         0\n",
      "   Class 371       0.00      0.00      0.00         1\n",
      "   Class 372       0.00      0.00      0.00         1\n",
      "   Class 373       0.00      0.00      0.00         0\n",
      "   Class 374       0.00      0.00      0.00         1\n",
      "   Class 375       0.50      1.00      0.67         1\n",
      "   Class 376       0.00      0.00      0.00         1\n",
      "   Class 377       0.00      0.00      0.00         1\n",
      "   Class 378       0.00      0.00      0.00         0\n",
      "   Class 379       0.50      0.14      0.22         7\n",
      "   Class 380       0.00      0.00      0.00         1\n",
      "   Class 381       0.00      0.00      0.00         1\n",
      "   Class 382       0.00      0.00      0.00         1\n",
      "   Class 383       1.00      1.00      1.00         1\n",
      "   Class 384       0.00      0.00      0.00         1\n",
      "   Class 385       1.00      1.00      1.00         1\n",
      "   Class 386       0.00      0.00      0.00         1\n",
      "   Class 387       0.00      0.00      0.00         1\n",
      "   Class 388       0.00      0.00      0.00         1\n",
      "   Class 389       0.00      0.00      0.00         0\n",
      "   Class 390       0.57      0.80      0.67         5\n",
      "   Class 391       0.00      0.00      0.00         1\n",
      "   Class 392       0.00      0.00      0.00         1\n",
      "   Class 393       0.00      0.00      0.00         0\n",
      "   Class 394       0.00      0.00      0.00         1\n",
      "   Class 395       0.00      0.00      0.00         1\n",
      "   Class 396       0.00      0.00      0.00         1\n",
      "   Class 397       1.00      0.50      0.67         2\n",
      "   Class 398       0.00      0.00      0.00         0\n",
      "   Class 399       1.00      1.00      1.00         2\n",
      "   Class 400       0.00      0.00      0.00         1\n",
      "   Class 401       0.00      0.00      0.00         1\n",
      "   Class 402       0.75      0.60      0.67         5\n",
      "   Class 403       1.00      0.67      0.80         3\n",
      "   Class 404       1.00      1.00      1.00         1\n",
      "   Class 405       0.00      0.00      0.00         1\n",
      "   Class 406       1.00      0.50      0.67         2\n",
      "   Class 407       0.00      0.00      0.00         1\n",
      "   Class 408       0.00      0.00      0.00         1\n",
      "   Class 409       0.00      0.00      0.00         1\n",
      "   Class 410       0.00      0.00      0.00         1\n",
      "   Class 411       0.00      0.00      0.00         2\n",
      "   Class 412       0.00      0.00      0.00         0\n",
      "   Class 413       0.00      0.00      0.00         1\n",
      "   Class 414       0.00      0.00      0.00         0\n",
      "   Class 415       0.67      1.00      0.80         2\n",
      "   Class 416       0.60      0.50      0.55         6\n",
      "   Class 417       0.00      0.00      0.00         0\n",
      "   Class 418       0.00      0.00      0.00         1\n",
      "   Class 419       0.00      0.00      0.00         1\n",
      "   Class 420       0.00      0.00      0.00         1\n",
      "   Class 421       0.00      0.00      0.00         0\n",
      "   Class 422       0.00      0.00      0.00         1\n",
      "   Class 423       0.00      0.00      0.00         1\n",
      "   Class 424       0.00      0.00      0.00         0\n",
      "   Class 425       0.00      0.00      0.00         1\n",
      "   Class 426       0.00      0.00      0.00         1\n",
      "   Class 427       0.00      0.00      0.00         0\n",
      "   Class 428       0.00      0.00      0.00         1\n",
      "   Class 429       0.75      1.00      0.86         3\n",
      "   Class 430       0.00      0.00      0.00         1\n",
      "   Class 431       0.00      0.00      0.00         1\n",
      "   Class 432       0.00      0.00      0.00         0\n",
      "   Class 433       0.00      0.00      0.00         1\n",
      "   Class 434       0.00      0.00      0.00         0\n",
      "   Class 435       0.00      0.00      0.00         0\n",
      "   Class 436       1.00      1.00      1.00         2\n",
      "   Class 437       0.00      0.00      0.00         2\n",
      "   Class 438       0.00      0.00      0.00         1\n",
      "   Class 439       0.00      0.00      0.00         0\n",
      "   Class 440       0.00      0.00      0.00         0\n",
      "   Class 441       0.00      0.00      0.00         0\n",
      "   Class 442       0.00      0.00      0.00         1\n",
      "   Class 443       0.00      0.00      0.00         1\n",
      "   Class 444       0.00      0.00      0.00         1\n",
      "   Class 445       0.00      0.00      0.00         1\n",
      "   Class 446       0.00      0.00      0.00         1\n",
      "   Class 447       0.00      0.00      0.00         1\n",
      "   Class 448       0.00      0.00      0.00         1\n",
      "   Class 449       0.00      0.00      0.00         1\n",
      "   Class 450       0.00      0.00      0.00         1\n",
      "   Class 451       0.00      0.00      0.00         1\n",
      "   Class 452       0.00      0.00      0.00         1\n",
      "   Class 453       0.00      0.00      0.00         0\n",
      "   Class 454       0.00      0.00      0.00         1\n",
      "   Class 455       0.00      0.00      0.00         1\n",
      "   Class 456       0.00      0.00      0.00         1\n",
      "   Class 457       0.00      0.00      0.00         1\n",
      "   Class 458       0.00      0.00      0.00         1\n",
      "   Class 459       0.00      0.00      0.00         1\n",
      "   Class 460       0.00      0.00      0.00         1\n",
      "   Class 461       0.00      0.00      0.00         0\n",
      "   Class 462       0.80      1.00      0.89         4\n",
      "   Class 463       0.00      0.00      0.00         2\n",
      "   Class 464       0.00      0.00      0.00         0\n",
      "   Class 465       0.00      0.00      0.00         1\n",
      "   Class 466       0.00      0.00      0.00         0\n",
      "   Class 467       1.00      0.50      0.67         2\n",
      "   Class 468       0.00      0.00      0.00         1\n",
      "   Class 469       0.00      0.00      0.00         3\n",
      "   Class 470       0.00      0.00      0.00         1\n",
      "   Class 471       1.00      1.00      1.00         1\n",
      "   Class 472       0.00      0.00      0.00         1\n",
      "   Class 473       0.00      0.00      0.00         0\n",
      "   Class 474       0.00      0.00      0.00         1\n",
      "   Class 475       1.00      1.00      1.00         1\n",
      "   Class 476       0.00      0.00      0.00         1\n",
      "   Class 477       0.00      0.00      0.00         1\n",
      "   Class 478       0.00      0.00      0.00         1\n",
      "   Class 479       1.00      1.00      1.00         1\n",
      "   Class 480       0.00      0.00      0.00         0\n",
      "   Class 481       0.00      0.00      0.00         1\n",
      "   Class 482       0.00      0.00      0.00         1\n",
      "   Class 483       0.00      0.00      0.00         1\n",
      "   Class 484       0.00      0.00      0.00         0\n",
      "   Class 485       0.00      0.00      0.00         1\n",
      "   Class 486       0.00      0.00      0.00         1\n",
      "   Class 487       0.00      0.00      0.00         0\n",
      "   Class 488       0.00      0.00      0.00         1\n",
      "   Class 489       0.00      0.00      0.00         1\n",
      "   Class 490       1.00      1.00      1.00         1\n",
      "   Class 491       0.00      0.00      0.00         1\n",
      "   Class 492       0.00      0.00      0.00         1\n",
      "   Class 493       0.00      0.00      0.00         1\n",
      "   Class 494       1.00      1.00      1.00         1\n",
      "   Class 495       0.00      0.00      0.00         1\n",
      "   Class 496       0.00      0.00      0.00         1\n",
      "   Class 497       0.00      0.00      0.00         0\n",
      "   Class 498       0.00      0.00      0.00         1\n",
      "   Class 499       0.00      0.00      0.00         1\n",
      "   Class 500       0.00      0.00      0.00         2\n",
      "   Class 501       0.00      0.00      0.00         0\n",
      "   Class 502       0.00      0.00      0.00         1\n",
      "   Class 503       0.00      0.00      0.00         1\n",
      "   Class 504       0.00      0.00      0.00         1\n",
      "   Class 505       0.00      0.00      0.00         1\n",
      "   Class 506       0.00      0.00      0.00         1\n",
      "   Class 507       0.00      0.00      0.00         0\n",
      "   Class 508       1.00      1.00      1.00         1\n",
      "   Class 509       1.00      1.00      1.00         1\n",
      "   Class 510       0.00      0.00      0.00         1\n",
      "   Class 511       0.00      0.00      0.00         0\n",
      "   Class 512       0.00      0.00      0.00         1\n",
      "   Class 513       0.00      0.00      0.00         1\n",
      "   Class 514       0.00      0.00      0.00         1\n",
      "   Class 515       0.00      0.00      0.00         1\n",
      "   Class 516       0.00      0.00      0.00         0\n",
      "   Class 517       0.00      0.00      0.00         1\n",
      "   Class 518       0.00      0.00      0.00         1\n",
      "   Class 519       0.00      0.00      0.00         1\n",
      "   Class 520       0.00      0.00      0.00         0\n",
      "   Class 521       0.00      0.00      0.00         0\n",
      "   Class 522       0.00      0.00      0.00         1\n",
      "   Class 523       0.00      0.00      0.00         2\n",
      "   Class 524       0.00      0.00      0.00         1\n",
      "   Class 525       0.00      0.00      0.00         1\n",
      "   Class 526       0.00      0.00      0.00         1\n",
      "   Class 527       1.00      1.00      1.00         2\n",
      "   Class 528       0.00      0.00      0.00         1\n",
      "   Class 529       0.00      0.00      0.00         1\n",
      "   Class 530       0.00      0.00      0.00         0\n",
      "   Class 531       0.00      0.00      0.00         1\n",
      "   Class 532       0.00      0.00      0.00         1\n",
      "   Class 533       0.00      0.00      0.00         0\n",
      "   Class 534       0.00      0.00      0.00         1\n",
      "   Class 535       0.00      0.00      0.00         1\n",
      "   Class 536       0.00      0.00      0.00         1\n",
      "   Class 537       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.35      1017\n",
      "   macro avg       0.19      0.19      0.18      1017\n",
      "weighted avg       0.36      0.35      0.34      1017\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_heatmap_and_error_report(model_2,X_test,Y_product_test,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "874dd596-693b-43f7-b765-778ebf40c107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565, 7372)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#follow the same steps as before to vectorize the title_validation \n",
    "#into a form that can be loaded into the trained model\n",
    "\n",
    "#to avoid having different number of features for the train set and the validation set with the tfidf vectorizer we are \n",
    "#going to use the tfidf vectorizer fitted for the train set to the validation set \n",
    "#the function we created previously will not suffise because it will create a unique representation based on the validation set \n",
    "#with number of features different than the ones in the training set\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()  #create a tfidfvectorizer object\n",
    "\n",
    "\n",
    "tfidf_vectorizer.fit(title)        #fit it for the title of the training set \n",
    "                                                   \n",
    "X_center=tfidf_vectorizer.transform(title)      #transform the title of the training set into a format where each entry is a \n",
    "                                                #numeric vector of shape (m,n) where m \n",
    "                                                #is the number of examples in the given set and n is the number of features\n",
    "\n",
    "X_center_format=X_center.toarray()             #follow this conversion to get an array form\n",
    "                                               #\n",
    "\n",
    "\n",
    "X_val_0=tfidf_vectorizer.transform(title_validation)   #use the vectorizer that is fitted to the training set to transform the input \n",
    "                                                       #from the validation set. the result will be a numeric vector of shape (m_val,n)\n",
    "                                                       #where m_val is the number of entries in the validation set and \n",
    "                                                       #n is the number of features which is the same as the number of features \n",
    "                                                       # used for training\n",
    "\n",
    "X_val_1=X_val_0.toarray()\n",
    "\n",
    "X_val_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "87daccf9-fa14-471f-91e5-b6b8fb6c2d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "#predictions in text format from the trained dense model named model\n",
    "\n",
    "P_val_prediction=from_pred_and_dictionary_to_labels(X_val_1,model_2,one_hot_encode_product)  #use the aforementioned function\n",
    "                                                       #to predict the product labels for the validation set using the \n",
    "                                                       #validation title. This function also receives as input the dictionary with \n",
    "                                                       #the labels and their one hot encoding in order to convert the predictions back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "7ed13761-8991-469e-b4ce-5b5d9a5edce8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PZ_val_prediction\n",
    "\n",
    "len(H_val_prediction)\n",
    "len(P_val_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "e38bd683-d5b7-4c5b-ae1e-9a46f72a730f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn the predictions into columns of a new dataframe and save them to a csv file\n",
    "predicted_data_final={\n",
    "'hazard': H_val_prediction,\n",
    "'product': P_val_prediction\n",
    "    \n",
    "}\n",
    "df_final=pd.DataFrame(predicted_data_final)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_final.to_csv('st2_title_dense.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e7d046-df60-4f86-ab7e-f7944c0a5c36",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
