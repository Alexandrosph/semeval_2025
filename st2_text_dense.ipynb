{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ebd995c-136d-48b2-a577-946e7ba5b80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the necessary libraries\n",
    "import pandas as pd \n",
    "import numpy as  np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#import all the necessary libraries to build a neural network classifier from tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential                   \n",
    "from tensorflow.keras.layers import Dense                        #import fully connected neural net layers\n",
    "from tensorflow.keras.optimizers import Adam                     #choose adam as the optimization algorithm\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy      #cost function needed for softmax classification if the labels are \n",
    "                                                                 #one hot encoded\n",
    "\n",
    "from tensorflow.keras.regularizers import l2                     #add regularization in order to avoid overfitting \n",
    "\n",
    "from sklearn.model_selection import train_test_split             #to split the training set into train and test set \n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report  #to proceed with error analysis on our predictions\n",
    "\n",
    "\n",
    "import seaborn as sns                                                 #to visualize confusion matrices as a heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dropout              #import long-short term memory neural net layers\n",
    "                                                               #and dropout regularization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd3cc9c-db8e-4de5-b125-4c98ec4b31cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "257be34f-b20e-4530-a652-69290be457a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  vectorize_string_csv_column_TF_idf(string_column):  \n",
    "                                                #create a function that receives a column of strings from a csv file \n",
    "                                                  #and converts each entry into a unique tfidf vector depending on the \n",
    "                                                  #unique vocabulary\n",
    "                                                   \n",
    "                                                 #necessary to convert a text input into a numeric vector to be used \n",
    "                                                 #as input to the classifier\n",
    "\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()  \n",
    "                                      #create a TF_idf_vectorizer model that will receive the entire csv column\n",
    "                                      #and will eventually turn each entry into a numeric vector\n",
    "                                      #the length of each vector will be the number of the unique words in the vocabulary\n",
    "                                      #if this length=N the resulting vectors for each entry will be of dimension (N,)\n",
    "    \n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(string_column)  \n",
    "                                                    #apply the created model to the column title of the csv\n",
    "\n",
    "\n",
    "    tfidf_vectors = tfidf_matrix.toarray()          #each entry of the tfidf_vectors is a numeric vector\n",
    "                                                    #corresponding to a title entry\n",
    "                                                    #convert it to an array format for ease of handling\n",
    "\n",
    "    Vocabulary=tfidf_vectorizer.get_feature_names_out()  #export the unique vocabulary out of the created vectorizer model\n",
    "\n",
    "    return tfidf_vectors, Vocabulary   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51283f3e-482f-4c32-8d3a-9c060cf5e1b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b1eed9b3-c457-422a-b05a-e0f6e3cfd4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that finds all the unique labels in a column \n",
    "def find_unique_column_labels(column):\n",
    "     \n",
    "    unique_labels_list=[]               #create a list that will contain all the unique labels found in the input column\n",
    "\n",
    "    for i in column : #search the entire column\n",
    "        if i not in unique_labels_list:   #if the element i is not found in the unique list \n",
    "            unique_labels_list.append(i)  #append it to the list\n",
    "    \n",
    "    return  unique_labels_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "075f49f9-420e-4fbe-bad6-5ebe759b28b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89be652b-8d26-4383-84c9-dc910cd18c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_labels(unique_labels_of_a_column):\n",
    "    #create a dictionary for these categories that coresponds each one into a one hot numpy vector\n",
    "    #this is necessary in order to use a softmax classifier \n",
    "\n",
    "    number_of_classes=len(unique_labels_of_a_column)  #find the number of classes/possible labels from the vector that contains the unique labels\n",
    "\n",
    "                                               #create a numpy I matrix I lxl where l is the number of classes \n",
    "                                               #each row of the I matrix will correspond to a one hot encoding for each label\n",
    "    I=np.eye(number_of_classes)\n",
    "\n",
    "    #create a dictionary to correspond each class name with it's one hot encoded label\n",
    "\n",
    "    dict_labels={} #initialize an empty dictionary where the keys will be the labels and the values will be their one hot encoding\n",
    "\n",
    "\n",
    "    for i in range(number_of_classes):\n",
    "        dict_labels[unique_labels_of_a_column[i]]=I[i,:]\n",
    "\n",
    "    return dict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b4f65a-6f32-41b5-b4df-7b7901964fb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "58b45ea2-7c12-4920-8660-dcebe2c150df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Y_label(initial_label_column,dict_labels):\n",
    "    #create the Y part of the dataset by receiving  a column  and the dict_labels corresponding to that column\n",
    "\n",
    "\n",
    "\n",
    "    #len(dict_labels['biological'])\n",
    "\n",
    "    Y1={}   #initialize an empty dictionary\n",
    "    count=0 #and a count variable\n",
    "    for i in initial_label_column:    #search through the hazard_category column of the data frame\n",
    "        for j in dict_labels.keys():  #and through all the keys of the labels dictionary with keys all the unique labels \n",
    "                                  #and values their one hot encoded representation\n",
    "            if i==j:                   #if you find a match\n",
    "                Y1[count]=dict_labels[j]   #assign the category with it's one hot encoding\n",
    "                count+=1\n",
    "    #Y\n",
    "    #now the dictionary above should be turned into a numpy matrix with its elements being column vectors\n",
    "\n",
    "    # Convert dictionary values to a numpy matrix\n",
    "    matrix = np.array([v for v in Y1.values()]).T\n",
    "\n",
    "    matrix.shape #Nxm format\n",
    "    return matrix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f4fea2-2138-4402-a8cf-550222971aca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9cc6129d-668f-4151-be01-b8ae255dd8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_eval_dense_nn(X_train,Y_train,X_test,Y_test,hu_1,hu_2,number_of_classes,a_epochs,a_batch_size):\n",
    "    #define a function that trains a fully connected dense neural net to fit the pre-processed data\n",
    "    #X_train are the training data\n",
    "    #Y_train are the training labels \n",
    "    #X_test,Y_test are the data we are going to use for validation\n",
    "    #hu_1 is the number of hidden units in layer 1\n",
    "    #hu_2 is the number of hidden units in layer 2\n",
    "    #number of classes is self explanatory and it is required for the softmax classification layer\n",
    "    #a_epochs is the number of training epochs \n",
    "    #and a_batch size is the number of batches that we use to split the training data\n",
    "    #the activation function is chosen as the relu activation f(z)=max(z,0)\n",
    "    #and the regularization is set to zero after manual tuning\n",
    "    \n",
    "    model = Sequential([                                          #model architecture    \n",
    "    Dense(hu_1, activation='relu', input_shape=(X_train.shape[1],),kernel_regularizer=l2(0)),  \n",
    "    Dense(hu_2, activation='relu',kernel_regularizer=l2(0)),                      \n",
    "    Dense(number_of_classes, activation='softmax',kernel_regularizer=l2(0))         \n",
    "    ])\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001),           #for training the model the Adam optimization algorithm has been chosen \n",
    "                                                                  #with initial learning rate=0.001\n",
    "        loss=CategoricalCrossentropy(),                            #since the output labels are one-hot encoded we have chosen the \n",
    "                                                                 #Categorical cross entropy function as a loss function\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    model.fit(     #the method fit is used to actually train the model using the X_train and Y_train \n",
    "    X_train, Y_train,    #as training data and X_test,Y_test as validation data \n",
    "    validation_data=(X_test, Y_test),\n",
    "    epochs=a_epochs,          #the number of epochs chosen for the training are 30\n",
    "    batch_size=a_batch_size,     #the training itself is mini batch gradient-descent with adam and the training data -set is split into \n",
    "    verbose=1          #32 batches    \n",
    "    )\n",
    "\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, Y_test, verbose=1)   #we now evaluate the models performance on the\n",
    "                                                                       #X_test and Y_test data for which we now the labels \n",
    "                                                                       #but these labels are unkown to the model \n",
    "                                                                       #using only the input data X_test it predicts the output \n",
    "                                                                       #labels and then compares them with the actual Y_test labels\n",
    "\n",
    "    print(\"Test Loss:\", test_loss)                           #depict the final value of the cost function for the test set                                     \n",
    "    print(\"Test Accuracy:\", test_accuracy)                  #depict the accuracy of predictions in the test set\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6030abde-ae3c-49a6-87f3-9eabbfecb4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbb3d2ea-0951-479b-b18a-a3777fbf2167",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heatmap_and_error_report(model,X_test,Y_test,num_of_classes):  #input the X_test \n",
    "                                                                    #and Y_test\n",
    "                                                                    #make predictions on X_test with the model\n",
    "                                                                    #compare the model predictions with \n",
    "                                                                    #the actual Y_test and create an error analysis\n",
    "\n",
    "    \n",
    "    Y_predict_initial=model.predict(X_test) #we predict the model output for the X_test \n",
    "        #and we are going to compare with the actual lables from Y_test\n",
    "\n",
    "    #Each entry y in Y_predict_initial is a vector of  outputs= number of classes, containing the probabilities that show \n",
    "    #how likely it is for the model to assign an entry x of x_test to a specific class.\n",
    "    #For the entry x with prediction y if y[0] is the highest amongst the elements of y, x will be assigned to \n",
    "    #class 0. If y[1] is the highest then x will be assigned to class_1 and so on\n",
    "\n",
    "    #As a result, we need to find the index of  maximum element of each y in Y_predict \n",
    "    #that will show us in which class x corresponds to:\n",
    "    Y_intermediate = np.argmax(Y_predict_initial, axis=1) #find the index of the maximum element for each y in Y_predict_initial\n",
    "    \n",
    "    #we are also going to convert Y_test from a one hot encoding to \n",
    "    #the number of class this one hot encoding represents and compare it with \n",
    "    #the predicted class stored in Y_intermediate\n",
    "\n",
    "    Y_true = np.argmax(Y_test, axis=1)  #due to the fact that we have a one hot encoding, finding the index of the max \n",
    "                                    #element will lead directly to the number of class it represents\n",
    "\n",
    "    #\n",
    "    #we are going to find out where Y_true matches our prediction in Y_intermediate \n",
    "    #and we are going to display a confusion matrix of the true vs the prediction\n",
    "\n",
    "    \n",
    "    conf_mat=confusion_matrix(Y_true,Y_intermediate)\n",
    "    \n",
    "\n",
    "   # sns.heatmap(conf_mat,annot=True, fmt='d', cmap='Blues')\n",
    "   # plt.xlabel('Predicted Class')\n",
    "   # plt.ylabel('True Class')\n",
    "   # plt.title('Confusion Matrix')\n",
    "   # plt.show()\n",
    "\n",
    "#the classification report is applied to the test set which is a random split from the entire training set\n",
    "    #as a result it may not include the entire number of classes and we will get the error report based on the \n",
    "    #classes stored in the confusion matrix and the test set\n",
    "    \n",
    "    report = classification_report(Y_true, Y_intermediate, target_names=[f\"Class {i}\" for i in range(conf_mat.shape[0])]) \n",
    "    print(\"\\n\",report)\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5e48cdd-35ca-4935-8174-bbfb969a875c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d38dcfe8-0ea7-46f9-b37d-11d438c66015",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a function that inputs data X\n",
    "#the model that makes predictions and the \n",
    "#dictionary with the labels and their one hot encoding. \n",
    "#It computes the numeric predictions for X and turns them into the text of the label \n",
    "#they correspond to\n",
    "\n",
    "def from_pred_and_dictionary_to_labels(X,model,one_hot_dictionary):\n",
    "    \n",
    "        #predicted output\n",
    "    Y_pr=model.predict(X)\n",
    "\n",
    "        #create a new dictionary with the key being the index/label of the one_hot_dictionary\n",
    "        #and the value of this new  dictionary being the phrase of the one hot encoding \n",
    "\n",
    "    new_dict={}\n",
    "\n",
    "    for key,value in one_hot_dictionary.items():   \n",
    "        convert_one_hot_encode_to_number=np.argmax(value)    #get the number representation of the one hot encoding\n",
    "        label=key                                            #get the phrase of the one hot encoding \n",
    "        new_dict[convert_one_hot_encode_to_number]=label     #store them as number-> phrase\n",
    "\n",
    "    #use the newly created dictionary to map the predictions into the labels\n",
    "    predictions_text_format=[]\n",
    "\n",
    "    for i in Y_pr:\n",
    "        chosen_label=np.argmax(i) #loop through the predictions of the model and choose to which label the prediction is assigned\n",
    "        predictions_text_format.append(new_dict[chosen_label]) #get the phrase corresponding to that label and append it to a list\n",
    "        \n",
    "    return predictions_text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a4d3b-9cfc-43f7-b8aa-6139085e0743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fe85558c-a3de-40d5-8823-d1aca2e275bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv incidents_train into a data frame\n",
    "\n",
    "df=pd.read_csv('incidents_train.csv')\n",
    "#df.head()\n",
    "\n",
    "#isolate the text category which is to be used as the input X to a classifier\n",
    "text=df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0090e0c-04e9-47e8-abc6-ce9a15ec93cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e8bcb6c7-edad-4a61-944b-6de15552289d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,unique_text_voc=vectorize_string_csv_column_TF_idf(text)\n",
    "#X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d63ec23-1723-4b85-b8f5-0cace014cbb5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ec56f00-7caf-4e37-9e84-ef3b4f84bf62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5082, 128)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hazard=df['hazard']  #get the hazard  for the data frame refering to the training set\n",
    "unique_hazard_labels=find_unique_column_labels(hazard) #find the unique labels of product category\n",
    "#print(len(unique_hazard_labels))\n",
    "\n",
    "one_hot_encode_hazard=one_hot_encode_labels(unique_hazard_labels)#one hot encode these labels and get a \n",
    "#dictionary with the key being the label and the value being its one hot encoding\n",
    "#one_hot_encode_hazard_category\n",
    "\n",
    "Y_hazard_transposed=create_Y_label(hazard,one_hot_encode_hazard)\n",
    "#create the Y part of the data-set but in a transposed format\n",
    "\n",
    "Y_hazard=Y_hazard_transposed.T\n",
    "Y_hazard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a8c57762-7268-43ae-83f8-d26b2391092e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#one_hot_encode_hazard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "184aacae-128f-4f84-950b-dab8cba25fa6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4065, 41409)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split the data intro train and test set\n",
    "#using the processed title data as X\n",
    "#and hazard  as Y\n",
    "X_train, X_test, Y_hazard_train, Y_hazard_test = train_test_split(X, Y_hazard, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dbe7b439-9f8e-4d83-a56b-9572a4859e82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - accuracy: 0.0471 - loss: 4.6390 - val_accuracy: 0.1563 - val_loss: 3.5184\n",
      "Epoch 2/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.1644 - loss: 3.3463 - val_accuracy: 0.2783 - val_loss: 3.1746\n",
      "Epoch 3/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.2915 - loss: 2.9397 - val_accuracy: 0.3274 - val_loss: 2.8942\n",
      "Epoch 4/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.3557 - loss: 2.6333 - val_accuracy: 0.3697 - val_loss: 2.6940\n",
      "Epoch 5/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.4421 - loss: 2.3336 - val_accuracy: 0.4415 - val_loss: 2.5198\n",
      "Epoch 6/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5194 - loss: 2.0671 - val_accuracy: 0.4592 - val_loss: 2.3790\n",
      "Epoch 7/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.5697 - loss: 1.8423 - val_accuracy: 0.4946 - val_loss: 2.2623\n",
      "Epoch 8/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6186 - loss: 1.6460 - val_accuracy: 0.5438 - val_loss: 2.1549\n",
      "Epoch 9/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.6795 - loss: 1.4799 - val_accuracy: 0.5634 - val_loss: 2.0711\n",
      "Epoch 10/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7161 - loss: 1.3049 - val_accuracy: 0.5762 - val_loss: 1.9980\n",
      "Epoch 11/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7448 - loss: 1.1402 - val_accuracy: 0.5811 - val_loss: 1.9459\n",
      "Epoch 12/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7513 - loss: 1.0275 - val_accuracy: 0.5851 - val_loss: 1.8974\n",
      "Epoch 13/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.7900 - loss: 0.8900 - val_accuracy: 0.5880 - val_loss: 1.8608\n",
      "Epoch 14/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8229 - loss: 0.7736 - val_accuracy: 0.5919 - val_loss: 1.8243\n",
      "Epoch 15/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.8590 - loss: 0.6992 - val_accuracy: 0.5929 - val_loss: 1.8126\n",
      "Epoch 16/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.8799 - loss: 0.6291 - val_accuracy: 0.6067 - val_loss: 1.7951\n",
      "Epoch 17/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9016 - loss: 0.5634 - val_accuracy: 0.6028 - val_loss: 1.8009\n",
      "Epoch 18/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9161 - loss: 0.4798 - val_accuracy: 0.6116 - val_loss: 1.7786\n",
      "Epoch 19/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9224 - loss: 0.4234 - val_accuracy: 0.6106 - val_loss: 1.7830\n",
      "Epoch 20/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9367 - loss: 0.3502 - val_accuracy: 0.6106 - val_loss: 1.7888\n",
      "Epoch 21/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9439 - loss: 0.3232 - val_accuracy: 0.6136 - val_loss: 1.8041\n",
      "Epoch 22/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9486 - loss: 0.2929 - val_accuracy: 0.6155 - val_loss: 1.8228\n",
      "Epoch 23/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9582 - loss: 0.2483 - val_accuracy: 0.6136 - val_loss: 1.8489\n",
      "Epoch 24/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9663 - loss: 0.2016 - val_accuracy: 0.6077 - val_loss: 1.8639\n",
      "Epoch 25/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9733 - loss: 0.1717 - val_accuracy: 0.6136 - val_loss: 1.8855\n",
      "Epoch 26/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9727 - loss: 0.1679 - val_accuracy: 0.6067 - val_loss: 1.9109\n",
      "Epoch 27/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9800 - loss: 0.1392 - val_accuracy: 0.6077 - val_loss: 1.9199\n",
      "Epoch 28/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9810 - loss: 0.1215 - val_accuracy: 0.6018 - val_loss: 1.9712\n",
      "Epoch 29/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9830 - loss: 0.1111 - val_accuracy: 0.6067 - val_loss: 1.9827\n",
      "Epoch 30/30\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9869 - loss: 0.0930 - val_accuracy: 0.6037 - val_loss: 2.0365\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6201 - loss: 1.9463\n",
      "Test Loss: 2.036480188369751\n",
      "Test Accuracy: 0.6037364602088928\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "#number of unique hazards is 1280\n",
    "\n",
    "model=train_and_eval_dense_nn(X_train,Y_hazard_train,X_test,Y_hazard_test,21,10,128,30,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9c291a55-bb84-4d20-baac-91c26da85073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.92      0.91      0.91       139\n",
      "     Class 1       0.00      0.00      0.00         2\n",
      "     Class 2       0.42      0.87      0.57        39\n",
      "     Class 3       0.95      0.81      0.87        47\n",
      "     Class 4       1.00      1.00      1.00         1\n",
      "     Class 5       0.88      0.83      0.85       121\n",
      "     Class 6       0.00      0.00      0.00         4\n",
      "     Class 7       0.00      0.00      0.00         3\n",
      "     Class 8       0.00      0.00      0.00         0\n",
      "     Class 9       0.00      0.00      0.00         4\n",
      "    Class 10       0.57      0.59      0.58        34\n",
      "    Class 11       0.89      0.47      0.62        36\n",
      "    Class 12       0.58      0.86      0.69       108\n",
      "    Class 13       0.71      0.24      0.36        21\n",
      "    Class 14       0.80      1.00      0.89         4\n",
      "    Class 15       1.00      0.33      0.50         6\n",
      "    Class 16       0.55      0.49      0.52        45\n",
      "    Class 17       0.14      0.50      0.22         2\n",
      "    Class 18       0.14      0.25      0.18         4\n",
      "    Class 19       0.00      0.00      0.00         0\n",
      "    Class 20       0.59      0.62      0.60        42\n",
      "    Class 21       0.00      0.00      0.00         1\n",
      "    Class 22       0.60      0.21      0.32        14\n",
      "    Class 23       0.00      0.00      0.00         4\n",
      "    Class 24       1.00      0.62      0.77         8\n",
      "    Class 25       0.00      0.00      0.00         5\n",
      "    Class 26       0.00      0.00      0.00         0\n",
      "    Class 27       0.00      0.00      0.00         3\n",
      "    Class 28       0.00      0.00      0.00         0\n",
      "    Class 29       0.00      0.00      0.00         2\n",
      "    Class 30       1.00      0.25      0.40         4\n",
      "    Class 31       1.00      1.00      1.00         1\n",
      "    Class 32       0.00      0.00      0.00         1\n",
      "    Class 33       0.74      0.65      0.69        43\n",
      "    Class 34       0.26      0.26      0.26        23\n",
      "    Class 35       1.00      0.33      0.50         3\n",
      "    Class 36       0.40      0.40      0.40         5\n",
      "    Class 37       0.00      0.00      0.00         2\n",
      "    Class 38       0.00      0.00      0.00         6\n",
      "    Class 39       0.20      0.30      0.24        10\n",
      "    Class 40       0.78      0.78      0.78        18\n",
      "    Class 41       0.00      0.00      0.00         1\n",
      "    Class 42       0.07      0.12      0.09         8\n",
      "    Class 43       1.00      0.33      0.50         3\n",
      "    Class 44       1.00      0.17      0.29         6\n",
      "    Class 45       1.00      0.43      0.60         7\n",
      "    Class 46       0.50      0.33      0.40         3\n",
      "    Class 47       0.00      0.00      0.00         2\n",
      "    Class 48       0.00      0.00      0.00         1\n",
      "    Class 49       0.00      0.00      0.00         2\n",
      "    Class 50       1.00      1.00      1.00         1\n",
      "    Class 51       0.17      1.00      0.29         2\n",
      "    Class 52       0.00      0.00      0.00         2\n",
      "    Class 53       1.00      0.25      0.40         4\n",
      "    Class 54       1.00      0.17      0.29         6\n",
      "    Class 55       0.00      0.00      0.00         1\n",
      "    Class 56       0.00      0.00      0.00         1\n",
      "    Class 57       0.00      0.00      0.00         5\n",
      "    Class 58       0.00      0.00      0.00         3\n",
      "    Class 59       0.00      0.00      0.00         1\n",
      "    Class 60       1.00      0.30      0.46        10\n",
      "    Class 61       0.00      0.00      0.00         1\n",
      "    Class 62       0.50      0.71      0.59         7\n",
      "    Class 63       1.00      0.31      0.48        16\n",
      "    Class 64       0.00      0.00      0.00         2\n",
      "    Class 65       0.00      0.00      0.00         1\n",
      "    Class 66       1.00      0.33      0.50         3\n",
      "    Class 67       0.00      0.00      0.00         1\n",
      "    Class 68       0.00      0.00      0.00         2\n",
      "    Class 69       1.00      1.00      1.00         1\n",
      "    Class 70       0.00      0.00      0.00         3\n",
      "    Class 71       0.67      1.00      0.80         2\n",
      "    Class 72       0.53      0.89      0.67         9\n",
      "    Class 73       0.00      0.00      0.00         1\n",
      "    Class 74       0.00      0.00      0.00         1\n",
      "    Class 75       0.50      0.12      0.20         8\n",
      "    Class 76       1.00      0.25      0.40         4\n",
      "    Class 77       0.00      0.00      0.00         1\n",
      "    Class 78       0.00      0.00      0.00         1\n",
      "    Class 79       0.00      0.00      0.00         3\n",
      "    Class 80       0.00      0.00      0.00         1\n",
      "    Class 81       0.00      0.00      0.00         4\n",
      "    Class 82       1.00      1.00      1.00         1\n",
      "    Class 83       1.00      0.67      0.80         3\n",
      "    Class 84       0.00      0.00      0.00         2\n",
      "    Class 85       0.00      0.00      0.00         2\n",
      "    Class 86       0.00      0.00      0.00         3\n",
      "    Class 87       0.00      0.00      0.00         2\n",
      "    Class 88       0.00      0.00      0.00         1\n",
      "    Class 89       0.00      0.00      0.00         2\n",
      "    Class 90       0.00      0.00      0.00         1\n",
      "    Class 91       0.00      0.00      0.00         2\n",
      "    Class 92       1.00      0.40      0.57         5\n",
      "    Class 93       0.00      0.00      0.00         2\n",
      "    Class 94       0.00      0.00      0.00         1\n",
      "    Class 95       0.00      0.00      0.00         1\n",
      "    Class 96       0.00      0.00      0.00         1\n",
      "    Class 97       0.00      0.00      0.00         1\n",
      "    Class 98       0.00      0.00      0.00         1\n",
      "    Class 99       0.00      0.00      0.00         1\n",
      "   Class 100       0.00      0.00      0.00         0\n",
      "   Class 101       1.00      1.00      1.00         2\n",
      "   Class 102       0.67      0.80      0.73         5\n",
      "   Class 103       0.00      0.00      0.00         1\n",
      "   Class 104       0.00      0.00      0.00         1\n",
      "   Class 105       1.00      1.00      1.00         9\n",
      "   Class 106       0.00      0.00      0.00         1\n",
      "   Class 107       0.00      0.00      0.00         3\n",
      "   Class 108       0.00      0.00      0.00         2\n",
      "   Class 109       0.00      0.00      0.00         1\n",
      "   Class 110       0.00      0.00      0.00         1\n",
      "   Class 111       0.00      0.00      0.00         1\n",
      "   Class 112       1.00      0.50      0.67         2\n",
      "\n",
      "    accuracy                           0.60      1017\n",
      "   macro avg       0.33      0.25      0.26      1017\n",
      "weighted avg       0.66      0.60      0.60      1017\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#note again that the f1 score report does not include all the classes\n",
    "#it includes only the classes found in the artificial test set \n",
    "\n",
    "create_heatmap_and_error_report(model,X_test,Y_hazard_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ccf0bcf4-ae1e-4d80-bcb7-436384448751",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the validation_file for hazard category and create a prediction vector\n",
    "\n",
    "#load the csv incidents_val into a data frame\n",
    "\n",
    "df_validation=pd.read_csv('incidents_val.csv')\n",
    "#df.head()\n",
    "\n",
    "#isolate the text category which is to be used as the input X to a classifier\n",
    "text_validation=df_validation['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9ae9cd-1e6f-42cc-939d-b39d86db2f14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4e7b84e6-bb59-467a-8d54-162d935f6ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565, 41409)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#follow the same steps as before to vectorize the text_validation \n",
    "#into a form that can be loaded into the trained model\n",
    "\n",
    "#to avoid having different number of features for the train set and the validation set with the tfidf vectorizer we are \n",
    "#going to use the tfidf vectorizer fitted for the train set to the validation set \n",
    "#the function we created previously will not suffise because it will create a unique representation based on the validation set \n",
    "#with number of features different than the ones in the training set\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()  #create a tfidfvectorizer object\n",
    "\n",
    "\n",
    "tfidf_vectorizer.fit(text)        #fit it for the text of the training set \n",
    "                                                   \n",
    "X_center=tfidf_vectorizer.transform(text)      #transform the title of the training set into a format where each entry is a \n",
    "                                                #numeric vector of shape (m,n) where m \n",
    "                                                #is the number of examples in the given set and n is the number of features\n",
    "\n",
    "X_center_format=X_center.toarray()             #follow this conversion to get an array form\n",
    "                                               #\n",
    "\n",
    "\n",
    "X_val_0=tfidf_vectorizer.transform(text_validation)   #use the vectorizer that is fitted to the training set to transform the input \n",
    "                                                       #from the validation set. the result will be a numeric vector of shape (m_val,n)\n",
    "                                                       #where m_val is the number of entries in the validation set and \n",
    "                                                       #n is the number of features which is the same as the number of features \n",
    "                                                       # used for training\n",
    "\n",
    "X_val_1=X_val_0.toarray()\n",
    "\n",
    "X_val_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4ded164-65d4-4ee3-922d-20b341fa314a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b029bc1c-9064-4cb4-a4a6-5e6ab99efdbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n"
     ]
    }
   ],
   "source": [
    "#predictions in text format from the trained dense model named model\n",
    "\n",
    "H_val_prediction=from_pred_and_dictionary_to_labels(X_val_1,model,one_hot_encode_hazard)  #use the aforementioned function\n",
    "                                                       #to predict the Hazard category labels for the validation set using the \n",
    "                                                       #validation title. This function also receives as input the dictionary with \n",
    "                                                       #the labels and their one hot encoding in order to convert the predictions back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "acce04ed-0659-433f-ac92-0c59610d5e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HZ_val_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "837d9ec1-cf93-42d1-bc00-e570a2e13b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are going to follow the exact same procedure and train a seperate model to predict \n",
    "#the product category first for the training set and then for the validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "69cf1482-b6b0-44ae-9918-caf4d50e65de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5082, 1022)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product=df['product'] #get the product  for the data frame refering to the training set\n",
    "unique_product_labels=find_unique_column_labels(product) #find the unique labels of product \n",
    "\n",
    "one_hot_encode_product=one_hot_encode_labels(unique_product_labels) #one hot encode these labels and get a \n",
    "#dictionary with the key being the label and the value being its one hot encoding\n",
    "\n",
    "\n",
    "Y_product_transposed=create_Y_label(product,one_hot_encode_product)\n",
    "#create the Y part of the data set\n",
    "\n",
    "Y_product=Y_product_transposed.T\n",
    "Y_product.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "96b6ab72-305a-42c5-8e55-5990b23e8065",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data intro train and test set\n",
    "#using the processed text data as X\n",
    "#and product category as Y\n",
    "X_train, X_test, Y_product_train, Y_product_test = train_test_split(X, Y_product, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446565fe-1465-47e5-b1ff-7986435ef467",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "57d82a2f-e53e-40a7-9138-1fb63615386c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 120ms/step - accuracy: 0.0292 - loss: 6.6507 - val_accuracy: 0.0541 - val_loss: 6.1162\n",
      "Epoch 2/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 123ms/step - accuracy: 0.0563 - loss: 5.6351 - val_accuracy: 0.0787 - val_loss: 5.8521\n",
      "Epoch 3/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 121ms/step - accuracy: 0.0945 - loss: 5.0109 - val_accuracy: 0.1563 - val_loss: 5.8323\n",
      "Epoch 4/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 119ms/step - accuracy: 0.2301 - loss: 4.1218 - val_accuracy: 0.2311 - val_loss: 5.9065\n",
      "Epoch 5/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 116ms/step - accuracy: 0.4235 - loss: 3.1819 - val_accuracy: 0.2714 - val_loss: 6.1839\n",
      "Epoch 6/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 115ms/step - accuracy: 0.6076 - loss: 2.2946 - val_accuracy: 0.2822 - val_loss: 6.5198\n",
      "Epoch 7/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 115ms/step - accuracy: 0.7196 - loss: 1.6442 - val_accuracy: 0.2999 - val_loss: 6.7535\n",
      "Epoch 8/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 116ms/step - accuracy: 0.8322 - loss: 1.0448 - val_accuracy: 0.3107 - val_loss: 7.4022\n",
      "Epoch 9/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 116ms/step - accuracy: 0.8867 - loss: 0.6727 - val_accuracy: 0.3147 - val_loss: 7.8372\n",
      "Epoch 10/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 116ms/step - accuracy: 0.9376 - loss: 0.3976 - val_accuracy: 0.3117 - val_loss: 8.5035\n",
      "Epoch 11/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 115ms/step - accuracy: 0.9680 - loss: 0.2782 - val_accuracy: 0.2930 - val_loss: 9.0923\n",
      "Epoch 12/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 116ms/step - accuracy: 0.9894 - loss: 0.1207 - val_accuracy: 0.3088 - val_loss: 9.5619\n",
      "Epoch 13/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 116ms/step - accuracy: 0.9943 - loss: 0.0592 - val_accuracy: 0.3048 - val_loss: 9.8873\n",
      "Epoch 14/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 116ms/step - accuracy: 0.9968 - loss: 0.0378 - val_accuracy: 0.3048 - val_loss: 10.1422\n",
      "Epoch 15/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 118ms/step - accuracy: 0.9956 - loss: 0.0313 - val_accuracy: 0.3186 - val_loss: 10.2337\n",
      "Epoch 16/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 117ms/step - accuracy: 0.9952 - loss: 0.0263 - val_accuracy: 0.3196 - val_loss: 10.4615\n",
      "Epoch 17/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 117ms/step - accuracy: 0.9960 - loss: 0.0222 - val_accuracy: 0.3078 - val_loss: 10.4147\n",
      "Epoch 18/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 117ms/step - accuracy: 0.9976 - loss: 0.0164 - val_accuracy: 0.2999 - val_loss: 10.5900\n",
      "Epoch 19/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 117ms/step - accuracy: 0.9980 - loss: 0.0154 - val_accuracy: 0.3117 - val_loss: 10.4465\n",
      "Epoch 20/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 117ms/step - accuracy: 0.9973 - loss: 0.0165 - val_accuracy: 0.3147 - val_loss: 10.7912\n",
      "Epoch 21/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 117ms/step - accuracy: 0.9934 - loss: 0.0222 - val_accuracy: 0.3196 - val_loss: 10.7120\n",
      "Epoch 22/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 121ms/step - accuracy: 0.9968 - loss: 0.0139 - val_accuracy: 0.3117 - val_loss: 10.9167\n",
      "Epoch 23/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 119ms/step - accuracy: 0.9981 - loss: 0.0096 - val_accuracy: 0.3068 - val_loss: 10.8725\n",
      "Epoch 24/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 118ms/step - accuracy: 0.9973 - loss: 0.0130 - val_accuracy: 0.3166 - val_loss: 10.8462\n",
      "Epoch 25/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 118ms/step - accuracy: 0.9975 - loss: 0.0080 - val_accuracy: 0.3127 - val_loss: 10.9994\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.3263 - loss: 10.8245\n",
      "Test Loss: 10.999438285827637\n",
      "Test Accuracy: 0.3126843571662903\n"
     ]
    }
   ],
   "source": [
    "#train a different dense net than before in order to predict the product column of the data frame\n",
    "#1022 is the number of unique categories for product\n",
    "model_2=train_and_eval_dense_nn(X_train,Y_product_train,X_test,Y_product_test,256,128,1022,25,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "3ddd359b-e022-4aab-8e85-84172a9b3cd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1017, 22)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_product_category_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "04e35052-0eee-4837-8600-3dd66740c126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.00      0.00      0.00         4\n",
      "     Class 1       0.00      0.00      0.00         7\n",
      "     Class 2       0.00      0.00      0.00         7\n",
      "     Class 3       0.11      1.00      0.20         1\n",
      "     Class 4       0.42      0.89      0.57         9\n",
      "     Class 5       0.40      0.29      0.33         7\n",
      "     Class 6       0.00      0.00      0.00         1\n",
      "     Class 7       0.00      0.00      0.00         1\n",
      "     Class 8       0.43      0.52      0.47        23\n",
      "     Class 9       0.00      0.00      0.00         1\n",
      "    Class 10       0.80      1.00      0.89         4\n",
      "    Class 11       0.00      0.00      0.00         3\n",
      "    Class 12       0.00      0.00      0.00         2\n",
      "    Class 13       0.00      0.00      0.00         2\n",
      "    Class 14       0.33      0.12      0.18         8\n",
      "    Class 15       0.00      0.00      0.00         3\n",
      "    Class 16       0.35      0.53      0.42        17\n",
      "    Class 17       0.50      0.25      0.33         8\n",
      "    Class 18       0.00      0.00      0.00         1\n",
      "    Class 19       0.33      0.11      0.17         9\n",
      "    Class 20       0.88      0.76      0.82        50\n",
      "    Class 21       1.00      0.50      0.67         2\n",
      "    Class 22       0.00      0.00      0.00         6\n",
      "    Class 23       0.00      0.00      0.00         1\n",
      "    Class 24       0.50      0.33      0.40         3\n",
      "    Class 25       0.20      1.00      0.33         2\n",
      "    Class 26       0.67      0.77      0.71        13\n",
      "    Class 27       0.23      0.60      0.33        10\n",
      "    Class 28       0.00      0.00      0.00         0\n",
      "    Class 29       0.00      0.00      0.00         2\n",
      "    Class 30       0.00      0.00      0.00         1\n",
      "    Class 31       0.00      0.00      0.00         1\n",
      "    Class 32       0.00      0.00      0.00         2\n",
      "    Class 33       0.50      0.20      0.29         5\n",
      "    Class 34       0.80      0.80      0.80         5\n",
      "    Class 35       0.36      0.36      0.36        11\n",
      "    Class 36       0.00      0.00      0.00         1\n",
      "    Class 37       0.00      0.00      0.00         1\n",
      "    Class 38       0.50      0.71      0.59         7\n",
      "    Class 39       0.12      1.00      0.22         1\n",
      "    Class 40       0.00      0.00      0.00         3\n",
      "    Class 41       0.00      0.00      0.00         2\n",
      "    Class 42       0.05      0.21      0.08        14\n",
      "    Class 43       0.25      0.20      0.22         5\n",
      "    Class 44       0.40      0.40      0.40         5\n",
      "    Class 45       0.00      0.00      0.00         2\n",
      "    Class 46       0.30      0.60      0.40         5\n",
      "    Class 47       1.00      0.20      0.33         5\n",
      "    Class 48       0.00      0.00      0.00         8\n",
      "    Class 49       0.50      0.50      0.50         2\n",
      "    Class 50       0.00      0.00      0.00         2\n",
      "    Class 51       0.54      0.88      0.67         8\n",
      "    Class 52       0.00      0.00      0.00         1\n",
      "    Class 53       1.00      1.00      1.00         1\n",
      "    Class 54       0.00      0.00      0.00         1\n",
      "    Class 55       0.00      0.00      0.00         1\n",
      "    Class 56       0.00      0.00      0.00         6\n",
      "    Class 57       0.00      0.00      0.00         4\n",
      "    Class 58       0.00      0.00      0.00         3\n",
      "    Class 59       0.00      0.00      0.00         1\n",
      "    Class 60       0.56      0.62      0.59         8\n",
      "    Class 61       0.25      1.00      0.40         1\n",
      "    Class 62       0.00      0.00      0.00         0\n",
      "    Class 63       1.00      0.75      0.86         4\n",
      "    Class 64       0.62      0.71      0.67         7\n",
      "    Class 65       0.22      0.67      0.33         6\n",
      "    Class 66       1.00      1.00      1.00         1\n",
      "    Class 67       0.00      0.00      0.00         1\n",
      "    Class 68       0.00      0.00      0.00         2\n",
      "    Class 69       0.00      0.00      0.00         1\n",
      "    Class 70       1.00      0.50      0.67         2\n",
      "    Class 71       0.00      0.00      0.00         2\n",
      "    Class 72       0.17      0.40      0.24         5\n",
      "    Class 73       0.00      0.00      0.00         1\n",
      "    Class 74       0.00      0.00      0.00         1\n",
      "    Class 75       0.00      0.00      0.00         2\n",
      "    Class 76       0.00      0.00      0.00         1\n",
      "    Class 77       0.00      0.00      0.00         2\n",
      "    Class 78       0.50      0.25      0.33         4\n",
      "    Class 79       0.50      1.00      0.67         1\n",
      "    Class 80       1.00      1.00      1.00         1\n",
      "    Class 81       0.67      0.67      0.67         9\n",
      "    Class 82       0.00      0.00      0.00         0\n",
      "    Class 83       0.00      0.00      0.00         1\n",
      "    Class 84       0.33      0.33      0.33         3\n",
      "    Class 85       0.00      0.00      0.00         2\n",
      "    Class 86       0.00      0.00      0.00         1\n",
      "    Class 87       0.00      0.00      0.00         0\n",
      "    Class 88       0.00      0.00      0.00         1\n",
      "    Class 89       0.00      0.00      0.00         2\n",
      "    Class 90       0.00      0.00      0.00         1\n",
      "    Class 91       0.00      0.00      0.00         4\n",
      "    Class 92       0.00      0.00      0.00         1\n",
      "    Class 93       0.00      0.00      0.00         1\n",
      "    Class 94       0.00      0.00      0.00         0\n",
      "    Class 95       0.00      0.00      0.00         1\n",
      "    Class 96       0.00      0.00      0.00         0\n",
      "    Class 97       1.00      0.33      0.50         3\n",
      "    Class 98       0.46      0.50      0.48        12\n",
      "    Class 99       0.00      0.00      0.00         1\n",
      "   Class 100       0.00      0.00      0.00         0\n",
      "   Class 101       0.00      0.00      0.00         0\n",
      "   Class 102       0.00      0.00      0.00         1\n",
      "   Class 103       0.00      0.00      0.00         1\n",
      "   Class 104       1.00      1.00      1.00         3\n",
      "   Class 105       0.00      0.00      0.00         1\n",
      "   Class 106       1.00      0.86      0.92         7\n",
      "   Class 107       0.00      0.00      0.00         2\n",
      "   Class 108       0.00      0.00      0.00         3\n",
      "   Class 109       0.33      0.25      0.29         4\n",
      "   Class 110       0.00      0.00      0.00         2\n",
      "   Class 111       0.00      0.00      0.00         3\n",
      "   Class 112       1.00      1.00      1.00         1\n",
      "   Class 113       0.00      0.00      0.00         5\n",
      "   Class 114       0.00      0.00      0.00         1\n",
      "   Class 115       0.50      0.40      0.44         5\n",
      "   Class 116       0.00      0.00      0.00         1\n",
      "   Class 117       0.29      1.00      0.44         2\n",
      "   Class 118       0.00      0.00      0.00         1\n",
      "   Class 119       0.00      0.00      0.00         0\n",
      "   Class 120       0.00      0.00      0.00         2\n",
      "   Class 121       1.00      1.00      1.00         1\n",
      "   Class 122       0.00      0.00      0.00         0\n",
      "   Class 123       0.00      0.00      0.00         2\n",
      "   Class 124       0.36      0.57      0.44         7\n",
      "   Class 125       0.00      0.00      0.00         1\n",
      "   Class 126       0.20      0.17      0.18         6\n",
      "   Class 127       0.00      0.00      0.00         2\n",
      "   Class 128       0.00      0.00      0.00         1\n",
      "   Class 129       0.00      0.00      0.00         1\n",
      "   Class 130       0.00      0.00      0.00         0\n",
      "   Class 131       0.11      0.50      0.18         2\n",
      "   Class 132       0.00      0.00      0.00         3\n",
      "   Class 133       0.80      0.57      0.67         7\n",
      "   Class 134       0.00      0.00      0.00         1\n",
      "   Class 135       0.00      0.00      0.00         0\n",
      "   Class 136       0.57      0.57      0.57         7\n",
      "   Class 137       1.00      1.00      1.00         2\n",
      "   Class 138       0.50      0.50      0.50         2\n",
      "   Class 139       0.71      0.56      0.62         9\n",
      "   Class 140       0.17      0.50      0.25         2\n",
      "   Class 141       0.00      0.00      0.00         1\n",
      "   Class 142       0.00      0.00      0.00         3\n",
      "   Class 143       0.75      0.75      0.75         4\n",
      "   Class 144       0.00      0.00      0.00         2\n",
      "   Class 145       0.00      0.00      0.00         1\n",
      "   Class 146       1.00      1.00      1.00         1\n",
      "   Class 147       0.00      0.00      0.00         1\n",
      "   Class 148       0.00      0.00      0.00         6\n",
      "   Class 149       1.00      1.00      1.00         1\n",
      "   Class 150       0.17      0.25      0.20         4\n",
      "   Class 151       0.00      0.00      0.00         1\n",
      "   Class 152       0.43      0.75      0.55         4\n",
      "   Class 153       0.75      0.33      0.46         9\n",
      "   Class 154       0.00      0.00      0.00         2\n",
      "   Class 155       1.00      1.00      1.00         1\n",
      "   Class 156       0.50      1.00      0.67         1\n",
      "   Class 157       0.00      0.00      0.00         0\n",
      "   Class 158       0.00      0.00      0.00         1\n",
      "   Class 159       0.00      0.00      0.00         1\n",
      "   Class 160       0.00      0.00      0.00         1\n",
      "   Class 161       0.00      0.00      0.00         5\n",
      "   Class 162       0.40      0.67      0.50         3\n",
      "   Class 163       0.00      0.00      0.00         6\n",
      "   Class 164       0.25      1.00      0.40         1\n",
      "   Class 165       0.00      0.00      0.00         2\n",
      "   Class 166       0.00      0.00      0.00         1\n",
      "   Class 167       0.00      0.00      0.00         1\n",
      "   Class 168       0.00      0.00      0.00         1\n",
      "   Class 169       0.00      0.00      0.00         3\n",
      "   Class 170       1.00      0.50      0.67         2\n",
      "   Class 171       0.00      0.00      0.00         1\n",
      "   Class 172       0.00      0.00      0.00         3\n",
      "   Class 173       0.00      0.00      0.00         3\n",
      "   Class 174       0.29      0.67      0.40         3\n",
      "   Class 175       0.00      0.00      0.00         1\n",
      "   Class 176       0.00      0.00      0.00         4\n",
      "   Class 177       0.00      0.00      0.00         0\n",
      "   Class 178       0.00      0.00      0.00         0\n",
      "   Class 179       0.00      0.00      0.00         0\n",
      "   Class 180       0.57      0.67      0.62         6\n",
      "   Class 181       0.00      0.00      0.00         3\n",
      "   Class 182       0.00      0.00      0.00         0\n",
      "   Class 183       0.00      0.00      0.00         1\n",
      "   Class 184       0.00      0.00      0.00         1\n",
      "   Class 185       0.00      0.00      0.00         4\n",
      "   Class 186       0.00      0.00      0.00         1\n",
      "   Class 187       0.00      0.00      0.00         0\n",
      "   Class 188       0.00      0.00      0.00         0\n",
      "   Class 189       0.00      0.00      0.00         1\n",
      "   Class 190       0.25      0.25      0.25         4\n",
      "   Class 191       0.00      0.00      0.00         2\n",
      "   Class 192       0.00      0.00      0.00         2\n",
      "   Class 193       0.00      0.00      0.00         1\n",
      "   Class 194       0.00      0.00      0.00         0\n",
      "   Class 195       0.60      0.60      0.60         5\n",
      "   Class 196       0.00      0.00      0.00         1\n",
      "   Class 197       0.50      0.50      0.50         2\n",
      "   Class 198       0.00      0.00      0.00         1\n",
      "   Class 199       0.00      0.00      0.00         2\n",
      "   Class 200       0.00      0.00      0.00         2\n",
      "   Class 201       0.67      0.57      0.62         7\n",
      "   Class 202       1.00      1.00      1.00         1\n",
      "   Class 203       1.00      0.50      0.67         2\n",
      "   Class 204       0.00      0.00      0.00         1\n",
      "   Class 205       0.00      0.00      0.00         2\n",
      "   Class 206       0.00      0.00      0.00         1\n",
      "   Class 207       1.00      1.00      1.00         1\n",
      "   Class 208       0.00      0.00      0.00         1\n",
      "   Class 209       0.00      0.00      0.00         1\n",
      "   Class 210       0.00      0.00      0.00         1\n",
      "   Class 211       0.00      0.00      0.00         1\n",
      "   Class 212       0.00      0.00      0.00         1\n",
      "   Class 213       0.00      0.00      0.00         2\n",
      "   Class 214       0.00      0.00      0.00         3\n",
      "   Class 215       0.33      0.20      0.25         5\n",
      "   Class 216       0.00      0.00      0.00         2\n",
      "   Class 217       0.00      0.00      0.00         4\n",
      "   Class 218       0.00      0.00      0.00         0\n",
      "   Class 219       0.00      0.00      0.00         1\n",
      "   Class 220       0.00      0.00      0.00         0\n",
      "   Class 221       0.00      0.00      0.00         1\n",
      "   Class 222       0.00      0.00      0.00         1\n",
      "   Class 223       0.33      0.33      0.33         3\n",
      "   Class 224       0.00      0.00      0.00         1\n",
      "   Class 225       0.00      0.00      0.00         2\n",
      "   Class 226       0.00      0.00      0.00         1\n",
      "   Class 227       0.00      0.00      0.00         0\n",
      "   Class 228       0.00      0.00      0.00         0\n",
      "   Class 229       0.00      0.00      0.00         0\n",
      "   Class 230       0.00      0.00      0.00         1\n",
      "   Class 231       1.00      0.50      0.67         4\n",
      "   Class 232       0.33      0.33      0.33         3\n",
      "   Class 233       0.00      0.00      0.00         2\n",
      "   Class 234       0.00      0.00      0.00         2\n",
      "   Class 235       0.00      0.00      0.00         1\n",
      "   Class 236       0.00      0.00      0.00         1\n",
      "   Class 237       1.00      1.00      1.00         1\n",
      "   Class 238       0.00      0.00      0.00         1\n",
      "   Class 239       0.00      0.00      0.00         2\n",
      "   Class 240       1.00      1.00      1.00         1\n",
      "   Class 241       1.00      0.50      0.67         4\n",
      "   Class 242       0.00      0.00      0.00         0\n",
      "   Class 243       0.00      0.00      0.00         0\n",
      "   Class 244       0.00      0.00      0.00         2\n",
      "   Class 245       0.00      0.00      0.00         2\n",
      "   Class 246       0.00      0.00      0.00         1\n",
      "   Class 247       1.00      1.00      1.00         1\n",
      "   Class 248       0.00      0.00      0.00         0\n",
      "   Class 249       0.00      0.00      0.00         2\n",
      "   Class 250       0.00      0.00      0.00         0\n",
      "   Class 251       0.00      0.00      0.00         0\n",
      "   Class 252       0.00      0.00      0.00         1\n",
      "   Class 253       0.00      0.00      0.00         1\n",
      "   Class 254       0.00      0.00      0.00         1\n",
      "   Class 255       0.00      0.00      0.00         0\n",
      "   Class 256       1.00      1.00      1.00         1\n",
      "   Class 257       0.00      0.00      0.00         1\n",
      "   Class 258       0.00      0.00      0.00         2\n",
      "   Class 259       0.00      0.00      0.00         1\n",
      "   Class 260       0.00      0.00      0.00         1\n",
      "   Class 261       0.00      0.00      0.00         1\n",
      "   Class 262       0.00      0.00      0.00         1\n",
      "   Class 263       0.00      0.00      0.00         1\n",
      "   Class 264       0.75      0.75      0.75         4\n",
      "   Class 265       0.00      0.00      0.00         1\n",
      "   Class 266       0.00      0.00      0.00         0\n",
      "   Class 267       0.00      0.00      0.00         1\n",
      "   Class 268       0.50      0.33      0.40         3\n",
      "   Class 269       1.00      1.00      1.00         1\n",
      "   Class 270       0.00      0.00      0.00         2\n",
      "   Class 271       0.00      0.00      0.00         1\n",
      "   Class 272       0.00      0.00      0.00         2\n",
      "   Class 273       0.17      0.33      0.22         3\n",
      "   Class 274       0.00      0.00      0.00         0\n",
      "   Class 275       0.00      0.00      0.00         1\n",
      "   Class 276       0.00      0.00      0.00         1\n",
      "   Class 277       0.00      0.00      0.00         1\n",
      "   Class 278       0.00      0.00      0.00         0\n",
      "   Class 279       0.00      0.00      0.00         1\n",
      "   Class 280       0.00      0.00      0.00         1\n",
      "   Class 281       0.00      0.00      0.00         1\n",
      "   Class 282       0.00      0.00      0.00         2\n",
      "   Class 283       0.00      0.00      0.00         2\n",
      "   Class 284       0.00      0.00      0.00         1\n",
      "   Class 285       0.00      0.00      0.00         1\n",
      "   Class 286       0.50      0.50      0.50         2\n",
      "   Class 287       0.50      0.67      0.57         3\n",
      "   Class 288       0.00      0.00      0.00         1\n",
      "   Class 289       0.00      0.00      0.00         1\n",
      "   Class 290       0.00      0.00      0.00         1\n",
      "   Class 291       0.00      0.00      0.00         1\n",
      "   Class 292       0.75      0.75      0.75         4\n",
      "   Class 293       0.00      0.00      0.00         1\n",
      "   Class 294       0.00      0.00      0.00         0\n",
      "   Class 295       1.00      1.00      1.00         2\n",
      "   Class 296       1.00      1.00      1.00         1\n",
      "   Class 297       0.00      0.00      0.00         0\n",
      "   Class 298       0.00      0.00      0.00         1\n",
      "   Class 299       0.00      0.00      0.00         1\n",
      "   Class 300       0.00      0.00      0.00         1\n",
      "   Class 301       0.00      0.00      0.00         2\n",
      "   Class 302       0.00      0.00      0.00         1\n",
      "   Class 303       0.00      0.00      0.00         0\n",
      "   Class 304       0.00      0.00      0.00         1\n",
      "   Class 305       0.00      0.00      0.00         2\n",
      "   Class 306       0.00      0.00      0.00         1\n",
      "   Class 307       1.00      1.00      1.00         1\n",
      "   Class 308       0.00      0.00      0.00         1\n",
      "   Class 309       0.00      0.00      0.00         2\n",
      "   Class 310       0.00      0.00      0.00         1\n",
      "   Class 311       0.00      0.00      0.00         1\n",
      "   Class 312       0.00      0.00      0.00         1\n",
      "   Class 313       0.00      0.00      0.00         0\n",
      "   Class 314       1.00      1.00      1.00         1\n",
      "   Class 315       0.00      0.00      0.00         1\n",
      "   Class 316       0.00      0.00      0.00         1\n",
      "   Class 317       0.50      1.00      0.67         1\n",
      "   Class 318       1.00      0.50      0.67         2\n",
      "   Class 319       0.00      0.00      0.00         1\n",
      "   Class 320       0.00      0.00      0.00         2\n",
      "   Class 321       0.00      0.00      0.00         1\n",
      "   Class 322       0.00      0.00      0.00         1\n",
      "   Class 323       0.00      0.00      0.00         2\n",
      "   Class 324       0.00      0.00      0.00         1\n",
      "   Class 325       0.00      0.00      0.00         1\n",
      "   Class 326       0.00      0.00      0.00         1\n",
      "   Class 327       0.00      0.00      0.00         1\n",
      "   Class 328       0.00      0.00      0.00         1\n",
      "   Class 329       0.00      0.00      0.00         1\n",
      "   Class 330       0.00      0.00      0.00         1\n",
      "   Class 331       1.00      1.00      1.00         1\n",
      "   Class 332       0.00      0.00      0.00         1\n",
      "   Class 333       0.00      0.00      0.00         1\n",
      "   Class 334       0.50      1.00      0.67         1\n",
      "   Class 335       0.00      0.00      0.00         1\n",
      "   Class 336       0.00      0.00      0.00         1\n",
      "   Class 337       0.00      0.00      0.00         2\n",
      "   Class 338       0.50      1.00      0.67         1\n",
      "   Class 339       0.00      0.00      0.00         0\n",
      "   Class 340       0.00      0.00      0.00         1\n",
      "   Class 341       1.00      0.33      0.50         3\n",
      "   Class 342       0.00      0.00      0.00         1\n",
      "   Class 343       0.00      0.00      0.00         1\n",
      "   Class 344       0.00      0.00      0.00         1\n",
      "   Class 345       0.00      0.00      0.00         1\n",
      "   Class 346       0.00      0.00      0.00         0\n",
      "   Class 347       0.00      0.00      0.00         1\n",
      "   Class 348       0.00      0.00      0.00         1\n",
      "   Class 349       0.00      0.00      0.00         1\n",
      "   Class 350       0.33      0.25      0.29         4\n",
      "   Class 351       0.00      0.00      0.00         1\n",
      "   Class 352       0.00      0.00      0.00         1\n",
      "   Class 353       0.00      0.00      0.00         1\n",
      "   Class 354       0.00      0.00      0.00         1\n",
      "   Class 355       0.00      0.00      0.00         1\n",
      "   Class 356       0.00      0.00      0.00         1\n",
      "   Class 357       0.00      0.00      0.00         1\n",
      "   Class 358       0.00      0.00      0.00         1\n",
      "   Class 359       0.00      0.00      0.00         1\n",
      "   Class 360       0.00      0.00      0.00         1\n",
      "   Class 361       0.00      0.00      0.00         1\n",
      "   Class 362       0.00      0.00      0.00         1\n",
      "   Class 363       0.50      0.14      0.22         7\n",
      "   Class 364       0.00      0.00      0.00         1\n",
      "   Class 365       0.00      0.00      0.00         1\n",
      "   Class 366       0.00      0.00      0.00         1\n",
      "   Class 367       0.00      0.00      0.00         0\n",
      "   Class 368       0.00      0.00      0.00         0\n",
      "   Class 369       1.00      1.00      1.00         1\n",
      "   Class 370       0.00      0.00      0.00         1\n",
      "   Class 371       1.00      1.00      1.00         1\n",
      "   Class 372       0.00      0.00      0.00         1\n",
      "   Class 373       0.00      0.00      0.00         1\n",
      "   Class 374       0.00      0.00      0.00         1\n",
      "   Class 375       0.00      0.00      0.00         0\n",
      "   Class 376       1.00      0.60      0.75         5\n",
      "   Class 377       0.00      0.00      0.00         1\n",
      "   Class 378       0.00      0.00      0.00         1\n",
      "   Class 379       0.00      0.00      0.00         0\n",
      "   Class 380       0.00      0.00      0.00         0\n",
      "   Class 381       0.00      0.00      0.00         0\n",
      "   Class 382       0.00      0.00      0.00         1\n",
      "   Class 383       0.00      0.00      0.00         0\n",
      "   Class 384       0.00      0.00      0.00         1\n",
      "   Class 385       0.00      0.00      0.00         1\n",
      "   Class 386       1.00      0.50      0.67         2\n",
      "   Class 387       1.00      0.50      0.67         2\n",
      "   Class 388       0.00      0.00      0.00         1\n",
      "   Class 389       0.00      0.00      0.00         1\n",
      "   Class 390       1.00      0.40      0.57         5\n",
      "   Class 391       1.00      0.33      0.50         3\n",
      "   Class 392       1.00      1.00      1.00         1\n",
      "   Class 393       0.00      0.00      0.00         1\n",
      "   Class 394       0.00      0.00      0.00         2\n",
      "   Class 395       0.00      0.00      0.00         0\n",
      "   Class 396       0.00      0.00      0.00         0\n",
      "   Class 397       0.00      0.00      0.00         1\n",
      "   Class 398       0.00      0.00      0.00         0\n",
      "   Class 399       0.00      0.00      0.00         1\n",
      "   Class 400       0.00      0.00      0.00         1\n",
      "   Class 401       0.00      0.00      0.00         1\n",
      "   Class 402       0.00      0.00      0.00         2\n",
      "   Class 403       0.00      0.00      0.00         1\n",
      "   Class 404       1.00      1.00      1.00         2\n",
      "   Class 405       0.00      0.00      0.00         0\n",
      "   Class 406       0.67      1.00      0.80         6\n",
      "   Class 407       0.00      0.00      0.00         1\n",
      "   Class 408       0.00      0.00      0.00         1\n",
      "   Class 409       0.00      0.00      0.00         1\n",
      "   Class 410       0.00      0.00      0.00         1\n",
      "   Class 411       0.00      0.00      0.00         1\n",
      "   Class 412       0.00      0.00      0.00         1\n",
      "   Class 413       0.00      0.00      0.00         0\n",
      "   Class 414       0.00      0.00      0.00         1\n",
      "   Class 415       0.00      0.00      0.00         0\n",
      "   Class 416       0.00      0.00      0.00         1\n",
      "   Class 417       1.00      0.67      0.80         3\n",
      "   Class 418       0.00      0.00      0.00         1\n",
      "   Class 419       0.00      0.00      0.00         1\n",
      "   Class 420       0.00      0.00      0.00         1\n",
      "   Class 421       0.50      0.50      0.50         2\n",
      "   Class 422       0.00      0.00      0.00         2\n",
      "   Class 423       0.00      0.00      0.00         1\n",
      "   Class 424       0.00      0.00      0.00         0\n",
      "   Class 425       0.00      0.00      0.00         0\n",
      "   Class 426       0.00      0.00      0.00         1\n",
      "   Class 427       0.00      0.00      0.00         1\n",
      "   Class 428       0.00      0.00      0.00         1\n",
      "   Class 429       0.00      0.00      0.00         1\n",
      "   Class 430       0.00      0.00      0.00         1\n",
      "   Class 431       0.00      0.00      0.00         1\n",
      "   Class 432       0.00      0.00      0.00         1\n",
      "   Class 433       0.00      0.00      0.00         1\n",
      "   Class 434       0.00      0.00      0.00         1\n",
      "   Class 435       0.00      0.00      0.00         1\n",
      "   Class 436       0.00      0.00      0.00         1\n",
      "   Class 437       0.00      0.00      0.00         1\n",
      "   Class 438       0.00      0.00      0.00         1\n",
      "   Class 439       0.00      0.00      0.00         1\n",
      "   Class 440       0.00      0.00      0.00         1\n",
      "   Class 441       0.00      0.00      0.00         1\n",
      "   Class 442       0.00      0.00      0.00         0\n",
      "   Class 443       0.00      0.00      0.00         1\n",
      "   Class 444       0.00      0.00      0.00         1\n",
      "   Class 445       0.80      1.00      0.89         4\n",
      "   Class 446       0.00      0.00      0.00         2\n",
      "   Class 447       0.00      0.00      0.00         1\n",
      "   Class 448       0.00      0.00      0.00         0\n",
      "   Class 449       1.00      0.50      0.67         2\n",
      "   Class 450       0.00      0.00      0.00         1\n",
      "   Class 451       0.00      0.00      0.00         3\n",
      "   Class 452       0.00      0.00      0.00         1\n",
      "   Class 453       1.00      1.00      1.00         1\n",
      "   Class 454       0.00      0.00      0.00         1\n",
      "   Class 455       0.00      0.00      0.00         0\n",
      "   Class 456       0.00      0.00      0.00         1\n",
      "   Class 457       1.00      1.00      1.00         1\n",
      "   Class 458       0.00      0.00      0.00         1\n",
      "   Class 459       0.00      0.00      0.00         1\n",
      "   Class 460       0.00      0.00      0.00         1\n",
      "   Class 461       1.00      1.00      1.00         1\n",
      "   Class 462       0.00      0.00      0.00         0\n",
      "   Class 463       0.00      0.00      0.00         0\n",
      "   Class 464       0.00      0.00      0.00         1\n",
      "   Class 465       0.00      0.00      0.00         1\n",
      "   Class 466       0.00      0.00      0.00         1\n",
      "   Class 467       0.00      0.00      0.00         1\n",
      "   Class 468       0.00      0.00      0.00         1\n",
      "   Class 469       0.00      0.00      0.00         1\n",
      "   Class 470       0.00      0.00      0.00         1\n",
      "   Class 471       1.00      1.00      1.00         1\n",
      "   Class 472       0.00      0.00      0.00         1\n",
      "   Class 473       0.00      0.00      0.00         1\n",
      "   Class 474       0.00      0.00      0.00         1\n",
      "   Class 475       0.00      0.00      0.00         1\n",
      "   Class 476       0.00      0.00      0.00         1\n",
      "   Class 477       0.00      0.00      0.00         1\n",
      "   Class 478       0.00      0.00      0.00         1\n",
      "   Class 479       0.00      0.00      0.00         1\n",
      "   Class 480       0.00      0.00      0.00         2\n",
      "   Class 481       0.00      0.00      0.00         1\n",
      "   Class 482       0.00      0.00      0.00         1\n",
      "   Class 483       0.00      0.00      0.00         1\n",
      "   Class 484       0.00      0.00      0.00         1\n",
      "   Class 485       0.00      0.00      0.00         1\n",
      "   Class 486       0.00      0.00      0.00         0\n",
      "   Class 487       1.00      1.00      1.00         1\n",
      "   Class 488       0.50      1.00      0.67         1\n",
      "   Class 489       0.00      0.00      0.00         1\n",
      "   Class 490       0.00      0.00      0.00         1\n",
      "   Class 491       0.00      0.00      0.00         1\n",
      "   Class 492       1.00      1.00      1.00         1\n",
      "   Class 493       0.00      0.00      0.00         1\n",
      "   Class 494       0.00      0.00      0.00         1\n",
      "   Class 495       0.00      0.00      0.00         1\n",
      "   Class 496       0.00      0.00      0.00         1\n",
      "   Class 497       0.00      0.00      0.00         1\n",
      "   Class 498       0.00      0.00      0.00         2\n",
      "   Class 499       0.00      0.00      0.00         1\n",
      "   Class 500       0.00      0.00      0.00         1\n",
      "   Class 501       0.00      0.00      0.00         1\n",
      "   Class 502       0.67      1.00      0.80         2\n",
      "   Class 503       0.00      0.00      0.00         1\n",
      "   Class 504       0.00      0.00      0.00         1\n",
      "   Class 505       0.00      0.00      0.00         0\n",
      "   Class 506       0.00      0.00      0.00         1\n",
      "   Class 507       0.00      0.00      0.00         1\n",
      "   Class 508       0.00      0.00      0.00         0\n",
      "   Class 509       0.00      0.00      0.00         1\n",
      "   Class 510       0.00      0.00      0.00         1\n",
      "   Class 511       0.00      0.00      0.00         1\n",
      "   Class 512       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.31      1017\n",
      "   macro avg       0.17      0.17      0.16      1017\n",
      "weighted avg       0.32      0.31      0.30      1017\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_heatmap_and_error_report(model_2,X_test,Y_product_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfdf8cd-67f1-4a3c-8e55-df1e37ff1a0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "572f8322-2b9f-4807-9b1f-ba8f06fa6901",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565, 41409)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#follow the same steps as before to vectorize the text_validation \n",
    "#into a form that can be loaded into the trained model\n",
    "\n",
    "#to avoid having different number of features for the train set and the validation set with the tfidf vectorizer we are \n",
    "#going to use the tfidf vectorizer fitted for the train set to the validation set \n",
    "#the function we created previously will not suffise because it will create a unique representation based on the validation set \n",
    "#with number of features different than the ones in the training set\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()  #create a tfidfvectorizer object\n",
    "\n",
    "\n",
    "tfidf_vectorizer.fit(text)        #fit it for the title of the training set \n",
    "                                                   \n",
    "X_center=tfidf_vectorizer.transform(text)      #transform the title of the training set into a format where each entry is a \n",
    "                                                #numeric vector of shape (m,n) where m \n",
    "                                                #is the number of examples in the given set and n is the number of features\n",
    "\n",
    "X_center_format=X_center.toarray()             #follow this conversion to get an array form\n",
    "                                               #\n",
    "\n",
    "\n",
    "X_val_0=tfidf_vectorizer.transform(text_validation)   #use the vectorizer that is fitted to the training set to transform the input \n",
    "                                                       #from the validation set. the result will be a numeric vector of shape (m_val,n)\n",
    "                                                       #where m_val is the number of entries in the validation set and \n",
    "                                                       #n is the number of features which is the same as the number of features \n",
    "                                                       # used for training\n",
    "\n",
    "X_val_1=X_val_0.toarray()\n",
    "\n",
    "X_val_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd5612b-fced-475d-b74d-f936dcd8ae07",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c47b72c4-6381-44c4-b542-5d0a38ccea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
     ]
    }
   ],
   "source": [
    "#predictions in text format from the trained dense model named model\n",
    "\n",
    "P_val_prediction=from_pred_and_dictionary_to_labels(X_val_1,model_2,one_hot_encode_product)  #use the aforementioned function\n",
    "                                                       #to predict the product labels for the validation set using the \n",
    "                                                       #validation title. This function also receives as input the dictionary with \n",
    "                                                       #the labels and their one hot encoding in order to convert the predictions back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f2ca71fc-5bec-468b-87ef-e8e75c688518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PZ_val_prediction\n",
    "\n",
    "len(H_val_prediction)\n",
    "len(P_val_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "fc1a95dd-3dce-481d-86b1-e05dc474292a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn the predictions into columns of a new dataframe and save them to a csv file\n",
    "predicted_data_final={\n",
    "'hazard': H_val_prediction,\n",
    "'product': P_val_prediction\n",
    "    \n",
    "}\n",
    "df_final=pd.DataFrame(predicted_data_final)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_final.to_csv('st2_text_dense.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345e5f8b-0d72-4228-86ca-106718610afe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
