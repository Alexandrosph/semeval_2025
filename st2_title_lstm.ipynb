{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5411d694-03b7-415f-8862-215b5d008f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the necessary libraries\n",
    "import pandas as pd \n",
    "import numpy as  np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#import all the necessary libraries to build a neural network classifier from tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential                   \n",
    "from tensorflow.keras.layers import Dense                        #import fully connected neural net layers\n",
    "from tensorflow.keras.optimizers import Adam                     #choose adam as the optimization algorithm\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy      #cost function needed for softmax classification if the labels are \n",
    "                                                                 #one hot encoded\n",
    "\n",
    "from tensorflow.keras.regularizers import l2                     #add regularization in order to avoid overfitting \n",
    "\n",
    "from sklearn.model_selection import train_test_split             #to split the training set into train and test set \n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report  #to proceed with error analysis on our predictions\n",
    "\n",
    "\n",
    "import seaborn as sns                                                 #to visualize confusion matrices as a heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dropout              #import long-short term memory neural net layers\n",
    "                                                               #and dropout regularization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4faae7-d74e-4d58-a88f-82dcaaf5f318",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0bb3cd93-191d-485d-beb0-be3c3e3f8373",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  vectorize_string_csv_column_TF_idf(string_column):  \n",
    "                                                #create a function that receives a column of strings from a csv file \n",
    "                                                  #and converts each entry into a unique tfidf vector depending on the \n",
    "                                                  #unique vocabulary\n",
    "                                                   \n",
    "                                                 #necessary to convert a text input into a numeric vector to be used \n",
    "                                                 #as input to the classifier\n",
    "\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()  \n",
    "                                      #create a TF_idf_vectorizer model that will receive the entire csv column\n",
    "                                      #and will eventually turn each entry into a numeric vector\n",
    "                                      #the length of each vector will be the number of the unique words in the vocabulary\n",
    "                                      #if this length=N the resulting vectors for each entry will be of dimension (N,)\n",
    "    \n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(string_column)  \n",
    "                                                    #apply the created model to the column title of the csv\n",
    "\n",
    "\n",
    "    tfidf_vectors = tfidf_matrix.toarray()          #each entry of the tfidf_vectors is a numeric vector\n",
    "                                                    #corresponding to a title entry\n",
    "                                                    #convert it to an array format for ease of handling\n",
    "\n",
    "    Vocabulary=tfidf_vectorizer.get_feature_names_out()  #export the unique vocabulary out of the created vectorizer model\n",
    "\n",
    "    return tfidf_vectors, Vocabulary   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331cfc2e-51fa-4567-b682-2f457c7347e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e486934-6784-4bc6-aaec-89c551c0327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that finds all the unique labels in a column \n",
    "def find_unique_column_labels(column):\n",
    "     \n",
    "    unique_labels_list=[]               #create a list that will contain all the unique labels found in the input column\n",
    "\n",
    "    for i in column : #search the entire column\n",
    "        if i not in unique_labels_list:   #if the element i is not found in the unique list \n",
    "            unique_labels_list.append(i)  #append it to the list\n",
    "    \n",
    "    return  unique_labels_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a2ffda-775a-4e93-81ea-7c56ac1703cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a54b92d-250b-4ed2-8e26-4b3d2168ee43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_labels(unique_labels_of_a_column):\n",
    "    #create a dictionary for these categories that coresponds each one into a one hot numpy vector\n",
    "    #this is necessary in order to use a softmax classifier \n",
    "\n",
    "    number_of_classes=len(unique_labels_of_a_column)  #find the number of classes/possible labels from the vector that contains the unique labels\n",
    "\n",
    "                                               #create a numpy I matrix I lxl where l is the number of classes \n",
    "                                               #each row of the I matrix will correspond to a one hot encoding for each label\n",
    "    I=np.eye(number_of_classes)\n",
    "\n",
    "    #create a dictionary to correspond each class name with it's one hot encoded label\n",
    "\n",
    "    dict_labels={} #initialize an empty dictionary where the keys will be the labels and the values will be their one hot encoding\n",
    "\n",
    "\n",
    "    for i in range(number_of_classes):\n",
    "        dict_labels[unique_labels_of_a_column[i]]=I[i,:]\n",
    "\n",
    "    return dict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb804f80-ec2b-4325-b1ed-5605ac269bf7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1668ce71-b623-4290-a484-9c74f21290b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Y_label(initial_label_column,dict_labels):\n",
    "    #create the Y part of the dataset by receiving  a column  and the dict_labels corresponding to that column\n",
    "\n",
    "\n",
    "\n",
    "    #len(dict_labels['biological'])\n",
    "\n",
    "    Y1={}   #initialize an empty dictionary\n",
    "    count=0 #and a count variable\n",
    "    for i in initial_label_column:    #search through the hazard_category column of the data frame\n",
    "        for j in dict_labels.keys():  #and through all the keys of the labels dictionary with keys all the unique labels \n",
    "                                  #and values their one hot encoded representation\n",
    "            if i==j:                   #if you find a match\n",
    "                Y1[count]=dict_labels[j]   #assign the category with it's one hot encoding\n",
    "                count+=1\n",
    "    #Y\n",
    "    #now the dictionary above should be turned into a numpy matrix with its elements being column vectors\n",
    "\n",
    "    # Convert dictionary values to a numpy matrix\n",
    "    matrix = np.array([v for v in Y1.values()]).T\n",
    "\n",
    "    matrix.shape #Nxm format\n",
    "    return matrix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480e8791-8898-416a-8891-c1059baa2539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d59968bf-c544-4feb-a8b4-743f238769f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_lstm(X_train,Y_train,X_test,Y_test,hu_1,hu_2,number_of_classes,a_epochs,a_batch_size):\n",
    "\n",
    "\n",
    "#in the following lstm neural net dropout regularization layers have been added added and l2 regularization is turned to 0\n",
    "    \n",
    "# LSTM Model architecture      Tx, and Ty are set by default by tensorflow\n",
    "    model = Sequential([  \n",
    "    # first LSTM layer the hidden units are frozen to 256 and instead of a tanh which is the most common actiovation function for an \n",
    "        #lstm a relu function is being applied\n",
    "        LSTM(256, activation='relu', return_sequences=True, input_shape=(1, X_train.shape[1]), kernel_regularizer=l2(0)),\n",
    "        Dropout(0.2),  # Add dropout for regularization\n",
    "\n",
    "    # second LSTM layer\n",
    "        #similar logic to the previous lstm layer but the number of hidden units remains frozen to 128\n",
    "        LSTM(128, activation='relu', kernel_regularizer=l2(0)),\n",
    "        Dropout(0.2),\n",
    "\n",
    "    # fully connected layer with hu_1 number of hidden units, no regularization and relu activation\n",
    "        Dense(hu_1, activation='relu', kernel_regularizer=l2(0)),\n",
    "\n",
    "    # output layer with softmax activation with hidden units equal to the number of classes\n",
    "        Dense(number_of_classes, activation='softmax', kernel_regularizer=l2(0))\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),  # adam optimizer\n",
    "        loss=CategoricalCrossentropy(),       # categorical cross-entropy loss\n",
    "        metrics=['accuracy']                  \n",
    "    )\n",
    "\n",
    "    # Reshape the input data to be compatible with LSTM\n",
    "    X_train_new = X_train.reshape((X_train.shape[0],1, X_train.shape[1]))  # reshape to (samples, time_steps, features)\n",
    "    X_test_new = X_test.reshape((X_test.shape[0],1, X_test.shape[1]))      # reshape to (samples, time_steps, features)\n",
    "\n",
    "\n",
    "    #train the model to fit the training data\n",
    "    model.fit(\n",
    "        X_train_new, Y_train,  # training data\n",
    "        validation_data=(X_test_new, Y_test),  # validation data\n",
    "        epochs=a_epochs,   # number of epochs\n",
    "        batch_size=a_batch_size,  # batch size\n",
    "        verbose=1  \n",
    "    )\n",
    "\n",
    "    #model evaluation on the test data\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_new, Y_test, verbose=1)\n",
    "\n",
    "    \n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238415f5-354d-42a9-b06e-4017f699df0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "79e6daa6-834d-41fd-bea1-fe783510afa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_create_heatmap_and_error_report(model,X_test,Y_test,num_of_classes):  #input the X_test \n",
    "                                                                    #and Y_test\n",
    "                                                                    #make predictions on X_test with the model\n",
    "                                                                    #compare the model predictions with \n",
    "                                                                    #the actual Y_test and create an error analysis\n",
    "\n",
    "    X_test_new=X_test.reshape((X_test.shape[0],1, X_test.shape[1])) #reshape X_test to make it compatible with lstm \n",
    "    \n",
    "    Y_predict_initial=model.predict(X_test_new) #we predict the model output for the X_test \n",
    "        #and we are going to compare with the actual lables from Y_test\n",
    "\n",
    "    #Each entry y in Y_predict_initial is a vector of  outputs= number of classes, containing the probabilities that show \n",
    "    #how likely it is for the model to assign an entry x of x_test to a specific class.\n",
    "    #For the entry x with prediction y if y[0] is the highest amongst the elements of y, x will be assigned to \n",
    "    #class 0. If y[1] is the highest then x will be assigned to class_1 and so on\n",
    "\n",
    "    #As a result, we need to find the index of  maximum element of each y in Y_predict \n",
    "    #that will show us in which class x corresponds to:\n",
    "    Y_intermediate = np.argmax(Y_predict_initial, axis=1) #find the index of the maximum element for each y in Y_predict_initial\n",
    "    \n",
    "    #we are also going to convert Y_test from a one hot encoding to \n",
    "    #the number of class this one hot encoding represents and compare it with \n",
    "    #the predicted class stored in Y_intermediate\n",
    "\n",
    "    Y_true = np.argmax(Y_test, axis=1)  #due to the fact that we have a one hot encoding, finding the index of the max \n",
    "                                    #element will lead directly to the number of class it represents\n",
    "\n",
    "    #\n",
    "    #we are going to find out where Y_true matches our prediction in Y_intermediate \n",
    "    #and we are going to display a confusion matrix of the true vs the prediction\n",
    "\n",
    "    \n",
    "    conf_mat=confusion_matrix(Y_true,Y_intermediate)\n",
    "    \n",
    "\n",
    "  #  sns.heatmap(conf_mat,annot=True, fmt='d', cmap='Blues')\n",
    "  #  plt.xlabel('Predicted Class')\n",
    "  #  plt.ylabel('True Class')\n",
    "  #  plt.title('Confusion Matrix')\n",
    "  #  plt.show()\n",
    "    \n",
    "    #the classification report is applied to the test set which is a random split from the entire training set\n",
    "    #as a result it may not include the entire number of classes and we will get the error report based on the \n",
    "    #classes stored in the confusion matrix and the test set\n",
    "\n",
    "    report = classification_report(Y_true, Y_intermediate, target_names=[f\"Class {i}\" for i in range(conf_mat.shape[0])]) \n",
    "    print(\"\\n\",report)\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6387159-557b-4281-a189-6aec96dccca9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f61d2dda-cace-484d-a524-cf11795f71de",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a function that inputs data X\n",
    "#the model that makes predictions and the \n",
    "#dictionary with the labels and their one hot encoding. \n",
    "#It computes the numeric predictions for X and turns them into the text of the label \n",
    "#they correspond to\n",
    "\n",
    "def lstm_from_pred_and_dictionary_to_labels(X,model,one_hot_dictionary):\n",
    "    \n",
    "        #predicted output\n",
    "    X_new=X.reshape((X.shape[0],1, X.shape[1]))\n",
    "    \n",
    "    Y_pr=model.predict(X_new)\n",
    "\n",
    "        #create a new dictionary with the key being the index/label of the one_hot_dictionary\n",
    "        #and the value of this new  dictionary being the phrase of the one hot encoding \n",
    "\n",
    "    new_dict={}\n",
    "\n",
    "    for key,value in one_hot_dictionary.items():   \n",
    "        convert_one_hot_encode_to_number=np.argmax(value)    #get the number representation of the one hot encoding\n",
    "        label=key                                            #get the phrase of the one hot encoding \n",
    "        new_dict[convert_one_hot_encode_to_number]=label     #store them as number-> phrase\n",
    "\n",
    "    #use the newly created dictionary to map the predictions into the labels\n",
    "    predictions_text_format=[]\n",
    "\n",
    "    for i in Y_pr:\n",
    "        chosen_label=np.argmax(i) #loop through the predictions of the model and choose to which label the prediction is assigned\n",
    "        predictions_text_format.append(new_dict[chosen_label]) #get the phrase corresponding to that label and append it to a list\n",
    "        \n",
    "    return predictions_text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52433e5-4516-431d-88ab-a90a06ec1d25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1251e740-2401-4e8b-8693-b69e001af590",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv incidents_train into a data frame\n",
    "\n",
    "df=pd.read_csv('incidents_train.csv')\n",
    "#df.head()\n",
    "\n",
    "#isolate the title category which is to be used as the input X to a classifier\n",
    "title=df['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39059561-f04d-4efd-beae-86a90b1473bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "54e4f6e5-6984-4b00-a1e1-acbbb979e9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,unique_title_voc=vectorize_string_csv_column_TF_idf(title)\n",
    "#X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b06e66f-81ed-4d02-84f3-17fc3b86e694",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "68d977dc-dacc-42d9-b127-aca1e4ff49d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5082, 128)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hazard=df['hazard']\n",
    "unique_hazard_labels=find_unique_column_labels(hazard)\n",
    "\n",
    "one_hot_encode_hazard=one_hot_encode_labels(unique_hazard_labels)\n",
    "#one_hot_encode_hazard_category\n",
    "\n",
    "Y_hazard_transposed=create_Y_label(hazard,one_hot_encode_hazard)\n",
    "\n",
    "Y_hazard=Y_hazard_transposed.T\n",
    "Y_hazard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec21ab4-32ce-4166-9a20-adafc8731275",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "bbdae573-3833-4a04-bde7-2a76789b00f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data intro train and test set\n",
    "#using the processed title data as X\n",
    "#and hazard  as Y\n",
    "X_train, X_test, Y_hazard_train, Y_hazard_test = train_test_split(X, Y_hazard, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "37f1ec5f-8333-4f97-a6f1-f2c099159f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 96ms/step - accuracy: 0.1186 - loss: 4.3206 - val_accuracy: 0.1367 - val_loss: 3.3884\n",
      "Epoch 2/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 92ms/step - accuracy: 0.1353 - loss: 3.1747 - val_accuracy: 0.2055 - val_loss: 3.2174\n",
      "Epoch 3/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 93ms/step - accuracy: 0.2309 - loss: 2.8244 - val_accuracy: 0.3117 - val_loss: 2.9796\n",
      "Epoch 4/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 94ms/step - accuracy: 0.3336 - loss: 2.4540 - val_accuracy: 0.3736 - val_loss: 2.9043\n",
      "Epoch 5/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 93ms/step - accuracy: 0.4516 - loss: 2.0090 - val_accuracy: 0.3943 - val_loss: 3.0251\n",
      "Epoch 6/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 93ms/step - accuracy: 0.5487 - loss: 1.6631 - val_accuracy: 0.4376 - val_loss: 3.2786\n",
      "Epoch 7/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 93ms/step - accuracy: 0.6373 - loss: 1.3486 - val_accuracy: 0.4366 - val_loss: 3.4532\n",
      "Epoch 8/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 94ms/step - accuracy: 0.6988 - loss: 1.0844 - val_accuracy: 0.4582 - val_loss: 3.7653\n",
      "Epoch 9/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 94ms/step - accuracy: 0.7509 - loss: 0.8953 - val_accuracy: 0.4808 - val_loss: 4.0443\n",
      "Epoch 10/10\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 93ms/step - accuracy: 0.7992 - loss: 0.7086 - val_accuracy: 0.4749 - val_loss: 4.4123\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4804 - loss: 4.3690\n",
      "Test Loss: 4.412288665771484\n",
      "Test Accuracy: 0.47492626309394836\n"
     ]
    }
   ],
   "source": [
    "#the number of hazards is 128 as it was found by the unique one hot encoding of the hazard labels\n",
    "model_3=compile_lstm(X_train,Y_hazard_train,X_test,Y_hazard_test,256,128,128,10,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0b206d91-1c92-4af3-816e-e7fbb3089036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.87      0.76      0.81       139\n",
      "     Class 1       0.00      0.00      0.00         2\n",
      "     Class 2       0.53      0.59      0.56        39\n",
      "     Class 3       0.89      0.68      0.77        47\n",
      "     Class 4       0.00      0.00      0.00         1\n",
      "     Class 5       0.74      0.83      0.78       121\n",
      "     Class 6       0.07      0.25      0.11         4\n",
      "     Class 7       0.00      0.00      0.00         3\n",
      "     Class 8       0.00      0.00      0.00         4\n",
      "     Class 9       0.28      0.21      0.24        34\n",
      "    Class 10       0.54      0.53      0.54        36\n",
      "    Class 11       0.59      0.54      0.56       108\n",
      "    Class 12       0.33      0.24      0.28        21\n",
      "    Class 13       0.17      0.50      0.25         4\n",
      "    Class 14       0.06      0.17      0.09         6\n",
      "    Class 15       0.31      0.51      0.38        45\n",
      "    Class 16       0.00      0.00      0.00         2\n",
      "    Class 17       0.00      0.00      0.00         4\n",
      "    Class 18       0.00      0.00      0.00         0\n",
      "    Class 19       0.53      0.45      0.49        42\n",
      "    Class 20       0.00      0.00      0.00         1\n",
      "    Class 21       0.17      0.36      0.23        14\n",
      "    Class 22       0.00      0.00      0.00         4\n",
      "    Class 23       0.00      0.00      0.00         8\n",
      "    Class 24       0.00      0.00      0.00         5\n",
      "    Class 25       0.00      0.00      0.00         3\n",
      "    Class 26       0.00      0.00      0.00         2\n",
      "    Class 27       0.00      0.00      0.00         4\n",
      "    Class 28       0.00      0.00      0.00         1\n",
      "    Class 29       0.00      0.00      0.00         1\n",
      "    Class 30       0.35      0.44      0.39        43\n",
      "    Class 31       0.23      0.26      0.24        23\n",
      "    Class 32       0.00      0.00      0.00         3\n",
      "    Class 33       0.14      0.20      0.17         5\n",
      "    Class 34       0.00      0.00      0.00         2\n",
      "    Class 35       0.00      0.00      0.00         6\n",
      "    Class 36       0.25      0.20      0.22        10\n",
      "    Class 37       0.30      0.39      0.34        18\n",
      "    Class 38       0.00      0.00      0.00         1\n",
      "    Class 39       0.14      0.38      0.20         8\n",
      "    Class 40       0.00      0.00      0.00         3\n",
      "    Class 41       0.33      0.33      0.33         6\n",
      "    Class 42       0.27      0.43      0.33         7\n",
      "    Class 43       0.00      0.00      0.00         3\n",
      "    Class 44       0.00      0.00      0.00         2\n",
      "    Class 45       0.00      0.00      0.00         1\n",
      "    Class 46       0.00      0.00      0.00         2\n",
      "    Class 47       0.00      0.00      0.00         1\n",
      "    Class 48       0.50      0.50      0.50         2\n",
      "    Class 49       0.00      0.00      0.00         2\n",
      "    Class 50       0.00      0.00      0.00         4\n",
      "    Class 51       0.00      0.00      0.00         6\n",
      "    Class 52       0.00      0.00      0.00         1\n",
      "    Class 53       0.00      0.00      0.00         1\n",
      "    Class 54       0.00      0.00      0.00         5\n",
      "    Class 55       0.00      0.00      0.00         3\n",
      "    Class 56       0.00      0.00      0.00         1\n",
      "    Class 57       0.17      0.10      0.12        10\n",
      "    Class 58       0.00      0.00      0.00         1\n",
      "    Class 59       0.18      0.29      0.22         7\n",
      "    Class 60       0.80      1.00      0.89        16\n",
      "    Class 61       0.00      0.00      0.00         2\n",
      "    Class 62       0.00      0.00      0.00         1\n",
      "    Class 63       0.00      0.00      0.00         3\n",
      "    Class 64       0.00      0.00      0.00         1\n",
      "    Class 65       0.00      0.00      0.00         2\n",
      "    Class 66       0.00      0.00      0.00         1\n",
      "    Class 67       0.00      0.00      0.00         3\n",
      "    Class 68       0.00      0.00      0.00         2\n",
      "    Class 69       0.27      0.33      0.30         9\n",
      "    Class 70       0.00      0.00      0.00         1\n",
      "    Class 71       0.00      0.00      0.00         1\n",
      "    Class 72       0.00      0.00      0.00         8\n",
      "    Class 73       0.00      0.00      0.00         4\n",
      "    Class 74       0.00      0.00      0.00         1\n",
      "    Class 75       0.00      0.00      0.00         1\n",
      "    Class 76       0.00      0.00      0.00         3\n",
      "    Class 77       0.00      0.00      0.00         1\n",
      "    Class 78       0.00      0.00      0.00         4\n",
      "    Class 79       0.00      0.00      0.00         1\n",
      "    Class 80       0.00      0.00      0.00         3\n",
      "    Class 81       0.00      0.00      0.00         2\n",
      "    Class 82       0.00      0.00      0.00         2\n",
      "    Class 83       0.00      0.00      0.00         3\n",
      "    Class 84       0.00      0.00      0.00         2\n",
      "    Class 85       0.00      0.00      0.00         1\n",
      "    Class 86       0.00      0.00      0.00         2\n",
      "    Class 87       0.00      0.00      0.00         1\n",
      "    Class 88       0.00      0.00      0.00         2\n",
      "    Class 89       0.16      0.60      0.25         5\n",
      "    Class 90       0.00      0.00      0.00         2\n",
      "    Class 91       0.00      0.00      0.00         1\n",
      "    Class 92       0.00      0.00      0.00         1\n",
      "    Class 93       0.00      0.00      0.00         1\n",
      "    Class 94       0.00      0.00      0.00         1\n",
      "    Class 95       0.00      0.00      0.00         1\n",
      "    Class 96       0.00      0.00      0.00         1\n",
      "    Class 97       0.00      0.00      0.00         2\n",
      "    Class 98       0.17      0.40      0.24         5\n",
      "    Class 99       0.33      1.00      0.50         1\n",
      "   Class 100       0.00      0.00      0.00         1\n",
      "   Class 101       0.82      1.00      0.90         9\n",
      "   Class 102       0.00      0.00      0.00         1\n",
      "   Class 103       0.00      0.00      0.00         3\n",
      "   Class 104       0.00      0.00      0.00         2\n",
      "   Class 105       0.00      0.00      0.00         1\n",
      "   Class 106       0.00      0.00      0.00         1\n",
      "   Class 107       0.00      0.00      0.00         1\n",
      "   Class 108       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.47      1017\n",
      "   macro avg       0.11      0.13      0.11      1017\n",
      "weighted avg       0.47      0.47      0.46      1017\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_create_heatmap_and_error_report(model_3,X_test,Y_hazard_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fa24be5-09c7-42b9-a00d-9cf349190dae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4204f9e7-c23d-4b95-b77e-e53a599c9be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the validation_file for hazard  and create a prediction vector\n",
    "\n",
    "#load the csv incidents_val into a data frame\n",
    "\n",
    "df_validation=pd.read_csv('incidents_val.csv')\n",
    "#df.head()\n",
    "\n",
    "#isolate the title category which is to be used as the input X to a classifier\n",
    "title_validation=df_validation['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767abb8b-43de-4e47-bcdf-eee1884f812e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a38b3884-afa4-4f67-955e-8915193aeeda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565, 7372)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#follow the same steps as before to vectorize the title_validation \n",
    "#into a form that can be loaded into the trained model\n",
    "\n",
    "#to avoid having different number of features for the train set and the validation set with the tfidf vectorizer we are \n",
    "#going to use the tfidf vectorizer fitted for the train set to the validation set \n",
    "#the function we created previously will not suffise because it will create a unique representation based on the validation set \n",
    "#with number of features different than the ones in the training set\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()  #create a tfidfvectorizer object\n",
    "\n",
    "\n",
    "tfidf_vectorizer.fit(title)        #fit it for the title of the training set \n",
    "                                                   \n",
    "X_center=tfidf_vectorizer.transform(title)      #transform the title of the training set into a format where each entry is a \n",
    "                                                #numeric vector of shape (m,n) where m \n",
    "                                                #is the number of examples in the given set and n is the number of features\n",
    "\n",
    "X_center_format=X_center.toarray()             #follow this conversion to get an array form\n",
    "                                               #\n",
    "\n",
    "\n",
    "X_val_0=tfidf_vectorizer.transform(title_validation)   #use the vectorizer that is fitted to the training set to transform the input \n",
    "                                                       #from the validation set. the result will be a numeric vector of shape (m_val,n)\n",
    "                                                       #where m_val is the number of entries in the validation set and \n",
    "                                                       #n is the number of features which is the same as the number of features \n",
    "                                                       # used for training\n",
    "\n",
    "X_val_1=X_val_0.toarray()\n",
    "\n",
    "X_val_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2568df-f97a-42ab-bdeb-b8de25f5a5cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "4e3137eb-d73e-49dd-af66-88a3dee48eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    }
   ],
   "source": [
    "#predictions in text format from the trained dense model named model\n",
    "\n",
    "H_val_prediction=lstm_from_pred_and_dictionary_to_labels(X_val_1,model_3,one_hot_encode_hazard)  #use the aforementioned function\n",
    "                                                       #to predict the Hazard labels for the validation set using the \n",
    "                                                       #validation title. This function also receives as input the dictionary with \n",
    "                                                       #the labels and their one hot encoding in order to convert the predictions back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "df08b88f-474a-4907-bbe4-627d1af94a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HZ_val_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ffbb38-0db8-41bb-aedc-78f436e328ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "1e316a9a-4639-46d5-8e57-72db7fb71a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are going to follow the exact same procedure and train a seperate model to predict \n",
    "#the product  first for the training set and then for the validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "47b98043-d2f2-433a-8c29-bad242813780",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5082, 1022)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product=df['product'] #get the product for the data frame refering to the training set\n",
    "unique_product_labels=find_unique_column_labels(product) #find the unique labels of product category\n",
    "\n",
    "one_hot_encode_product=one_hot_encode_labels(unique_product_labels) #one hot encode these labels and get a \n",
    "#dictionary with the key being the label and the value being its one hot encoding\n",
    "\n",
    "\n",
    "Y_product_transposed=create_Y_label(product,one_hot_encode_product)\n",
    "#create the Y part of the data set\n",
    "\n",
    "Y_product=Y_product_transposed.T\n",
    "Y_product.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de295380-69ba-4651-a779-3120bc85537d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "c2f0e538-a1b3-4e2e-a30a-2c0fef032c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data intro train and test set\n",
    "#using the processed title data as X\n",
    "#and product  as Y\n",
    "X_train, X_test, Y_product_train, Y_product_test = train_test_split(X, Y_product, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af54a9b-0fe0-4f52-9c38-eeb49c416908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "47746f9d-847c-4bf8-8036-42f66c7f0cb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 100ms/step - accuracy: 0.0219 - loss: 6.7283 - val_accuracy: 0.0492 - val_loss: 6.1027\n",
      "Epoch 2/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 96ms/step - accuracy: 0.0352 - loss: 5.8265 - val_accuracy: 0.0482 - val_loss: 6.1095\n",
      "Epoch 3/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 98ms/step - accuracy: 0.0318 - loss: 5.5826 - val_accuracy: 0.0511 - val_loss: 6.1516\n",
      "Epoch 4/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 97ms/step - accuracy: 0.0372 - loss: 5.4600 - val_accuracy: 0.0678 - val_loss: 6.3678\n",
      "Epoch 5/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 97ms/step - accuracy: 0.0488 - loss: 5.3491 - val_accuracy: 0.0649 - val_loss: 6.4963\n",
      "Epoch 6/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 97ms/step - accuracy: 0.0558 - loss: 5.2713 - val_accuracy: 0.0698 - val_loss: 6.8244\n",
      "Epoch 7/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 98ms/step - accuracy: 0.0587 - loss: 5.1586 - val_accuracy: 0.0796 - val_loss: 6.8682\n",
      "Epoch 8/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 98ms/step - accuracy: 0.0833 - loss: 4.9672 - val_accuracy: 0.0728 - val_loss: 7.1819\n",
      "Epoch 9/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 97ms/step - accuracy: 0.0869 - loss: 4.8505 - val_accuracy: 0.0885 - val_loss: 7.7963\n",
      "Epoch 10/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 97ms/step - accuracy: 0.1025 - loss: 4.5401 - val_accuracy: 0.0728 - val_loss: 8.5271\n",
      "Epoch 11/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 97ms/step - accuracy: 0.1302 - loss: 4.3245 - val_accuracy: 0.0767 - val_loss: 8.8643\n",
      "Epoch 12/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.1596 - loss: 4.0414 - val_accuracy: 0.0924 - val_loss: 9.4217\n",
      "Epoch 13/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 104ms/step - accuracy: 0.1952 - loss: 3.7149 - val_accuracy: 0.0993 - val_loss: 9.9990\n",
      "Epoch 14/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 101ms/step - accuracy: 0.2173 - loss: 3.5259 - val_accuracy: 0.1072 - val_loss: 10.6292\n",
      "Epoch 15/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 98ms/step - accuracy: 0.2693 - loss: 3.2397 - val_accuracy: 0.1141 - val_loss: 11.3536\n",
      "Epoch 16/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 98ms/step - accuracy: 0.3083 - loss: 3.0091 - val_accuracy: 0.1209 - val_loss: 11.4126\n",
      "Epoch 17/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.3196 - loss: 2.8717 - val_accuracy: 0.1288 - val_loss: 12.4467\n",
      "Epoch 18/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.3682 - loss: 2.5974 - val_accuracy: 0.1426 - val_loss: 13.2409\n",
      "Epoch 19/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 99ms/step - accuracy: 0.3883 - loss: 2.4548 - val_accuracy: 0.1495 - val_loss: 13.6077\n",
      "Epoch 20/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 99ms/step - accuracy: 0.4284 - loss: 2.2656 - val_accuracy: 0.1495 - val_loss: 14.1174\n",
      "Epoch 21/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.4558 - loss: 2.0906 - val_accuracy: 0.1642 - val_loss: 14.8680\n",
      "Epoch 22/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 101ms/step - accuracy: 0.4763 - loss: 2.0036 - val_accuracy: 0.1681 - val_loss: 15.5968\n",
      "Epoch 23/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.5251 - loss: 1.8134 - val_accuracy: 0.1721 - val_loss: 15.8254\n",
      "Epoch 24/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 99ms/step - accuracy: 0.5335 - loss: 1.7376 - val_accuracy: 0.1750 - val_loss: 16.3385\n",
      "Epoch 25/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.5893 - loss: 1.5466 - val_accuracy: 0.1780 - val_loss: 17.0460\n",
      "Epoch 26/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 99ms/step - accuracy: 0.5926 - loss: 1.4925 - val_accuracy: 0.1878 - val_loss: 17.0465\n",
      "Epoch 27/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 101ms/step - accuracy: 0.6198 - loss: 1.4002 - val_accuracy: 0.1849 - val_loss: 17.2296\n",
      "Epoch 28/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.6521 - loss: 1.2392 - val_accuracy: 0.1917 - val_loss: 18.3880\n",
      "Epoch 29/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.6726 - loss: 1.2119 - val_accuracy: 0.1858 - val_loss: 18.2914\n",
      "Epoch 30/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.6850 - loss: 1.0903 - val_accuracy: 0.1908 - val_loss: 19.0955\n",
      "Epoch 31/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 101ms/step - accuracy: 0.7061 - loss: 1.0142 - val_accuracy: 0.1976 - val_loss: 19.5499\n",
      "Epoch 32/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 102ms/step - accuracy: 0.7186 - loss: 0.9423 - val_accuracy: 0.1976 - val_loss: 19.7782\n",
      "Epoch 33/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 101ms/step - accuracy: 0.7288 - loss: 0.9263 - val_accuracy: 0.2104 - val_loss: 20.0610\n",
      "Epoch 34/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 98ms/step - accuracy: 0.7536 - loss: 0.8475 - val_accuracy: 0.2104 - val_loss: 20.3862\n",
      "Epoch 35/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 99ms/step - accuracy: 0.7752 - loss: 0.7857 - val_accuracy: 0.2055 - val_loss: 20.1497\n",
      "Epoch 36/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 102ms/step - accuracy: 0.7864 - loss: 0.7304 - val_accuracy: 0.2134 - val_loss: 20.9661\n",
      "Epoch 37/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.7852 - loss: 0.7455 - val_accuracy: 0.2124 - val_loss: 20.7965\n",
      "Epoch 38/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.8075 - loss: 0.6321 - val_accuracy: 0.2124 - val_loss: 20.7927\n",
      "Epoch 39/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 101ms/step - accuracy: 0.8138 - loss: 0.6091 - val_accuracy: 0.2045 - val_loss: 21.2324\n",
      "Epoch 40/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 102ms/step - accuracy: 0.8143 - loss: 0.5932 - val_accuracy: 0.2173 - val_loss: 21.1660\n",
      "Epoch 41/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 104ms/step - accuracy: 0.8223 - loss: 0.6190 - val_accuracy: 0.2203 - val_loss: 21.1085\n",
      "Epoch 42/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 101ms/step - accuracy: 0.8279 - loss: 0.5722 - val_accuracy: 0.2193 - val_loss: 21.1245\n",
      "Epoch 43/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 101ms/step - accuracy: 0.8396 - loss: 0.5445 - val_accuracy: 0.2153 - val_loss: 21.8429\n",
      "Epoch 44/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.8634 - loss: 0.4583 - val_accuracy: 0.2252 - val_loss: 21.8963\n",
      "Epoch 45/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 98ms/step - accuracy: 0.8713 - loss: 0.4383 - val_accuracy: 0.2291 - val_loss: 22.0801\n",
      "Epoch 46/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 98ms/step - accuracy: 0.8473 - loss: 0.4944 - val_accuracy: 0.2291 - val_loss: 21.8534\n",
      "Epoch 47/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 99ms/step - accuracy: 0.8565 - loss: 0.4679 - val_accuracy: 0.2291 - val_loss: 21.8670\n",
      "Epoch 48/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.8715 - loss: 0.4046 - val_accuracy: 0.2311 - val_loss: 22.0112\n",
      "Epoch 49/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 99ms/step - accuracy: 0.8918 - loss: 0.3755 - val_accuracy: 0.2262 - val_loss: 21.8009\n",
      "Epoch 50/50\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 99ms/step - accuracy: 0.8686 - loss: 0.4716 - val_accuracy: 0.2262 - val_loss: 21.9174\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - accuracy: 0.2291 - loss: 21.2083\n",
      "Test Loss: 21.917434692382812\n",
      "Test Accuracy: 0.2261553555727005\n"
     ]
    }
   ],
   "source": [
    "#number of classes is 1022 for the product, as it was found by its unique one hot encoding\n",
    "model_4=compile_lstm(X_train,Y_product_train,X_test,Y_product_test,256,128,1022,50,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "fb8fbc44-13bf-46e7-9f63-4bd66ec55885",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.00      0.00      0.00         4\n",
      "     Class 1       0.00      0.00      0.00         7\n",
      "     Class 2       0.00      0.00      0.00         0\n",
      "     Class 3       0.00      0.00      0.00         7\n",
      "     Class 4       0.00      0.00      0.00         1\n",
      "     Class 5       0.00      0.00      0.00         9\n",
      "     Class 6       0.00      0.00      0.00         7\n",
      "     Class 7       0.00      0.00      0.00         1\n",
      "     Class 8       0.00      0.00      0.00         1\n",
      "     Class 9       0.00      0.00      0.00        23\n",
      "    Class 10       0.00      0.00      0.00         1\n",
      "    Class 11       0.04      0.25      0.07         4\n",
      "    Class 12       0.00      0.00      0.00         3\n",
      "    Class 13       0.00      0.00      0.00         2\n",
      "    Class 14       0.00      0.00      0.00         2\n",
      "    Class 15       0.00      0.00      0.00         8\n",
      "    Class 16       0.00      0.00      0.00         3\n",
      "    Class 17       0.04      0.18      0.07        17\n",
      "    Class 18       0.00      0.00      0.00         8\n",
      "    Class 19       0.00      0.00      0.00         1\n",
      "    Class 20       0.00      0.00      0.00         9\n",
      "    Class 21       0.03      0.02      0.02        50\n",
      "    Class 22       0.00      0.00      0.00         2\n",
      "    Class 23       0.00      0.00      0.00         6\n",
      "    Class 24       0.00      0.00      0.00         1\n",
      "    Class 25       0.00      0.00      0.00         3\n",
      "    Class 26       0.00      0.00      0.00         2\n",
      "    Class 27       0.00      0.00      0.00        13\n",
      "    Class 28       0.00      0.00      0.00        10\n",
      "    Class 29       0.00      0.00      0.00         2\n",
      "    Class 30       0.00      0.00      0.00         1\n",
      "    Class 31       0.00      0.00      0.00         1\n",
      "    Class 32       0.00      0.00      0.00         2\n",
      "    Class 33       0.02      0.20      0.03         5\n",
      "    Class 34       0.00      0.00      0.00         5\n",
      "    Class 35       0.00      0.00      0.00        11\n",
      "    Class 36       0.00      0.00      0.00         1\n",
      "    Class 37       0.00      0.00      0.00         1\n",
      "    Class 38       0.00      0.00      0.00         7\n",
      "    Class 39       0.00      0.00      0.00         0\n",
      "    Class 40       0.00      0.00      0.00         1\n",
      "    Class 41       0.00      0.00      0.00         3\n",
      "    Class 42       0.00      0.00      0.00         0\n",
      "    Class 43       0.00      0.00      0.00         2\n",
      "    Class 44       0.17      0.07      0.10        14\n",
      "    Class 45       0.00      0.00      0.00         5\n",
      "    Class 46       0.00      0.00      0.00         5\n",
      "    Class 47       0.00      0.00      0.00         2\n",
      "    Class 48       0.00      0.00      0.00         5\n",
      "    Class 49       0.07      0.20      0.11         5\n",
      "    Class 50       0.00      0.00      0.00         8\n",
      "    Class 51       0.00      0.00      0.00         2\n",
      "    Class 52       0.00      0.00      0.00         2\n",
      "    Class 53       0.00      0.00      0.00         8\n",
      "    Class 54       0.00      0.00      0.00         1\n",
      "    Class 55       0.00      0.00      0.00         1\n",
      "    Class 56       0.00      0.00      0.00         1\n",
      "    Class 57       0.00      0.00      0.00         0\n",
      "    Class 58       0.00      0.00      0.00         0\n",
      "    Class 59       0.00      0.00      0.00         1\n",
      "    Class 60       0.00      0.00      0.00         6\n",
      "    Class 61       0.00      0.00      0.00         4\n",
      "    Class 62       0.00      0.00      0.00         3\n",
      "    Class 63       0.00      0.00      0.00         1\n",
      "    Class 64       0.00      0.00      0.00         8\n",
      "    Class 65       0.00      0.00      0.00         1\n",
      "    Class 66       0.00      0.00      0.00         4\n",
      "    Class 67       0.00      0.00      0.00         0\n",
      "    Class 68       0.00      0.00      0.00         7\n",
      "    Class 69       0.00      0.00      0.00         6\n",
      "    Class 70       0.00      0.00      0.00         1\n",
      "    Class 71       0.00      0.00      0.00         1\n",
      "    Class 72       0.00      0.00      0.00         0\n",
      "    Class 73       0.00      0.00      0.00         2\n",
      "    Class 74       0.00      0.00      0.00         1\n",
      "    Class 75       0.00      0.00      0.00         2\n",
      "    Class 76       0.00      0.00      0.00         2\n",
      "    Class 77       0.00      0.00      0.00         5\n",
      "    Class 78       0.00      0.00      0.00         1\n",
      "    Class 79       0.00      0.00      0.00         1\n",
      "    Class 80       0.00      0.00      0.00         2\n",
      "    Class 81       0.00      0.00      0.00         1\n",
      "    Class 82       0.00      0.00      0.00         2\n",
      "    Class 83       0.00      0.00      0.00         4\n",
      "    Class 84       0.00      0.00      0.00         1\n",
      "    Class 85       0.00      0.00      0.00         1\n",
      "    Class 86       0.00      0.00      0.00         9\n",
      "    Class 87       0.00      0.00      0.00         1\n",
      "    Class 88       0.00      0.00      0.00         3\n",
      "    Class 89       0.00      0.00      0.00         2\n",
      "    Class 90       0.00      0.00      0.00         1\n",
      "    Class 91       0.00      0.00      0.00         1\n",
      "    Class 92       0.00      0.00      0.00         2\n",
      "    Class 93       0.00      0.00      0.00         1\n",
      "    Class 94       0.00      0.00      0.00         4\n",
      "    Class 95       0.00      0.00      0.00         1\n",
      "    Class 96       0.00      0.00      0.00         1\n",
      "    Class 97       0.00      0.00      0.00         1\n",
      "    Class 98       0.00      0.00      0.00         3\n",
      "    Class 99       0.00      0.00      0.00        12\n",
      "   Class 100       0.00      0.00      0.00         0\n",
      "   Class 101       0.00      0.00      0.00         0\n",
      "   Class 102       0.00      0.00      0.00         1\n",
      "   Class 103       0.00      0.00      0.00         1\n",
      "   Class 104       0.00      0.00      0.00         1\n",
      "   Class 105       0.00      0.00      0.00         3\n",
      "   Class 106       0.00      0.00      0.00         1\n",
      "   Class 107       0.00      0.00      0.00         7\n",
      "   Class 108       0.00      0.00      0.00         2\n",
      "   Class 109       0.00      0.00      0.00         3\n",
      "   Class 110       0.00      0.00      0.00         4\n",
      "   Class 111       0.00      0.00      0.00         2\n",
      "   Class 112       0.00      0.00      0.00         3\n",
      "   Class 113       0.00      0.00      0.00         1\n",
      "   Class 114       0.00      0.00      0.00         5\n",
      "   Class 115       0.00      0.00      0.00         1\n",
      "   Class 116       0.00      0.00      0.00         5\n",
      "   Class 117       0.00      0.00      0.00         1\n",
      "   Class 118       0.00      0.00      0.00         2\n",
      "   Class 119       0.00      0.00      0.00         1\n",
      "   Class 120       0.00      0.00      0.00         2\n",
      "   Class 121       0.00      0.00      0.00         1\n",
      "   Class 122       0.00      0.00      0.00         2\n",
      "   Class 123       0.00      0.00      0.00         7\n",
      "   Class 124       0.00      0.00      0.00         1\n",
      "   Class 125       0.00      0.00      0.00         6\n",
      "   Class 126       0.00      0.00      0.00         2\n",
      "   Class 127       0.00      0.00      0.00         1\n",
      "   Class 128       0.00      0.00      0.00         1\n",
      "   Class 129       0.00      0.00      0.00         2\n",
      "   Class 130       0.00      0.00      0.00         3\n",
      "   Class 131       0.00      0.00      0.00         7\n",
      "   Class 132       0.00      0.00      0.00         1\n",
      "   Class 133       0.00      0.00      0.00         7\n",
      "   Class 134       0.00      0.00      0.00         2\n",
      "   Class 135       0.00      0.00      0.00         2\n",
      "   Class 136       0.00      0.00      0.00         9\n",
      "   Class 137       0.00      0.00      0.00         2\n",
      "   Class 138       0.00      0.00      0.00         1\n",
      "   Class 139       0.00      0.00      0.00         3\n",
      "   Class 140       0.00      0.00      0.00         4\n",
      "   Class 141       0.00      0.00      0.00         2\n",
      "   Class 142       0.00      0.00      0.00         1\n",
      "   Class 143       0.00      0.00      0.00         1\n",
      "   Class 144       0.00      0.00      0.00         1\n",
      "   Class 145       0.00      0.00      0.00         6\n",
      "   Class 146       0.00      0.00      0.00         1\n",
      "   Class 147       0.00      0.00      0.00         4\n",
      "   Class 148       0.00      0.00      0.00         1\n",
      "   Class 149       0.00      0.00      0.00         4\n",
      "   Class 150       0.00      0.00      0.00         9\n",
      "   Class 151       0.00      0.00      0.00         2\n",
      "   Class 152       0.00      0.00      0.00         1\n",
      "   Class 153       0.00      0.00      0.00         1\n",
      "   Class 154       0.00      0.00      0.00         1\n",
      "   Class 155       0.00      0.00      0.00         1\n",
      "   Class 156       0.00      0.00      0.00         1\n",
      "   Class 157       0.00      0.00      0.00         5\n",
      "   Class 158       0.00      0.00      0.00         3\n",
      "   Class 159       0.00      0.00      0.00         6\n",
      "   Class 160       0.00      0.00      0.00         1\n",
      "   Class 161       0.00      0.00      0.00         2\n",
      "   Class 162       0.00      0.00      0.00         1\n",
      "   Class 163       0.00      0.00      0.00         1\n",
      "   Class 164       0.00      0.00      0.00         1\n",
      "   Class 165       0.00      0.00      0.00         3\n",
      "   Class 166       0.00      0.00      0.00         2\n",
      "   Class 167       0.00      0.00      0.00         1\n",
      "   Class 168       0.00      0.00      0.00         3\n",
      "   Class 169       0.00      0.00      0.00         3\n",
      "   Class 170       0.00      0.00      0.00         3\n",
      "   Class 171       0.00      0.00      0.00         1\n",
      "   Class 172       0.00      0.00      0.00         4\n",
      "   Class 173       0.00      0.00      0.00         6\n",
      "   Class 174       0.00      0.00      0.00         3\n",
      "   Class 175       0.00      0.00      0.00         1\n",
      "   Class 176       0.00      0.00      0.00         1\n",
      "   Class 177       0.00      0.00      0.00         4\n",
      "   Class 178       0.00      0.00      0.00         1\n",
      "   Class 179       0.00      0.00      0.00         1\n",
      "   Class 180       0.00      0.00      0.00         4\n",
      "   Class 181       0.00      0.00      0.00         2\n",
      "   Class 182       0.00      0.00      0.00         2\n",
      "   Class 183       0.00      0.00      0.00         1\n",
      "   Class 184       0.00      0.00      0.00         5\n",
      "   Class 185       0.00      0.00      0.00         1\n",
      "   Class 186       0.00      0.00      0.00         2\n",
      "   Class 187       0.00      0.00      0.00         1\n",
      "   Class 188       0.00      0.00      0.00         2\n",
      "   Class 189       0.00      0.00      0.00         2\n",
      "   Class 190       0.00      0.00      0.00         7\n",
      "   Class 191       0.00      0.00      0.00         1\n",
      "   Class 192       0.00      0.00      0.00         2\n",
      "   Class 193       0.00      0.00      0.00         1\n",
      "   Class 194       0.00      0.00      0.00         2\n",
      "   Class 195       0.00      0.00      0.00         1\n",
      "   Class 196       0.00      0.00      0.00         1\n",
      "   Class 197       0.00      0.00      0.00         1\n",
      "   Class 198       0.00      0.00      0.00         1\n",
      "   Class 199       0.00      0.00      0.00         1\n",
      "   Class 200       0.00      0.00      0.00         1\n",
      "   Class 201       0.00      0.00      0.00         1\n",
      "   Class 202       0.00      0.00      0.00         2\n",
      "   Class 203       0.00      0.00      0.00         3\n",
      "   Class 204       0.00      0.00      0.00         5\n",
      "   Class 205       0.00      0.00      0.00         2\n",
      "   Class 206       0.00      0.00      0.00         4\n",
      "   Class 207       0.00      0.00      0.00         1\n",
      "   Class 208       0.00      0.00      0.00         1\n",
      "   Class 209       0.00      0.00      0.00         1\n",
      "   Class 210       0.00      0.00      0.00         3\n",
      "   Class 211       0.00      0.00      0.00         1\n",
      "   Class 212       0.00      0.00      0.00         2\n",
      "   Class 213       0.00      0.00      0.00         1\n",
      "   Class 214       0.00      0.00      0.00         1\n",
      "   Class 215       0.00      0.00      0.00         4\n",
      "   Class 216       0.00      0.00      0.00         3\n",
      "   Class 217       0.00      0.00      0.00         2\n",
      "   Class 218       0.00      0.00      0.00         2\n",
      "   Class 219       0.00      0.00      0.00         1\n",
      "   Class 220       0.00      0.00      0.00         1\n",
      "   Class 221       0.00      0.00      0.00         1\n",
      "   Class 222       0.00      0.00      0.00         1\n",
      "   Class 223       0.00      0.00      0.00         2\n",
      "   Class 224       0.00      0.00      0.00         1\n",
      "   Class 225       0.00      0.00      0.00         4\n",
      "   Class 226       0.00      0.00      0.00         2\n",
      "   Class 227       0.00      0.00      0.00         2\n",
      "   Class 228       0.00      0.00      0.00         1\n",
      "   Class 229       0.00      0.00      0.00         1\n",
      "   Class 230       0.00      0.00      0.00         2\n",
      "   Class 231       0.00      0.00      0.00         1\n",
      "   Class 232       0.00      0.00      0.00         1\n",
      "   Class 233       0.00      0.00      0.00         1\n",
      "   Class 234       0.00      0.00      0.00         1\n",
      "   Class 235       0.00      0.00      0.00         1\n",
      "   Class 236       0.00      0.00      0.00         2\n",
      "   Class 237       0.00      0.00      0.00         1\n",
      "   Class 238       0.00      0.00      0.00         1\n",
      "   Class 239       0.00      0.00      0.00         1\n",
      "   Class 240       0.00      0.00      0.00         1\n",
      "   Class 241       0.00      0.00      0.00         1\n",
      "   Class 242       0.00      0.00      0.00         4\n",
      "   Class 243       0.00      0.00      0.00         1\n",
      "   Class 244       0.00      0.00      0.00         1\n",
      "   Class 245       0.00      0.00      0.00         3\n",
      "   Class 246       0.00      0.00      0.00         1\n",
      "   Class 247       0.00      0.00      0.00         2\n",
      "   Class 248       0.00      0.00      0.00         1\n",
      "   Class 249       0.00      0.00      0.00         2\n",
      "   Class 250       0.00      0.00      0.00         3\n",
      "   Class 251       0.00      0.00      0.00         1\n",
      "   Class 252       0.00      0.00      0.00         1\n",
      "   Class 253       0.00      0.00      0.00         1\n",
      "   Class 254       0.00      0.00      0.00         1\n",
      "   Class 255       0.00      0.00      0.00         1\n",
      "   Class 256       0.00      0.00      0.00         1\n",
      "   Class 257       0.00      0.00      0.00         2\n",
      "   Class 258       0.00      0.00      0.00         2\n",
      "   Class 259       0.00      0.00      0.00         1\n",
      "   Class 260       0.00      0.00      0.00         1\n",
      "   Class 261       0.00      0.00      0.00         2\n",
      "   Class 262       0.00      0.00      0.00         3\n",
      "   Class 263       0.00      0.00      0.00         1\n",
      "   Class 264       0.00      0.00      0.00         1\n",
      "   Class 265       0.00      0.00      0.00         1\n",
      "   Class 266       0.00      0.00      0.00         1\n",
      "   Class 267       0.00      0.00      0.00         4\n",
      "   Class 268       0.00      0.00      0.00         1\n",
      "   Class 269       0.00      0.00      0.00         2\n",
      "   Class 270       0.00      0.00      0.00         1\n",
      "   Class 271       0.00      0.00      0.00         1\n",
      "   Class 272       0.00      0.00      0.00         1\n",
      "   Class 273       0.00      0.00      0.00         1\n",
      "   Class 274       0.00      0.00      0.00         2\n",
      "   Class 275       0.00      0.00      0.00         1\n",
      "   Class 276       0.00      0.00      0.00         1\n",
      "   Class 277       0.00      0.00      0.00         2\n",
      "   Class 278       0.00      0.00      0.00         1\n",
      "   Class 279       0.00      0.00      0.00         1\n",
      "   Class 280       0.00      0.00      0.00         1\n",
      "   Class 281       0.00      0.00      0.00         2\n",
      "   Class 282       0.00      0.00      0.00         1\n",
      "   Class 283       0.00      0.00      0.00         1\n",
      "   Class 284       0.00      0.00      0.00         1\n",
      "   Class 285       0.00      0.00      0.00         1\n",
      "   Class 286       0.00      0.00      0.00         1\n",
      "   Class 287       0.00      0.00      0.00         1\n",
      "   Class 288       0.00      0.00      0.00         1\n",
      "   Class 289       0.00      0.00      0.00         2\n",
      "   Class 290       0.00      0.00      0.00         1\n",
      "   Class 291       0.00      0.00      0.00         2\n",
      "   Class 292       0.00      0.00      0.00         1\n",
      "   Class 293       0.00      0.00      0.00         1\n",
      "   Class 294       0.00      0.00      0.00         2\n",
      "   Class 295       0.00      0.00      0.00         1\n",
      "   Class 296       0.00      0.00      0.00         1\n",
      "   Class 297       0.00      0.00      0.00         1\n",
      "   Class 298       0.00      0.00      0.00         1\n",
      "   Class 299       0.00      0.00      0.00         1\n",
      "   Class 300       0.00      0.00      0.00         1\n",
      "   Class 301       0.00      0.00      0.00         1\n",
      "   Class 302       0.00      0.00      0.00         1\n",
      "   Class 303       0.00      0.00      0.00         1\n",
      "   Class 304       0.00      0.00      0.00         1\n",
      "   Class 305       0.00      0.00      0.00         1\n",
      "   Class 306       0.00      0.00      0.00         1\n",
      "   Class 307       0.00      0.00      0.00         1\n",
      "   Class 308       0.00      0.00      0.00         2\n",
      "   Class 309       0.00      0.00      0.00         1\n",
      "   Class 310       0.00      0.00      0.00         1\n",
      "   Class 311       0.00      0.00      0.00         3\n",
      "   Class 312       0.00      0.00      0.00         1\n",
      "   Class 313       0.00      0.00      0.00         1\n",
      "   Class 314       0.00      0.00      0.00         1\n",
      "   Class 315       0.00      0.00      0.00         1\n",
      "   Class 316       0.00      0.00      0.00         1\n",
      "   Class 317       0.00      0.00      0.00         1\n",
      "   Class 318       0.00      0.00      0.00         1\n",
      "   Class 319       0.00      0.00      0.00         4\n",
      "   Class 320       0.00      0.00      0.00         1\n",
      "   Class 321       0.00      0.00      0.00         1\n",
      "   Class 322       0.00      0.00      0.00         1\n",
      "   Class 323       0.00      0.00      0.00         1\n",
      "   Class 324       0.00      0.00      0.00         1\n",
      "   Class 325       0.00      0.00      0.00         1\n",
      "   Class 326       0.00      0.00      0.00         1\n",
      "   Class 327       0.00      0.00      0.00         1\n",
      "   Class 328       0.00      0.00      0.00         1\n",
      "   Class 329       0.00      0.00      0.00         1\n",
      "   Class 330       0.00      0.00      0.00         1\n",
      "   Class 331       0.00      0.00      0.00         1\n",
      "   Class 332       0.00      0.00      0.00         7\n",
      "   Class 333       0.00      0.00      0.00         1\n",
      "   Class 334       0.00      0.00      0.00         1\n",
      "   Class 335       0.00      0.00      0.00         1\n",
      "   Class 336       0.00      0.00      0.00         1\n",
      "   Class 337       0.00      0.00      0.00         1\n",
      "   Class 338       0.00      0.00      0.00         1\n",
      "   Class 339       0.00      0.00      0.00         1\n",
      "   Class 340       0.00      0.00      0.00         1\n",
      "   Class 341       0.00      0.00      0.00         1\n",
      "   Class 342       0.00      0.00      0.00         5\n",
      "   Class 343       0.00      0.00      0.00         1\n",
      "   Class 344       0.00      0.00      0.00         1\n",
      "   Class 345       0.00      0.00      0.00         1\n",
      "   Class 346       0.00      0.00      0.00         1\n",
      "   Class 347       0.00      0.00      0.00         1\n",
      "   Class 348       0.00      0.00      0.00         2\n",
      "   Class 349       0.00      0.00      0.00         2\n",
      "   Class 350       0.00      0.00      0.00         1\n",
      "   Class 351       0.00      0.00      0.00         1\n",
      "   Class 352       0.00      0.00      0.00         5\n",
      "   Class 353       0.00      0.00      0.00         3\n",
      "   Class 354       0.00      0.00      0.00         1\n",
      "   Class 355       0.00      0.00      0.00         1\n",
      "   Class 356       0.00      0.00      0.00         2\n",
      "   Class 357       0.00      0.00      0.00         1\n",
      "   Class 358       0.00      0.00      0.00         1\n",
      "   Class 359       0.00      0.00      0.00         1\n",
      "   Class 360       0.00      0.00      0.00         1\n",
      "   Class 361       0.00      0.00      0.00         2\n",
      "   Class 362       0.00      0.00      0.00         1\n",
      "   Class 363       0.00      0.00      0.00         2\n",
      "   Class 364       0.00      0.00      0.00         6\n",
      "   Class 365       0.00      0.00      0.00         1\n",
      "   Class 366       0.00      0.00      0.00         1\n",
      "   Class 367       0.00      0.00      0.00         1\n",
      "   Class 368       0.00      0.00      0.00         1\n",
      "   Class 369       0.00      0.00      0.00         1\n",
      "   Class 370       0.00      0.00      0.00         1\n",
      "   Class 371       0.00      0.00      0.00         1\n",
      "   Class 372       0.00      0.00      0.00         1\n",
      "   Class 373       0.00      0.00      0.00         3\n",
      "   Class 374       0.00      0.00      0.00         1\n",
      "   Class 375       0.00      0.00      0.00         1\n",
      "   Class 376       0.00      0.00      0.00         1\n",
      "   Class 377       0.00      0.00      0.00         2\n",
      "   Class 378       0.00      0.00      0.00         2\n",
      "   Class 379       0.00      0.00      0.00         1\n",
      "   Class 380       0.00      0.00      0.00         1\n",
      "   Class 381       0.00      0.00      0.00         1\n",
      "   Class 382       0.00      0.00      0.00         1\n",
      "   Class 383       0.00      0.00      0.00         1\n",
      "   Class 384       0.00      0.00      0.00         1\n",
      "   Class 385       0.00      0.00      0.00         1\n",
      "   Class 386       0.00      0.00      0.00         1\n",
      "   Class 387       0.00      0.00      0.00         1\n",
      "   Class 388       0.00      0.00      0.00         1\n",
      "   Class 389       0.00      0.00      0.00         1\n",
      "   Class 390       0.00      0.00      0.00         1\n",
      "   Class 391       0.00      0.00      0.00         1\n",
      "   Class 392       0.00      0.00      0.00         1\n",
      "   Class 393       0.00      0.00      0.00         1\n",
      "   Class 394       0.00      0.00      0.00         1\n",
      "   Class 395       0.00      0.00      0.00         1\n",
      "   Class 396       0.00      0.00      0.00         1\n",
      "   Class 397       0.00      0.00      0.00         1\n",
      "   Class 398       0.00      0.00      0.00         4\n",
      "   Class 399       0.00      0.00      0.00         2\n",
      "   Class 400       0.00      0.00      0.00         1\n",
      "   Class 401       0.00      0.00      0.00         2\n",
      "   Class 402       0.00      0.00      0.00         1\n",
      "   Class 403       0.00      0.00      0.00         3\n",
      "   Class 404       0.00      0.00      0.00         1\n",
      "   Class 405       0.00      0.00      0.00         1\n",
      "   Class 406       0.00      0.00      0.00         1\n",
      "   Class 407       0.00      0.00      0.00         1\n",
      "   Class 408       0.00      0.00      0.00         1\n",
      "   Class 409       0.00      0.00      0.00         1\n",
      "   Class 410       0.00      0.00      0.00         1\n",
      "   Class 411       0.00      0.00      0.00         1\n",
      "   Class 412       0.00      0.00      0.00         1\n",
      "   Class 413       0.00      0.00      0.00         1\n",
      "   Class 414       0.00      0.00      0.00         1\n",
      "   Class 415       0.00      0.00      0.00         1\n",
      "   Class 416       0.00      0.00      0.00         1\n",
      "   Class 417       0.00      0.00      0.00         1\n",
      "   Class 418       0.00      0.00      0.00         1\n",
      "   Class 419       0.00      0.00      0.00         1\n",
      "   Class 420       0.00      0.00      0.00         1\n",
      "   Class 421       0.00      0.00      0.00         1\n",
      "   Class 422       0.00      0.00      0.00         1\n",
      "   Class 423       0.00      0.00      0.00         1\n",
      "   Class 424       0.00      0.00      0.00         1\n",
      "   Class 425       0.00      0.00      0.00         1\n",
      "   Class 426       0.00      0.00      0.00         1\n",
      "   Class 427       0.00      0.00      0.00         1\n",
      "   Class 428       0.00      0.00      0.00         1\n",
      "   Class 429       0.00      0.00      0.00         2\n",
      "   Class 430       0.00      0.00      0.00         1\n",
      "   Class 431       0.00      0.00      0.00         1\n",
      "   Class 432       0.00      0.00      0.00         1\n",
      "   Class 433       0.00      0.00      0.00         1\n",
      "   Class 434       0.00      0.00      0.00         1\n",
      "   Class 435       0.00      0.00      0.00         1\n",
      "   Class 436       0.00      0.00      0.00         1\n",
      "   Class 437       0.00      0.00      0.00         1\n",
      "   Class 438       0.00      0.00      0.00         1\n",
      "   Class 439       0.00      0.00      0.00         1\n",
      "   Class 440       0.00      0.00      0.00         1\n",
      "   Class 441       0.00      0.00      0.00         1\n",
      "   Class 442       0.00      0.00      0.00         1\n",
      "   Class 443       0.00      0.00      0.00         1\n",
      "   Class 444       0.00      0.00      0.00         1\n",
      "   Class 445       0.00      0.00      0.00         1\n",
      "   Class 446       0.00      0.00      0.00         2\n",
      "   Class 447       0.00      0.00      0.00         1\n",
      "   Class 448       0.00      0.00      0.00         1\n",
      "   Class 449       0.00      0.00      0.00         1\n",
      "   Class 450       0.00      0.00      0.00         2\n",
      "   Class 451       0.00      0.00      0.00         1\n",
      "   Class 452       0.00      0.00      0.00         1\n",
      "   Class 453       0.00      0.00      0.00         1\n",
      "   Class 454       0.00      0.00      0.00         1\n",
      "   Class 455       0.00      0.00      0.00         1\n",
      "   Class 456       0.00      0.00      0.00         1\n",
      "   Class 457       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.01      1017\n",
      "   macro avg       0.00      0.00      0.00      1017\n",
      "weighted avg       0.00      0.01      0.00      1017\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_create_heatmap_and_error_report(model_3,X_test,Y_product_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2460f575-7124-4953-9b23-93da33f24c6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565, 7372)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#follow the same steps as before to vectorize the title_validation \n",
    "#into a form that can be loaded into the trained model\n",
    "\n",
    "#to avoid having different number of features for the train set and the validation set with the tfidf vectorizer we are \n",
    "#going to use the tfidf vectorizer fitted for the train set to the validation set \n",
    "#the function we created previously will not suffise because it will create a unique representation based on the validation set \n",
    "#with number of features different than the ones in the training set\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()  #create a tfidfvectorizer object\n",
    "\n",
    "\n",
    "tfidf_vectorizer.fit(title)        #fit it for the title of the training set \n",
    "                                                   \n",
    "X_center=tfidf_vectorizer.transform(title)      #transform the title of the training set into a format where each entry is a \n",
    "                                                #numeric vector of shape (m,n) where m \n",
    "                                                #is the number of examples in the given set and n is the number of features\n",
    "\n",
    "X_center_format=X_center.toarray()             #follow this conversion to get an array form\n",
    "                                               #\n",
    "\n",
    "\n",
    "X_val_0=tfidf_vectorizer.transform(title_validation)   #use the vectorizer that is fitted to the training set to transform the input \n",
    "                                                       #from the validation set. the result will be a numeric vector of shape (m_val,n)\n",
    "                                                       #where m_val is the number of entries in the validation set and \n",
    "                                                       #n is the number of features which is the same as the number of features \n",
    "                                                       # used for training\n",
    "\n",
    "X_val_1=X_val_0.toarray()\n",
    "\n",
    "X_val_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "5e8a6deb-64bb-4884-9012-b40cff10122c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step\n"
     ]
    }
   ],
   "source": [
    "P_val_prediction=lstm_from_pred_and_dictionary_to_labels(X_val_1,model_4,one_hot_encode_product)  #use the aforementioned function\n",
    "                                                       #to predict the product  labels for the validation set using the \n",
    "                                                       #validation title. This function also receives as input the dictionary with \n",
    "                                                       #the labels and their one hot encoding in order to convert the predictions back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "628ca363-c37b-4495-9db2-c541fc284eb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "565"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PZ_val_prediction\n",
    "\n",
    "len(H_val_prediction)\n",
    "len(P_val_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "55199d70-d53b-4e42-a2c4-dc65413a19ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn the predictions into columns of a new dataframe and save them to a csv file\n",
    "predicted_data_final={\n",
    "'hazard': H_val_prediction,\n",
    "'product': P_val_prediction\n",
    "    \n",
    "}\n",
    "df_final=pd.DataFrame(predicted_data_final)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_final.to_csv('st2_title_lstm.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35fb3dc-ce8d-4dba-b1bd-e88179d21e9a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
