{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa519b17-e992-456d-ae17-99792b21cb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import all the necessary libraries\n",
    "import pandas as pd \n",
    "import numpy as  np\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#import all the necessary libraries to build a neural network classifier from tensorflow\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential                   \n",
    "from tensorflow.keras.layers import Dense                        #import fully connected neural net layers\n",
    "from tensorflow.keras.optimizers import Adam                     #choose adam as the optimization algorithm\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy      #cost function needed for softmax classification if the labels are \n",
    "                                                                 #one hot encoded\n",
    "\n",
    "from tensorflow.keras.regularizers import l2                     #add regularization in order to avoid overfitting \n",
    "\n",
    "from sklearn.model_selection import train_test_split             #to split the training set into train and test set \n",
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report  #to proceed with error analysis on our predictions\n",
    "\n",
    "\n",
    "import seaborn as sns                                                 #to visualize confusion matrices as a heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import LSTM, Dropout              #import long-short term memory neural net layers\n",
    "                                                               #and dropout regularization layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f0d998-d2ba-493e-a2dd-b25a5916cd70",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "58627131-83d1-4fd8-962c-cb1553ec6f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def  vectorize_string_csv_column_TF_idf(string_column):  \n",
    "                                                #create a function that receives a column of strings from a csv file \n",
    "                                                  #and converts each entry into a unique tfidf vector depending on the \n",
    "                                                  #unique vocabulary\n",
    "                                                   \n",
    "                                                 #necessary to convert a text input into a numeric vector to be used \n",
    "                                                 #as input to the classifier\n",
    "\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()  \n",
    "                                      #create a TF_idf_vectorizer model that will receive the entire csv column\n",
    "                                      #and will eventually turn each entry into a numeric vector\n",
    "                                      #the length of each vector will be the number of the unique words in the vocabulary\n",
    "                                      #if this length=N the resulting vectors for each entry will be of dimension (N,)\n",
    "    \n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(string_column)  \n",
    "                                                    #apply the created model to the column title of the csv\n",
    "\n",
    "\n",
    "    tfidf_vectors = tfidf_matrix.toarray()          #each entry of the tfidf_vectors is a numeric vector\n",
    "                                                    #corresponding to a title entry\n",
    "                                                    #convert it to an array format for ease of handling\n",
    "\n",
    "    Vocabulary=tfidf_vectorizer.get_feature_names_out()  #export the unique vocabulary out of the created vectorizer model\n",
    "\n",
    "    return tfidf_vectors, Vocabulary   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f813fdd-3ca7-4979-a278-e5b634ee02ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b544d5fc-92cf-4784-8062-cb83c6427f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a function that finds all the unique labels in a column \n",
    "def find_unique_column_labels(column):\n",
    "     \n",
    "    unique_labels_list=[]               #create a list that will contain all the unique labels found in the input column\n",
    "\n",
    "    for i in column : #search the entire column\n",
    "        if i not in unique_labels_list:   #if the element i is not found in the unique list \n",
    "            unique_labels_list.append(i)  #append it to the list\n",
    "    \n",
    "    return  unique_labels_list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0d11c6-62e0-409a-adf8-0e2ee35082c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "74092351-7de1-4fbe-8d0c-5db4cc1fad28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode_labels(unique_labels_of_a_column):\n",
    "    #create a dictionary for these categories that coresponds each one into a one hot numpy vector\n",
    "    #this is necessary in order to use a softmax classifier \n",
    "\n",
    "    number_of_classes=len(unique_labels_of_a_column)  #find the number of classes/possible labels from the vector that contains the unique labels\n",
    "\n",
    "                                               #create a numpy I matrix I lxl where l is the number of classes \n",
    "                                               #each row of the I matrix will correspond to a one hot encoding for each label\n",
    "    I=np.eye(number_of_classes)\n",
    "\n",
    "    #create a dictionary to correspond each class name with it's one hot encoded label\n",
    "\n",
    "    dict_labels={} #initialize an empty dictionary where the keys will be the labels and the values will be their one hot encoding\n",
    "\n",
    "\n",
    "    for i in range(number_of_classes):\n",
    "        dict_labels[unique_labels_of_a_column[i]]=I[i,:]\n",
    "\n",
    "    return dict_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea0b17c9-eff1-4415-a2d1-2f072f563194",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b5e07df-ab4b-4cec-a63f-4aacbb64b90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_Y_label(initial_label_column,dict_labels):\n",
    "    #create the Y part of the dataset by receiving  a column  and the dict_labels corresponding to that column\n",
    "\n",
    "\n",
    "\n",
    "    #len(dict_labels['biological'])\n",
    "\n",
    "    Y1={}   #initialize an empty dictionary\n",
    "    count=0 #and a count variable\n",
    "    for i in initial_label_column:    #search through the hazard_category column of the data frame\n",
    "        for j in dict_labels.keys():  #and through all the keys of the labels dictionary with keys all the unique labels \n",
    "                                  #and values their one hot encoded representation\n",
    "            if i==j:                   #if you find a match\n",
    "                Y1[count]=dict_labels[j]   #assign the category with it's one hot encoding\n",
    "                count+=1\n",
    "    #Y\n",
    "    #now the dictionary above should be turned into a numpy matrix with its elements being column vectors\n",
    "\n",
    "    # Convert dictionary values to a numpy matrix\n",
    "    matrix = np.array([v for v in Y1.values()]).T\n",
    "\n",
    "    matrix.shape #Nxm format\n",
    "    return matrix    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5be51-c2c8-4ab7-91bd-922334e0f350",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa4a01f7-b0c5-4879-a0a9-eee58af062da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_lstm(X_train,Y_train,X_test,Y_test,hu_1,hu_2,number_of_classes,a_epochs,a_batch_size):\n",
    "\n",
    "\n",
    "#in the following lstm neural net dropout regularization layers have been added added and l2 regularization is turned to 0\n",
    "    \n",
    "# LSTM Model architecture      Tx, and Ty are set by default by tensorflow\n",
    "    model = Sequential([  \n",
    "    # first LSTM layer the hidden units are frozen to 64 and instead of a tanh which is the most common actiovation function for an \n",
    "        #lstm a relu function is being applied\n",
    "        LSTM(64, activation='relu', return_sequences=True, input_shape=(1, X_train.shape[1]), kernel_regularizer=l2(0)),\n",
    "        Dropout(0.2),  # Add dropout for regularization\n",
    "\n",
    "    # second LSTM layer\n",
    "        #similar logic to the previous lstm layer but the number of hidden units remains frozen to 32\n",
    "        LSTM(32, activation='relu', kernel_regularizer=l2(0)),\n",
    "        Dropout(0.2),\n",
    "\n",
    "    # fully connected layer with hu_1 number of hidden units, no regularization and relu activation\n",
    "        Dense(hu_1, activation='relu', kernel_regularizer=l2(0)),\n",
    "\n",
    "    # output layer with softmax activation with hidden units equal to the number of classes\n",
    "        Dense(number_of_classes, activation='softmax', kernel_regularizer=l2(0))\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.001),  # adam optimizer\n",
    "        loss=CategoricalCrossentropy(),       # categorical cross-entropy loss\n",
    "        metrics=['accuracy']                  \n",
    "    )\n",
    "\n",
    "    # Reshape the input data to be compatible with LSTM\n",
    "    X_train_new = X_train.reshape((X_train.shape[0],1, X_train.shape[1]))  # reshape to (samples, time_steps, features)\n",
    "    X_test_new = X_test.reshape((X_test.shape[0],1, X_test.shape[1]))      # reshape to (samples, time_steps, features)\n",
    "\n",
    "\n",
    "    #train the model to fit the training data\n",
    "    model.fit(\n",
    "        X_train_new, Y_train,  # training data\n",
    "        validation_data=(X_test_new, Y_test),  # validation data\n",
    "        epochs=a_epochs,   # number of epochs\n",
    "        batch_size=a_batch_size,  # batch size\n",
    "        verbose=1  \n",
    "    )\n",
    "\n",
    "    #model evaluation on the test data\n",
    "    test_loss, test_accuracy = model.evaluate(X_test_new, Y_test, verbose=1)\n",
    "\n",
    "    \n",
    "    print(\"Test Loss:\", test_loss)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559dc9b4-2799-446e-97ee-b16325dcd666",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7e9c8a4-26c2-40df-a75b-5eeb65282d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_create_heatmap_and_error_report(model,X_test,Y_test,num_of_classes):  #input the X_test \n",
    "                                                                    #and Y_test\n",
    "                                                                    #make predictions on X_test with the model\n",
    "                                                                    #compare the model predictions with \n",
    "                                                                    #the actual Y_test and create an error analysis\n",
    "\n",
    "    X_test_new=X_test.reshape((X_test.shape[0],1, X_test.shape[1])) #reshape X_test to make it compatible with lstm \n",
    "    \n",
    "    Y_predict_initial=model.predict(X_test_new) #we predict the model output for the X_test \n",
    "        #and we are going to compare with the actual lables from Y_test\n",
    "\n",
    "    #Each entry y in Y_predict_initial is a vector of  outputs= number of classes, containing the probabilities that show \n",
    "    #how likely it is for the model to assign an entry x of x_test to a specific class.\n",
    "    #For the entry x with prediction y if y[0] is the highest amongst the elements of y, x will be assigned to \n",
    "    #class 0. If y[1] is the highest then x will be assigned to class_1 and so on\n",
    "\n",
    "    #As a result, we need to find the index of  maximum element of each y in Y_predict \n",
    "    #that will show us in which class x corresponds to:\n",
    "    Y_intermediate = np.argmax(Y_predict_initial, axis=1) #find the index of the maximum element for each y in Y_predict_initial\n",
    "    \n",
    "    #we are also going to convert Y_test from a one hot encoding to \n",
    "    #the number of class this one hot encoding represents and compare it with \n",
    "    #the predicted class stored in Y_intermediate\n",
    "\n",
    "    Y_true = np.argmax(Y_test, axis=1)  #due to the fact that we have a one hot encoding, finding the index of the max \n",
    "                                    #element will lead directly to the number of class it represents\n",
    "\n",
    "    #\n",
    "    #we are going to find out where Y_true matches our prediction in Y_intermediate \n",
    "    #and we are going to display a confusion matrix of the true vs the prediction\n",
    "\n",
    "    \n",
    "    conf_mat=confusion_matrix(Y_true,Y_intermediate)\n",
    "    \n",
    "\n",
    "    #sns.heatmap(conf_mat,annot=True, fmt='d', cmap='Blues')\n",
    "    #plt.xlabel('Predicted Class')\n",
    "    #plt.ylabel('True Class')\n",
    "    #plt.title('Confusion Matrix')\n",
    "    #plt.show()\n",
    "\n",
    "    #the classification report is applied to the test set which is a random split from the entire training set\n",
    "    #as a result it may not include the entire number of classes and we will get the error report based on the \n",
    "    #classes stored in the confusion matrix and the test set\n",
    "    report = classification_report(Y_true, Y_intermediate, target_names=[f\"Class {i}\" for i in range(conf_mat.shape[0])]) \n",
    "    print(\"\\n\",report)\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d4e234-f5bc-4236-9e76-e0c45c6d9fdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d75df5aa-066c-4bf5-8c1a-92b4b2bc8ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a function that inputs data X\n",
    "#the model that makes predictions and the \n",
    "#dictionary with the labels and their one hot encoding. \n",
    "#It computes the numeric predictions for X and turns them into the text of the label \n",
    "#they correspond to\n",
    "\n",
    "def lstm_from_pred_and_dictionary_to_labels(X,model,one_hot_dictionary):\n",
    "    \n",
    "        #predicted output\n",
    "    X_new=X.reshape((X.shape[0],1, X.shape[1]))\n",
    "    \n",
    "    Y_pr=model.predict(X_new)\n",
    "\n",
    "        #create a new dictionary with the key being the index/label of the one_hot_dictionary\n",
    "        #and the value of this new  dictionary being the phrase of the one hot encoding \n",
    "\n",
    "    new_dict={}\n",
    "\n",
    "    for key,value in one_hot_dictionary.items():   \n",
    "        convert_one_hot_encode_to_number=np.argmax(value)    #get the number representation of the one hot encoding\n",
    "        label=key                                            #get the phrase of the one hot encoding \n",
    "        new_dict[convert_one_hot_encode_to_number]=label     #store them as number-> phrase\n",
    "\n",
    "    #use the newly created dictionary to map the predictions into the labels\n",
    "    predictions_text_format=[]\n",
    "\n",
    "    for i in Y_pr:\n",
    "        chosen_label=np.argmax(i) #loop through the predictions of the model and choose to which label the prediction is assigned\n",
    "        predictions_text_format.append(new_dict[chosen_label]) #get the phrase corresponding to that label and append it to a list\n",
    "        \n",
    "    return predictions_text_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8197c026-0477-4616-8458-11d041eaebd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8cabb5d8-4090-4406-a510-3195252df253",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the csv incidents_train into a data frame\n",
    "\n",
    "df=pd.read_csv('incidents_train.csv')\n",
    "#df.head()\n",
    "\n",
    "#isolate the text category which is to be used as the input X to a classifier\n",
    "text=df['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f36ee9-8e45-4b25-82e5-7e2a08240fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b2a3e16-dbf5-4e20-be1b-7b31dc61bba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X,unique_title_voc=vectorize_string_csv_column_TF_idf(text)\n",
    "#X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c8d0f95-e3ed-40cc-b994-1e8eb7f583b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5082, 41409)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fedca382-f6e0-42a6-9c99-ae9669a9e7d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5082, 128)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hazard=df['hazard']\n",
    "unique_hazard_labels=find_unique_column_labels(hazard)\n",
    "\n",
    "one_hot_encode_hazard=one_hot_encode_labels(unique_hazard_labels)\n",
    "#one_hot_encode_hazard\n",
    "\n",
    "Y_hazard_transposed=create_Y_label(hazard,one_hot_encode_hazard)\n",
    "\n",
    "Y_hazard=Y_hazard_transposed.T\n",
    "Y_hazard.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b856785-3e9f-4db2-aa41-2b712b98730a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5ac8b052-4a05-4956-91ff-a66d61344ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data intro train and test set\n",
    "#using the processed title data as X\n",
    "#and hazard  as Y\n",
    "X_train, X_test, Y_hazard_train, Y_hazard_test = train_test_split(X, Y_hazard, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e545fc-2e0e-4b21-a747-795c1167a7a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "718a91ae-720b-449d-88b0-0fd6cd9e2991",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 134ms/step - accuracy: 0.1263 - loss: 4.6452 - val_accuracy: 0.1367 - val_loss: 3.4786\n",
      "Epoch 2/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.1184 - loss: 3.3735 - val_accuracy: 0.1475 - val_loss: 3.2413\n",
      "Epoch 3/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 124ms/step - accuracy: 0.1845 - loss: 3.0042 - val_accuracy: 0.3314 - val_loss: 2.9840\n",
      "Epoch 4/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 124ms/step - accuracy: 0.3193 - loss: 2.6773 - val_accuracy: 0.3894 - val_loss: 2.7648\n",
      "Epoch 5/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 124ms/step - accuracy: 0.4188 - loss: 2.2464 - val_accuracy: 0.4326 - val_loss: 2.6431\n",
      "Epoch 6/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 124ms/step - accuracy: 0.4646 - loss: 1.9803 - val_accuracy: 0.4690 - val_loss: 2.5766\n",
      "Epoch 7/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 130ms/step - accuracy: 0.5233 - loss: 1.7743 - val_accuracy: 0.4867 - val_loss: 2.5234\n",
      "Epoch 8/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.5577 - loss: 1.6242 - val_accuracy: 0.5172 - val_loss: 2.4767\n",
      "Epoch 9/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.5952 - loss: 1.4564 - val_accuracy: 0.5162 - val_loss: 2.5209\n",
      "Epoch 10/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 124ms/step - accuracy: 0.6470 - loss: 1.2776 - val_accuracy: 0.5113 - val_loss: 2.4958\n",
      "Epoch 11/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 124ms/step - accuracy: 0.6666 - loss: 1.1896 - val_accuracy: 0.5536 - val_loss: 2.6229\n",
      "Epoch 12/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 124ms/step - accuracy: 0.6984 - loss: 1.0693 - val_accuracy: 0.5575 - val_loss: 2.6157\n",
      "Epoch 13/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 124ms/step - accuracy: 0.7231 - loss: 0.9856 - val_accuracy: 0.5516 - val_loss: 2.6595\n",
      "Epoch 14/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 124ms/step - accuracy: 0.7498 - loss: 0.9046 - val_accuracy: 0.5841 - val_loss: 2.7271\n",
      "Epoch 15/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 124ms/step - accuracy: 0.7581 - loss: 0.8426 - val_accuracy: 0.5683 - val_loss: 2.7525\n",
      "Epoch 16/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.7723 - loss: 0.8043 - val_accuracy: 0.5831 - val_loss: 2.8542\n",
      "Epoch 17/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.7710 - loss: 0.7759 - val_accuracy: 0.5792 - val_loss: 2.8428\n",
      "Epoch 18/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.7942 - loss: 0.6921 - val_accuracy: 0.5870 - val_loss: 2.9339\n",
      "Epoch 19/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 125ms/step - accuracy: 0.8135 - loss: 0.6432 - val_accuracy: 0.5762 - val_loss: 2.9326\n",
      "Epoch 20/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 127ms/step - accuracy: 0.8143 - loss: 0.6077 - val_accuracy: 0.5959 - val_loss: 2.9897\n",
      "Epoch 21/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 128ms/step - accuracy: 0.8406 - loss: 0.5364 - val_accuracy: 0.5998 - val_loss: 3.1750\n",
      "Epoch 22/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 129ms/step - accuracy: 0.8412 - loss: 0.5395 - val_accuracy: 0.6008 - val_loss: 3.1379\n",
      "Epoch 23/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 130ms/step - accuracy: 0.8640 - loss: 0.4802 - val_accuracy: 0.5910 - val_loss: 3.1971\n",
      "Epoch 24/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 129ms/step - accuracy: 0.8439 - loss: 0.5050 - val_accuracy: 0.6136 - val_loss: 3.3073\n",
      "Epoch 25/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 128ms/step - accuracy: 0.8749 - loss: 0.4343 - val_accuracy: 0.6057 - val_loss: 3.3470\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.6232 - loss: 3.1535\n",
      "Test Loss: 3.3469607830047607\n",
      "Test Accuracy: 0.6057030558586121\n"
     ]
    }
   ],
   "source": [
    "#the number of classes is 128 as it was found previously by the unique one-hot encoding\n",
    "model_3=compile_lstm(X_train,Y_hazard_train,X_test,Y_hazard_test,64,32,128,25,32)\n",
    "#15 epochs yields good results lets try to increase it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70ad70c1-ca2d-4ccd-a8eb-2b02bbb6020c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "4d1e75bb-fff3-48cf-a597-0782e8622ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.91      0.92      0.92       139\n",
      "     Class 1       0.00      0.00      0.00         2\n",
      "     Class 2       0.59      0.74      0.66        39\n",
      "     Class 3       0.97      0.81      0.88        47\n",
      "     Class 4       0.00      0.00      0.00         1\n",
      "     Class 5       0.88      0.88      0.88       121\n",
      "     Class 6       0.07      0.25      0.11         4\n",
      "     Class 7       0.00      0.00      0.00         3\n",
      "     Class 8       0.00      0.00      0.00         4\n",
      "     Class 9       0.37      0.53      0.43        34\n",
      "    Class 10       0.82      0.64      0.72        36\n",
      "    Class 11       0.72      0.73      0.72       108\n",
      "    Class 12       0.34      0.52      0.42        21\n",
      "    Class 13       0.57      1.00      0.73         4\n",
      "    Class 14       0.17      0.17      0.17         6\n",
      "    Class 15       0.54      0.73      0.62        45\n",
      "    Class 16       0.00      0.00      0.00         2\n",
      "    Class 17       1.00      0.25      0.40         4\n",
      "    Class 18       0.67      0.57      0.62        42\n",
      "    Class 19       0.00      0.00      0.00         1\n",
      "    Class 20       0.40      0.43      0.41        14\n",
      "    Class 21       0.00      0.00      0.00         4\n",
      "    Class 22       0.33      0.12      0.18         8\n",
      "    Class 23       0.00      0.00      0.00         5\n",
      "    Class 24       0.00      0.00      0.00         3\n",
      "    Class 25       0.00      0.00      0.00         2\n",
      "    Class 26       0.12      0.50      0.20         4\n",
      "    Class 27       0.00      0.00      0.00         1\n",
      "    Class 28       0.00      0.00      0.00         1\n",
      "    Class 29       0.62      0.58      0.60        43\n",
      "    Class 30       0.21      0.13      0.16        23\n",
      "    Class 31       1.00      0.67      0.80         3\n",
      "    Class 32       0.14      0.40      0.21         5\n",
      "    Class 33       0.00      0.00      0.00         2\n",
      "    Class 34       0.00      0.00      0.00         6\n",
      "    Class 35       0.31      0.40      0.35        10\n",
      "    Class 36       0.81      0.72      0.76        18\n",
      "    Class 37       0.00      0.00      0.00         1\n",
      "    Class 38       0.07      0.12      0.09         8\n",
      "    Class 39       0.00      0.00      0.00         3\n",
      "    Class 40       0.33      0.50      0.40         6\n",
      "    Class 41       0.75      0.43      0.55         7\n",
      "    Class 42       0.14      0.33      0.20         3\n",
      "    Class 43       0.00      0.00      0.00         2\n",
      "    Class 44       0.00      0.00      0.00         1\n",
      "    Class 45       0.00      0.00      0.00         2\n",
      "    Class 46       0.00      0.00      0.00         1\n",
      "    Class 47       0.06      0.50      0.11         2\n",
      "    Class 48       0.00      0.00      0.00         2\n",
      "    Class 49       0.00      0.00      0.00         4\n",
      "    Class 50       0.20      0.17      0.18         6\n",
      "    Class 51       0.00      0.00      0.00         1\n",
      "    Class 52       0.20      1.00      0.33         1\n",
      "    Class 53       0.00      0.00      0.00         5\n",
      "    Class 54       0.00      0.00      0.00         3\n",
      "    Class 55       0.00      0.00      0.00         1\n",
      "    Class 56       0.67      0.40      0.50        10\n",
      "    Class 57       0.00      0.00      0.00         1\n",
      "    Class 58       0.07      0.14      0.10         7\n",
      "    Class 59       0.82      0.88      0.85        16\n",
      "    Class 60       0.00      0.00      0.00         2\n",
      "    Class 61       0.00      0.00      0.00         1\n",
      "    Class 62       0.00      0.00      0.00         3\n",
      "    Class 63       0.00      0.00      0.00         1\n",
      "    Class 64       0.00      0.00      0.00         2\n",
      "    Class 65       0.00      0.00      0.00         1\n",
      "    Class 66       0.00      0.00      0.00         3\n",
      "    Class 67       0.50      1.00      0.67         2\n",
      "    Class 68       0.62      0.89      0.73         9\n",
      "    Class 69       0.00      0.00      0.00         1\n",
      "    Class 70       0.00      0.00      0.00         1\n",
      "    Class 71       0.25      0.12      0.17         8\n",
      "    Class 72       0.25      0.25      0.25         4\n",
      "    Class 73       0.00      0.00      0.00         1\n",
      "    Class 74       0.00      0.00      0.00         1\n",
      "    Class 75       0.00      0.00      0.00         3\n",
      "    Class 76       0.14      1.00      0.25         1\n",
      "    Class 77       0.00      0.00      0.00         4\n",
      "    Class 78       0.00      0.00      0.00         1\n",
      "    Class 79       0.00      0.00      0.00         3\n",
      "    Class 80       0.00      0.00      0.00         2\n",
      "    Class 81       0.00      0.00      0.00         2\n",
      "    Class 82       0.00      0.00      0.00         3\n",
      "    Class 83       0.00      0.00      0.00         2\n",
      "    Class 84       0.00      0.00      0.00         1\n",
      "    Class 85       0.00      0.00      0.00         2\n",
      "    Class 86       0.00      0.00      0.00         1\n",
      "    Class 87       0.00      0.00      0.00         2\n",
      "    Class 88       1.00      0.60      0.75         5\n",
      "    Class 89       0.00      0.00      0.00         2\n",
      "    Class 90       0.00      0.00      0.00         1\n",
      "    Class 91       0.00      0.00      0.00         1\n",
      "    Class 92       0.00      0.00      0.00         1\n",
      "    Class 93       0.00      0.00      0.00         1\n",
      "    Class 94       0.00      0.00      0.00         1\n",
      "    Class 95       0.00      0.00      0.00         1\n",
      "    Class 96       1.00      1.00      1.00         2\n",
      "    Class 97       0.80      0.80      0.80         5\n",
      "    Class 98       1.00      1.00      1.00         1\n",
      "    Class 99       0.00      0.00      0.00         1\n",
      "   Class 100       1.00      1.00      1.00         9\n",
      "   Class 101       0.00      0.00      0.00         1\n",
      "   Class 102       0.00      0.00      0.00         3\n",
      "   Class 103       0.00      0.00      0.00         2\n",
      "   Class 104       0.00      0.00      0.00         1\n",
      "   Class 105       0.00      0.00      0.00         1\n",
      "   Class 106       0.00      0.00      0.00         1\n",
      "   Class 107       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.61      1017\n",
      "   macro avg       0.21      0.23      0.20      1017\n",
      "weighted avg       0.60      0.61      0.60      1017\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_create_heatmap_and_error_report(model_3,X_test,Y_hazard_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e42f260-cd13-4aa2-bc16-5844674f727c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2926b618-9c86-4dcf-8be0-2210578c1414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the validation_file for hazard and create a prediction vector\n",
    "\n",
    "#load the csv incidents_val into a data frame\n",
    "\n",
    "df_validation=pd.read_csv('incidents_val.csv')\n",
    "#df.head()\n",
    "\n",
    "#isolate the text category which is to be used as the input X to a classifier\n",
    "text_validation=df_validation['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "8f501763-25a3-45ed-a644-45582208d6f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565, 41409)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#follow the same steps as before to vectorize the text_validation \n",
    "#into a form that can be loaded into the trained model\n",
    "\n",
    "#to avoid having different number of features for the train set and the validation set with the tfidf vectorizer we are \n",
    "#going to use the tfidf vectorizer fitted for the train set to the validation set \n",
    "#the function we created previously will not suffise because it will create a unique representation based on the validation set \n",
    "#with number of features different than the ones in the training set\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()  #create a tfidfvectorizer object\n",
    "\n",
    "\n",
    "tfidf_vectorizer.fit(text)        #fit it for the title of the training set \n",
    "                                                   \n",
    "X_center=tfidf_vectorizer.transform(text)      #transform the title of the training set into a format where each entry is a \n",
    "                                                #numeric vector of shape (m,n) where m \n",
    "                                                #is the number of examples in the given set and n is the number of features\n",
    "\n",
    "X_center_format=X_center.toarray()             #follow this conversion to get an array form\n",
    "                                               #\n",
    "\n",
    "\n",
    "X_val_0=tfidf_vectorizer.transform(text_validation)   #use the vectorizer that is fitted to the training set to transform the input \n",
    "                                                       #from the validation set. the result will be a numeric vector of shape (m_val,n)\n",
    "                                                       #where m_val is the number of entries in the validation set and \n",
    "                                                       #n is the number of features which is the same as the number of features \n",
    "                                                       # used for training\n",
    "\n",
    "X_val_1=X_val_0.toarray()\n",
    "\n",
    "X_val_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f86a1de8-90c5-4f21-9574-5b3562f4a2d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    }
   ],
   "source": [
    "#predictions in text format from the trained dense model named model\n",
    "\n",
    "H_val_prediction=lstm_from_pred_and_dictionary_to_labels(X_val_1,model_3,one_hot_encode_hazard)  #use the aforementioned function\n",
    "                                                       #to predict the Hazard labels for the validation set using the \n",
    "                                                       #validation title. This function also receives as input the dictionary with \n",
    "                                                       #the labels and their one hot encoding in order to convert the predictions back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7782bc6-c95d-4cdf-b0dd-7998a6eaf8db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "cea4c1bc-5686-4084-9d06-63e79a570ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we are going to follow the exact same procedure and train a seperate model to predict \n",
    "#the product category first for the training set and then for the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "17190819-41c5-4ce7-bda8-d4dd4968b697",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5082, 1022)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "product=df['product'] #get the product  for the data frame refering to the training set\n",
    "unique_product_labels=find_unique_column_labels(product) #find the unique labels of product \n",
    "\n",
    "one_hot_encode_product=one_hot_encode_labels(unique_product_labels) #one hot encode these labels and get a \n",
    "#dictionary with the key being the label and the value being its one hot encoding\n",
    "\n",
    "\n",
    "Y_product_transposed=create_Y_label(product,one_hot_encode_product)\n",
    "#create the Y part of the data set\n",
    "\n",
    "Y_product=Y_product_transposed.T\n",
    "Y_product.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6da3c5e-3f36-4750-a129-5a366cb2a05a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "481a956c-a2bf-410f-8c4a-72d52171ed40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#split the data intro train and test set\n",
    "#using the processed text data as X\n",
    "#and product  as Y\n",
    "X_train, X_test, Y_product_train, Y_product_test = train_test_split(X, Y_product, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86f5149-4c6b-4e65-84b1-7eb5a0984aed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9f369269-f500-4645-9658-b92dbedd3523",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 132ms/step - accuracy: 0.0315 - loss: 6.8126 - val_accuracy: 0.0492 - val_loss: 6.2377\n",
      "Epoch 2/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 128ms/step - accuracy: 0.0269 - loss: 6.0268 - val_accuracy: 0.0492 - val_loss: 6.2102\n",
      "Epoch 3/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 127ms/step - accuracy: 0.0327 - loss: 5.8082 - val_accuracy: 0.0492 - val_loss: 6.1589\n",
      "Epoch 4/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 127ms/step - accuracy: 0.0362 - loss: 5.5635 - val_accuracy: 0.0570 - val_loss: 6.4026\n",
      "Epoch 5/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.0390 - loss: 5.4693 - val_accuracy: 0.0619 - val_loss: 6.3977\n",
      "Epoch 6/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 125ms/step - accuracy: 0.0485 - loss: 5.3533 - val_accuracy: 0.0669 - val_loss: 6.6538\n",
      "Epoch 7/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 125ms/step - accuracy: 0.0643 - loss: 5.1271 - val_accuracy: 0.0796 - val_loss: 7.1111\n",
      "Epoch 8/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 131ms/step - accuracy: 0.0638 - loss: 4.8961 - val_accuracy: 0.0708 - val_loss: 7.0166\n",
      "Epoch 9/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.0846 - loss: 4.6205 - val_accuracy: 0.0875 - val_loss: 7.7490\n",
      "Epoch 10/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.1014 - loss: 4.4534 - val_accuracy: 0.0777 - val_loss: 8.1828\n",
      "Epoch 11/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 127ms/step - accuracy: 0.1041 - loss: 4.2521 - val_accuracy: 0.0757 - val_loss: 8.2136\n",
      "Epoch 12/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.1172 - loss: 4.1062 - val_accuracy: 0.0787 - val_loss: 9.0149\n",
      "Epoch 13/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.1333 - loss: 3.9441 - val_accuracy: 0.0836 - val_loss: 8.8966\n",
      "Epoch 14/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.1458 - loss: 3.8335 - val_accuracy: 0.0865 - val_loss: 9.6206\n",
      "Epoch 15/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 127ms/step - accuracy: 0.1691 - loss: 3.7321 - val_accuracy: 0.0914 - val_loss: 10.0500\n",
      "Epoch 16/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 127ms/step - accuracy: 0.1786 - loss: 3.6576 - val_accuracy: 0.0846 - val_loss: 10.1577\n",
      "Epoch 17/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 127ms/step - accuracy: 0.1978 - loss: 3.4810 - val_accuracy: 0.1072 - val_loss: 10.8148\n",
      "Epoch 18/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 128ms/step - accuracy: 0.1870 - loss: 3.5209 - val_accuracy: 0.1101 - val_loss: 10.5151\n",
      "Epoch 19/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 127ms/step - accuracy: 0.2134 - loss: 3.3589 - val_accuracy: 0.1150 - val_loss: 11.2385\n",
      "Epoch 20/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 127ms/step - accuracy: 0.2336 - loss: 3.2125 - val_accuracy: 0.1200 - val_loss: 11.2664\n",
      "Epoch 21/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.2575 - loss: 3.1343 - val_accuracy: 0.1111 - val_loss: 11.7105\n",
      "Epoch 22/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 129ms/step - accuracy: 0.2707 - loss: 3.0627 - val_accuracy: 0.1150 - val_loss: 11.4750\n",
      "Epoch 23/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.2909 - loss: 2.9555 - val_accuracy: 0.1229 - val_loss: 12.1873\n",
      "Epoch 24/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 127ms/step - accuracy: 0.3135 - loss: 2.8394 - val_accuracy: 0.1406 - val_loss: 12.5066\n",
      "Epoch 25/25\n",
      "\u001b[1m128/128\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 126ms/step - accuracy: 0.3169 - loss: 2.7901 - val_accuracy: 0.1445 - val_loss: 13.0976\n",
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.1419 - loss: 12.6648\n",
      "Test Loss: 13.09758186340332\n",
      "Test Accuracy: 0.14454276859760284\n"
     ]
    }
   ],
   "source": [
    "#number of classes is 1022 for the product in the training set as it was found by the \n",
    "#function that creates the unique one hot encodings\n",
    "model_4=compile_lstm(X_train,Y_product_train,X_test,Y_product_test,128,64,1022,25,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d83cf936-db2f-4d6c-926a-d5a4d321bb62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m32/32\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     Class 0       0.00      0.00      0.00         4\n",
      "     Class 1       0.00      0.00      0.00         7\n",
      "     Class 2       0.09      0.14      0.11         7\n",
      "     Class 3       0.00      0.00      0.00         1\n",
      "     Class 4       0.31      0.44      0.36         9\n",
      "     Class 5       0.00      0.00      0.00         7\n",
      "     Class 6       0.00      0.00      0.00         1\n",
      "     Class 7       0.00      0.00      0.00         1\n",
      "     Class 8       0.38      0.52      0.44        23\n",
      "     Class 9       0.00      0.00      0.00         1\n",
      "    Class 10       0.00      0.00      0.00         4\n",
      "    Class 11       0.05      0.33      0.08         3\n",
      "    Class 12       0.00      0.00      0.00         2\n",
      "    Class 13       0.00      0.00      0.00         2\n",
      "    Class 14       0.00      0.00      0.00         8\n",
      "    Class 15       0.00      0.00      0.00         3\n",
      "    Class 16       0.18      0.29      0.22        17\n",
      "    Class 17       0.38      0.38      0.38         8\n",
      "    Class 18       0.00      0.00      0.00         1\n",
      "    Class 19       0.00      0.00      0.00         9\n",
      "    Class 20       0.91      0.80      0.85        50\n",
      "    Class 21       0.00      0.00      0.00         2\n",
      "    Class 22       0.00      0.00      0.00         6\n",
      "    Class 23       0.00      0.00      0.00         1\n",
      "    Class 24       0.00      0.00      0.00         3\n",
      "    Class 25       0.00      0.00      0.00         2\n",
      "    Class 26       0.86      0.46      0.60        13\n",
      "    Class 27       0.43      0.30      0.35        10\n",
      "    Class 28       0.00      0.00      0.00         0\n",
      "    Class 29       0.00      0.00      0.00         2\n",
      "    Class 30       0.00      0.00      0.00         1\n",
      "    Class 31       0.00      0.00      0.00         1\n",
      "    Class 32       0.00      0.00      0.00         2\n",
      "    Class 33       0.00      0.00      0.00         5\n",
      "    Class 34       1.00      0.20      0.33         5\n",
      "    Class 35       0.40      0.18      0.25        11\n",
      "    Class 36       0.00      0.00      0.00         1\n",
      "    Class 37       0.00      0.00      0.00         1\n",
      "    Class 38       0.40      0.29      0.33         7\n",
      "    Class 39       0.00      0.00      0.00         1\n",
      "    Class 40       0.00      0.00      0.00         3\n",
      "    Class 41       0.00      0.00      0.00         2\n",
      "    Class 42       0.14      0.07      0.10        14\n",
      "    Class 43       0.00      0.00      0.00         5\n",
      "    Class 44       0.00      0.00      0.00         5\n",
      "    Class 45       0.00      0.00      0.00         2\n",
      "    Class 46       0.67      0.40      0.50         5\n",
      "    Class 47       0.00      0.00      0.00         5\n",
      "    Class 48       0.00      0.00      0.00         8\n",
      "    Class 49       0.00      0.00      0.00         2\n",
      "    Class 50       0.00      0.00      0.00         2\n",
      "    Class 51       0.40      0.25      0.31         8\n",
      "    Class 52       0.00      0.00      0.00         1\n",
      "    Class 53       0.00      0.00      0.00         1\n",
      "    Class 54       0.00      0.00      0.00         1\n",
      "    Class 55       0.00      0.00      0.00         0\n",
      "    Class 56       0.00      0.00      0.00         1\n",
      "    Class 57       0.00      0.00      0.00         6\n",
      "    Class 58       0.00      0.00      0.00         4\n",
      "    Class 59       0.00      0.00      0.00         3\n",
      "    Class 60       0.00      0.00      0.00         1\n",
      "    Class 61       0.00      0.00      0.00         8\n",
      "    Class 62       0.20      1.00      0.33         1\n",
      "    Class 63       0.00      0.00      0.00         0\n",
      "    Class 64       1.00      0.50      0.67         4\n",
      "    Class 65       0.12      0.29      0.17         7\n",
      "    Class 66       0.00      0.00      0.00         6\n",
      "    Class 67       0.00      0.00      0.00         1\n",
      "    Class 68       0.00      0.00      0.00         1\n",
      "    Class 69       0.00      0.00      0.00         2\n",
      "    Class 70       0.00      0.00      0.00         1\n",
      "    Class 71       0.00      0.00      0.00         2\n",
      "    Class 72       0.00      0.00      0.00         2\n",
      "    Class 73       0.00      0.00      0.00         5\n",
      "    Class 74       0.00      0.00      0.00         1\n",
      "    Class 75       0.00      0.00      0.00         1\n",
      "    Class 76       0.02      0.50      0.05         2\n",
      "    Class 77       0.00      0.00      0.00         1\n",
      "    Class 78       0.00      0.00      0.00         2\n",
      "    Class 79       0.00      0.00      0.00         4\n",
      "    Class 80       0.00      0.00      0.00         1\n",
      "    Class 81       0.00      0.00      0.00         1\n",
      "    Class 82       1.00      0.44      0.62         9\n",
      "    Class 83       0.00      0.00      0.00         0\n",
      "    Class 84       0.00      0.00      0.00         1\n",
      "    Class 85       0.00      0.00      0.00         3\n",
      "    Class 86       0.00      0.00      0.00         2\n",
      "    Class 87       0.00      0.00      0.00         1\n",
      "    Class 88       0.00      0.00      0.00         0\n",
      "    Class 89       0.00      0.00      0.00         1\n",
      "    Class 90       0.00      0.00      0.00         2\n",
      "    Class 91       0.00      0.00      0.00         1\n",
      "    Class 92       0.00      0.00      0.00         4\n",
      "    Class 93       0.00      0.00      0.00         1\n",
      "    Class 94       0.00      0.00      0.00         1\n",
      "    Class 95       0.00      0.00      0.00         1\n",
      "    Class 96       0.00      0.00      0.00         0\n",
      "    Class 97       0.00      0.00      0.00         3\n",
      "    Class 98       0.50      0.42      0.45        12\n",
      "    Class 99       0.00      0.00      0.00         1\n",
      "   Class 100       0.00      0.00      0.00         0\n",
      "   Class 101       0.00      0.00      0.00         0\n",
      "   Class 102       0.00      0.00      0.00         1\n",
      "   Class 103       0.00      0.00      0.00         1\n",
      "   Class 104       0.50      0.67      0.57         3\n",
      "   Class 105       0.00      0.00      0.00         1\n",
      "   Class 106       0.00      0.00      0.00         7\n",
      "   Class 107       0.00      0.00      0.00         2\n",
      "   Class 108       0.00      0.00      0.00         3\n",
      "   Class 109       0.00      0.00      0.00         4\n",
      "   Class 110       0.00      0.00      0.00         2\n",
      "   Class 111       0.00      0.00      0.00         3\n",
      "   Class 112       0.00      0.00      0.00         1\n",
      "   Class 113       0.00      0.00      0.00         5\n",
      "   Class 114       0.00      0.00      0.00         1\n",
      "   Class 115       0.00      0.00      0.00         5\n",
      "   Class 116       0.00      0.00      0.00         1\n",
      "   Class 117       0.00      0.00      0.00         2\n",
      "   Class 118       0.00      0.00      0.00         0\n",
      "   Class 119       0.00      0.00      0.00         1\n",
      "   Class 120       0.00      0.00      0.00         0\n",
      "   Class 121       0.00      0.00      0.00         2\n",
      "   Class 122       0.00      0.00      0.00         1\n",
      "   Class 123       0.00      0.00      0.00         2\n",
      "   Class 124       0.00      0.00      0.00         7\n",
      "   Class 125       0.00      0.00      0.00         1\n",
      "   Class 126       0.00      0.00      0.00         6\n",
      "   Class 127       0.00      0.00      0.00         2\n",
      "   Class 128       0.00      0.00      0.00         0\n",
      "   Class 129       0.00      0.00      0.00         1\n",
      "   Class 130       0.00      0.00      0.00         0\n",
      "   Class 131       0.00      0.00      0.00         1\n",
      "   Class 132       0.00      0.00      0.00         2\n",
      "   Class 133       0.00      0.00      0.00         3\n",
      "   Class 134       0.00      0.00      0.00         7\n",
      "   Class 135       0.00      0.00      0.00         1\n",
      "   Class 136       0.00      0.00      0.00         0\n",
      "   Class 137       0.18      0.43      0.25         7\n",
      "   Class 138       0.00      0.00      0.00         2\n",
      "   Class 139       0.00      0.00      0.00         2\n",
      "   Class 140       0.62      0.56      0.59         9\n",
      "   Class 141       0.00      0.00      0.00         2\n",
      "   Class 142       0.00      0.00      0.00         1\n",
      "   Class 143       0.00      0.00      0.00         3\n",
      "   Class 144       0.06      0.50      0.11         4\n",
      "   Class 145       0.00      0.00      0.00         2\n",
      "   Class 146       0.00      0.00      0.00         1\n",
      "   Class 147       0.00      0.00      0.00         1\n",
      "   Class 148       0.00      0.00      0.00         1\n",
      "   Class 149       0.14      0.17      0.15         6\n",
      "   Class 150       0.00      0.00      0.00         1\n",
      "   Class 151       0.00      0.00      0.00         4\n",
      "   Class 152       0.00      0.00      0.00         1\n",
      "   Class 153       0.00      0.00      0.00         4\n",
      "   Class 154       0.60      0.33      0.43         9\n",
      "   Class 155       0.00      0.00      0.00         2\n",
      "   Class 156       0.17      1.00      0.29         1\n",
      "   Class 157       0.00      0.00      0.00         1\n",
      "   Class 158       0.00      0.00      0.00         0\n",
      "   Class 159       0.00      0.00      0.00         1\n",
      "   Class 160       0.00      0.00      0.00         1\n",
      "   Class 161       0.00      0.00      0.00         1\n",
      "   Class 162       0.00      0.00      0.00         5\n",
      "   Class 163       0.50      0.67      0.57         3\n",
      "   Class 164       0.00      0.00      0.00         6\n",
      "   Class 165       0.00      0.00      0.00         1\n",
      "   Class 166       0.00      0.00      0.00         2\n",
      "   Class 167       0.00      0.00      0.00         1\n",
      "   Class 168       0.00      0.00      0.00         1\n",
      "   Class 169       0.00      0.00      0.00         1\n",
      "   Class 170       0.00      0.00      0.00         3\n",
      "   Class 171       0.00      0.00      0.00         2\n",
      "   Class 172       0.00      0.00      0.00         1\n",
      "   Class 173       0.00      0.00      0.00         3\n",
      "   Class 174       0.00      0.00      0.00         3\n",
      "   Class 175       0.00      0.00      0.00         3\n",
      "   Class 176       0.00      0.00      0.00         1\n",
      "   Class 177       0.00      0.00      0.00         4\n",
      "   Class 178       0.00      0.00      0.00         0\n",
      "   Class 179       0.33      0.17      0.22         6\n",
      "   Class 180       0.00      0.00      0.00         0\n",
      "   Class 181       0.00      0.00      0.00         3\n",
      "   Class 182       0.00      0.00      0.00         1\n",
      "   Class 183       0.00      0.00      0.00         1\n",
      "   Class 184       0.00      0.00      0.00         4\n",
      "   Class 185       0.00      0.00      0.00         1\n",
      "   Class 186       0.00      0.00      0.00         0\n",
      "   Class 187       0.00      0.00      0.00         1\n",
      "   Class 188       0.05      0.25      0.09         4\n",
      "   Class 189       0.00      0.00      0.00         2\n",
      "   Class 190       0.00      0.00      0.00         2\n",
      "   Class 191       0.00      0.00      0.00         1\n",
      "   Class 192       0.00      0.00      0.00         5\n",
      "   Class 193       0.00      0.00      0.00         0\n",
      "   Class 194       0.00      0.00      0.00         1\n",
      "   Class 195       0.00      0.00      0.00         2\n",
      "   Class 196       0.00      0.00      0.00         1\n",
      "   Class 197       0.00      0.00      0.00         2\n",
      "   Class 198       0.00      0.00      0.00         2\n",
      "   Class 199       0.10      0.14      0.12         7\n",
      "   Class 200       0.00      0.00      0.00         1\n",
      "   Class 201       0.00      0.00      0.00         2\n",
      "   Class 202       0.00      0.00      0.00         1\n",
      "   Class 203       0.00      0.00      0.00         2\n",
      "   Class 204       0.00      0.00      0.00         1\n",
      "   Class 205       0.05      1.00      0.10         1\n",
      "   Class 206       0.00      0.00      0.00         1\n",
      "   Class 207       0.00      0.00      0.00         1\n",
      "   Class 208       0.00      0.00      0.00         1\n",
      "   Class 209       0.00      0.00      0.00         1\n",
      "   Class 210       0.00      0.00      0.00         1\n",
      "   Class 211       0.00      0.00      0.00         2\n",
      "   Class 212       0.00      0.00      0.00         3\n",
      "   Class 213       0.00      0.00      0.00         5\n",
      "   Class 214       0.00      0.00      0.00         2\n",
      "   Class 215       0.00      0.00      0.00         4\n",
      "   Class 216       0.00      0.00      0.00         1\n",
      "   Class 217       0.00      0.00      0.00         1\n",
      "   Class 218       0.00      0.00      0.00         1\n",
      "   Class 219       0.00      0.00      0.00         3\n",
      "   Class 220       0.00      0.00      0.00         1\n",
      "   Class 221       0.00      0.00      0.00         2\n",
      "   Class 222       0.00      0.00      0.00         1\n",
      "   Class 223       0.00      0.00      0.00         0\n",
      "   Class 224       0.00      0.00      0.00         1\n",
      "   Class 225       0.00      0.00      0.00         0\n",
      "   Class 226       0.00      0.00      0.00         4\n",
      "   Class 227       0.00      0.00      0.00         3\n",
      "   Class 228       0.00      0.00      0.00         2\n",
      "   Class 229       0.00      0.00      0.00         2\n",
      "   Class 230       0.00      0.00      0.00         1\n",
      "   Class 231       0.00      0.00      0.00         1\n",
      "   Class 232       0.00      0.00      0.00         1\n",
      "   Class 233       0.00      0.00      0.00         1\n",
      "   Class 234       0.00      0.00      0.00         2\n",
      "   Class 235       0.00      0.00      0.00         0\n",
      "   Class 236       0.00      0.00      0.00         1\n",
      "   Class 237       0.14      0.50      0.22         4\n",
      "   Class 238       0.00      0.00      0.00         2\n",
      "   Class 239       0.00      0.00      0.00         2\n",
      "   Class 240       0.00      0.00      0.00         1\n",
      "   Class 241       0.00      0.00      0.00         0\n",
      "   Class 242       0.00      0.00      0.00         1\n",
      "   Class 243       0.00      0.00      0.00         2\n",
      "   Class 244       0.00      0.00      0.00         1\n",
      "   Class 245       0.00      0.00      0.00         1\n",
      "   Class 246       0.00      0.00      0.00         1\n",
      "   Class 247       0.00      0.00      0.00         1\n",
      "   Class 248       0.00      0.00      0.00         1\n",
      "   Class 249       0.00      0.00      0.00         2\n",
      "   Class 250       0.00      0.00      0.00         1\n",
      "   Class 251       0.00      0.00      0.00         1\n",
      "   Class 252       0.00      0.00      0.00         1\n",
      "   Class 253       0.00      0.00      0.00         1\n",
      "   Class 254       0.00      0.00      0.00         1\n",
      "   Class 255       1.00      0.25      0.40         4\n",
      "   Class 256       0.00      0.00      0.00         1\n",
      "   Class 257       0.00      0.00      0.00         1\n",
      "   Class 258       0.00      0.00      0.00         3\n",
      "   Class 259       0.00      0.00      0.00         1\n",
      "   Class 260       0.00      0.00      0.00         2\n",
      "   Class 261       0.00      0.00      0.00         1\n",
      "   Class 262       0.00      0.00      0.00         2\n",
      "   Class 263       0.00      0.00      0.00         3\n",
      "   Class 264       0.00      0.00      0.00         1\n",
      "   Class 265       0.00      0.00      0.00         1\n",
      "   Class 266       0.00      0.00      0.00         1\n",
      "   Class 267       0.00      0.00      0.00         1\n",
      "   Class 268       0.00      0.00      0.00         1\n",
      "   Class 269       0.00      0.00      0.00         1\n",
      "   Class 270       0.00      0.00      0.00         2\n",
      "   Class 271       0.00      0.00      0.00         2\n",
      "   Class 272       0.00      0.00      0.00         1\n",
      "   Class 273       0.00      0.00      0.00         0\n",
      "   Class 274       0.00      0.00      0.00         1\n",
      "   Class 275       0.00      0.00      0.00         2\n",
      "   Class 276       0.00      0.00      0.00         3\n",
      "   Class 277       0.00      0.00      0.00         1\n",
      "   Class 278       0.00      0.00      0.00         1\n",
      "   Class 279       0.00      0.00      0.00         1\n",
      "   Class 280       0.00      0.00      0.00         1\n",
      "   Class 281       0.75      0.75      0.75         4\n",
      "   Class 282       0.00      0.00      0.00         0\n",
      "   Class 283       0.00      0.00      0.00         1\n",
      "   Class 284       1.00      1.00      1.00         2\n",
      "   Class 285       0.00      0.00      0.00         1\n",
      "   Class 286       0.00      0.00      0.00         1\n",
      "   Class 287       0.00      0.00      0.00         1\n",
      "   Class 288       0.00      0.00      0.00         1\n",
      "   Class 289       0.00      0.00      0.00         2\n",
      "   Class 290       0.00      0.00      0.00         1\n",
      "   Class 291       0.00      0.00      0.00         0\n",
      "   Class 292       0.00      0.00      0.00         1\n",
      "   Class 293       0.00      0.00      0.00         2\n",
      "   Class 294       0.00      0.00      0.00         1\n",
      "   Class 295       0.00      0.00      0.00         1\n",
      "   Class 296       0.00      0.00      0.00         1\n",
      "   Class 297       0.00      0.00      0.00         2\n",
      "   Class 298       0.00      0.00      0.00         1\n",
      "   Class 299       0.00      0.00      0.00         1\n",
      "   Class 300       0.00      0.00      0.00         1\n",
      "   Class 301       0.25      1.00      0.40         1\n",
      "   Class 302       0.00      0.00      0.00         1\n",
      "   Class 303       0.00      0.00      0.00         1\n",
      "   Class 304       0.50      1.00      0.67         1\n",
      "   Class 305       0.00      0.00      0.00         2\n",
      "   Class 306       0.00      0.00      0.00         1\n",
      "   Class 307       0.00      0.00      0.00         0\n",
      "   Class 308       0.17      0.50      0.25         2\n",
      "   Class 309       0.00      0.00      0.00         1\n",
      "   Class 310       0.00      0.00      0.00         0\n",
      "   Class 311       0.00      0.00      0.00         1\n",
      "   Class 312       0.00      0.00      0.00         2\n",
      "   Class 313       0.00      0.00      0.00         1\n",
      "   Class 314       0.00      0.00      0.00         1\n",
      "   Class 315       0.00      0.00      0.00         1\n",
      "   Class 316       0.00      0.00      0.00         1\n",
      "   Class 317       0.00      0.00      0.00         1\n",
      "   Class 318       0.00      0.00      0.00         0\n",
      "   Class 319       0.00      0.00      0.00         0\n",
      "   Class 320       0.00      0.00      0.00         1\n",
      "   Class 321       0.00      0.00      0.00         1\n",
      "   Class 322       0.00      0.00      0.00         1\n",
      "   Class 323       0.00      0.00      0.00         1\n",
      "   Class 324       0.00      0.00      0.00         1\n",
      "   Class 325       0.00      0.00      0.00         1\n",
      "   Class 326       0.00      0.00      0.00         1\n",
      "   Class 327       0.00      0.00      0.00         1\n",
      "   Class 328       0.00      0.00      0.00         2\n",
      "   Class 329       0.00      0.00      0.00         1\n",
      "   Class 330       0.00      0.00      0.00         1\n",
      "   Class 331       0.00      0.00      0.00         0\n",
      "   Class 332       0.00      0.00      0.00         0\n",
      "   Class 333       0.00      0.00      0.00         3\n",
      "   Class 334       0.00      0.00      0.00         1\n",
      "   Class 335       0.00      0.00      0.00         1\n",
      "   Class 336       0.00      0.00      0.00         1\n",
      "   Class 337       0.00      0.00      0.00         1\n",
      "   Class 338       0.00      0.00      0.00         1\n",
      "   Class 339       0.00      0.00      0.00         1\n",
      "   Class 340       0.00      0.00      0.00         0\n",
      "   Class 341       0.00      0.00      0.00         1\n",
      "   Class 342       0.00      0.00      0.00         4\n",
      "   Class 343       0.00      0.00      0.00         1\n",
      "   Class 344       0.00      0.00      0.00         1\n",
      "   Class 345       0.00      0.00      0.00         1\n",
      "   Class 346       0.00      0.00      0.00         1\n",
      "   Class 347       0.00      0.00      0.00         1\n",
      "   Class 348       0.00      0.00      0.00         1\n",
      "   Class 349       0.00      0.00      0.00         0\n",
      "   Class 350       0.00      0.00      0.00         1\n",
      "   Class 351       0.00      0.00      0.00         0\n",
      "   Class 352       0.00      0.00      0.00         1\n",
      "   Class 353       0.00      0.00      0.00         1\n",
      "   Class 354       0.00      0.00      0.00         1\n",
      "   Class 355       0.00      0.00      0.00         1\n",
      "   Class 356       0.00      0.00      0.00         1\n",
      "   Class 357       0.00      0.00      0.00         7\n",
      "   Class 358       0.00      0.00      0.00         1\n",
      "   Class 359       0.00      0.00      0.00         1\n",
      "   Class 360       0.00      0.00      0.00         1\n",
      "   Class 361       0.00      0.00      0.00         1\n",
      "   Class 362       0.00      0.00      0.00         1\n",
      "   Class 363       0.00      0.00      0.00         1\n",
      "   Class 364       0.00      0.00      0.00         1\n",
      "   Class 365       0.00      0.00      0.00         1\n",
      "   Class 366       0.00      0.00      0.00         1\n",
      "   Class 367       0.50      0.20      0.29         5\n",
      "   Class 368       0.00      0.00      0.00         1\n",
      "   Class 369       0.00      0.00      0.00         1\n",
      "   Class 370       0.00      0.00      0.00         0\n",
      "   Class 371       0.00      0.00      0.00         1\n",
      "   Class 372       0.00      0.00      0.00         1\n",
      "   Class 373       0.00      0.00      0.00         1\n",
      "   Class 374       0.00      0.00      0.00         2\n",
      "   Class 375       0.00      0.00      0.00         2\n",
      "   Class 376       0.00      0.00      0.00         1\n",
      "   Class 377       0.00      0.00      0.00         1\n",
      "   Class 378       0.20      0.40      0.27         5\n",
      "   Class 379       0.22      0.67      0.33         3\n",
      "   Class 380       0.00      0.00      0.00         1\n",
      "   Class 381       0.00      0.00      0.00         1\n",
      "   Class 382       0.00      0.00      0.00         2\n",
      "   Class 383       0.00      0.00      0.00         1\n",
      "   Class 384       0.00      0.00      0.00         1\n",
      "   Class 385       0.00      0.00      0.00         1\n",
      "   Class 386       0.00      0.00      0.00         1\n",
      "   Class 387       0.00      0.00      0.00         2\n",
      "   Class 388       0.00      0.00      0.00         1\n",
      "   Class 389       0.33      0.50      0.40         2\n",
      "   Class 390       1.00      0.17      0.29         6\n",
      "   Class 391       0.00      0.00      0.00         1\n",
      "   Class 392       0.00      0.00      0.00         1\n",
      "   Class 393       0.00      0.00      0.00         1\n",
      "   Class 394       0.00      0.00      0.00         1\n",
      "   Class 395       0.00      0.00      0.00         1\n",
      "   Class 396       0.00      0.00      0.00         1\n",
      "   Class 397       0.00      0.00      0.00         0\n",
      "   Class 398       0.00      0.00      0.00         0\n",
      "   Class 399       0.00      0.00      0.00         1\n",
      "   Class 400       0.00      0.00      0.00         0\n",
      "   Class 401       0.00      0.00      0.00         1\n",
      "   Class 402       0.00      0.00      0.00         3\n",
      "   Class 403       0.00      0.00      0.00         1\n",
      "   Class 404       0.00      0.00      0.00         1\n",
      "   Class 405       0.00      0.00      0.00         1\n",
      "   Class 406       0.00      0.00      0.00         0\n",
      "   Class 407       1.00      0.50      0.67         2\n",
      "   Class 408       0.00      0.00      0.00         0\n",
      "   Class 409       0.00      0.00      0.00         2\n",
      "   Class 410       0.00      0.00      0.00         1\n",
      "   Class 411       0.00      0.00      0.00         1\n",
      "   Class 412       0.00      0.00      0.00         1\n",
      "   Class 413       0.00      0.00      0.00         1\n",
      "   Class 414       0.00      0.00      0.00         1\n",
      "   Class 415       0.00      0.00      0.00         1\n",
      "   Class 416       0.00      0.00      0.00         1\n",
      "   Class 417       0.00      0.00      0.00         1\n",
      "   Class 418       0.00      0.00      0.00         1\n",
      "   Class 419       0.00      0.00      0.00         1\n",
      "   Class 420       0.00      0.00      0.00         1\n",
      "   Class 421       0.00      0.00      0.00         1\n",
      "   Class 422       0.00      0.00      0.00         1\n",
      "   Class 423       0.00      0.00      0.00         1\n",
      "   Class 424       0.00      0.00      0.00         1\n",
      "   Class 425       0.00      0.00      0.00         1\n",
      "   Class 426       0.00      0.00      0.00         1\n",
      "   Class 427       0.00      0.00      0.00         0\n",
      "   Class 428       0.00      0.00      0.00         1\n",
      "   Class 429       0.00      0.00      0.00         1\n",
      "   Class 430       0.80      1.00      0.89         4\n",
      "   Class 431       0.00      0.00      0.00         2\n",
      "   Class 432       0.00      0.00      0.00         1\n",
      "   Class 433       0.00      0.00      0.00         2\n",
      "   Class 434       0.00      0.00      0.00         1\n",
      "   Class 435       0.00      0.00      0.00         3\n",
      "   Class 436       0.00      0.00      0.00         1\n",
      "   Class 437       0.00      0.00      0.00         1\n",
      "   Class 438       0.00      0.00      0.00         1\n",
      "   Class 439       0.00      0.00      0.00         1\n",
      "   Class 440       0.00      0.00      0.00         1\n",
      "   Class 441       0.00      0.00      0.00         1\n",
      "   Class 442       0.00      0.00      0.00         1\n",
      "   Class 443       0.00      0.00      0.00         1\n",
      "   Class 444       1.00      1.00      1.00         1\n",
      "   Class 445       0.00      0.00      0.00         1\n",
      "   Class 446       0.00      0.00      0.00         1\n",
      "   Class 447       0.00      0.00      0.00         1\n",
      "   Class 448       0.00      0.00      0.00         1\n",
      "   Class 449       0.00      0.00      0.00         1\n",
      "   Class 450       0.00      0.00      0.00         1\n",
      "   Class 451       0.00      0.00      0.00         1\n",
      "   Class 452       0.00      0.00      0.00         1\n",
      "   Class 453       0.00      0.00      0.00         1\n",
      "   Class 454       0.00      0.00      0.00         1\n",
      "   Class 455       0.00      0.00      0.00         1\n",
      "   Class 456       0.00      0.00      0.00         1\n",
      "   Class 457       0.00      0.00      0.00         1\n",
      "   Class 458       0.00      0.00      0.00         1\n",
      "   Class 459       0.00      0.00      0.00         1\n",
      "   Class 460       0.00      0.00      0.00         1\n",
      "   Class 461       0.00      0.00      0.00         2\n",
      "   Class 462       0.00      0.00      0.00         1\n",
      "   Class 463       0.00      0.00      0.00         1\n",
      "   Class 464       0.00      0.00      0.00         1\n",
      "   Class 465       0.00      0.00      0.00         1\n",
      "   Class 466       0.00      0.00      0.00         1\n",
      "   Class 467       0.00      0.00      0.00         1\n",
      "   Class 468       0.00      0.00      0.00         1\n",
      "   Class 469       0.00      0.00      0.00         1\n",
      "   Class 470       0.00      0.00      0.00         1\n",
      "   Class 471       0.00      0.00      0.00         1\n",
      "   Class 472       0.00      0.00      0.00         1\n",
      "   Class 473       0.00      0.00      0.00         1\n",
      "   Class 474       0.00      0.00      0.00         1\n",
      "   Class 475       0.00      0.00      0.00         1\n",
      "   Class 476       0.00      0.00      0.00         1\n",
      "   Class 477       0.00      0.00      0.00         1\n",
      "   Class 478       0.00      0.00      0.00         2\n",
      "   Class 479       0.00      0.00      0.00         1\n",
      "   Class 480       0.00      0.00      0.00         1\n",
      "   Class 481       0.00      0.00      0.00         1\n",
      "   Class 482       0.00      0.00      0.00         2\n",
      "   Class 483       0.00      0.00      0.00         1\n",
      "   Class 484       0.00      0.00      0.00         1\n",
      "   Class 485       0.00      0.00      0.00         1\n",
      "   Class 486       0.00      0.00      0.00         1\n",
      "   Class 487       0.00      0.00      0.00         0\n",
      "   Class 488       0.00      0.00      0.00         1\n",
      "   Class 489       0.00      0.00      0.00         1\n",
      "   Class 490       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.14      1017\n",
      "   macro avg       0.04      0.05      0.04      1017\n",
      "weighted avg       0.16      0.14      0.14      1017\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "C:\\Users\\USER\\anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1531: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_create_heatmap_and_error_report(model_4,X_test,Y_product_test,22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "234249cb-de25-4f05-a10f-3bb1ddc8fd05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(565, 41409)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#follow the same steps as before to vectorize the text_validation \n",
    "#into a form that can be loaded into the trained model\n",
    "\n",
    "#to avoid having different number of features for the train set and the validation set with the tfidf vectorizer we are \n",
    "#going to use the tfidf vectorizer fitted for the train set to the validation set \n",
    "#the function we created previously will not suffise because it will create a unique representation based on the validation set \n",
    "#with number of features different than the ones in the training set\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()  #create a tfidfvectorizer object\n",
    "\n",
    "\n",
    "tfidf_vectorizer.fit(text)        #fit it for the title of the training set \n",
    "                                                   \n",
    "X_center=tfidf_vectorizer.transform(text)      #transform the title of the training set into a format where each entry is a \n",
    "                                                #numeric vector of shape (m,n) where m \n",
    "                                                #is the number of examples in the given set and n is the number of features\n",
    "\n",
    "X_center_format=X_center.toarray()             #follow this conversion to get an array form\n",
    "                                               #\n",
    "\n",
    "\n",
    "X_val_0=tfidf_vectorizer.transform(text_validation)   #use the vectorizer that is fitted to the training set to transform the input \n",
    "                                                       #from the validation set. the result will be a numeric vector of shape (m_val,n)\n",
    "                                                       #where m_val is the number of entries in the validation set and \n",
    "                                                       #n is the number of features which is the same as the number of features \n",
    "                                                       # used for training\n",
    "\n",
    "X_val_1=X_val_0.toarray()\n",
    "\n",
    "X_val_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5fb36793-9061-4381-b461-adb2c941ce95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
     ]
    }
   ],
   "source": [
    "P_val_prediction=lstm_from_pred_and_dictionary_to_labels(X_val_1,model_4,one_hot_encode_product)  #use the aforementioned function\n",
    "                                                       #to predict the product category labels for the validation set using the \n",
    "                                                       #validation title. This function also receives as input the dictionary with \n",
    "                                                       #the labels and their one hot encoding in order to convert the predictions back to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe313dea-68a1-4ae3-85fc-e07f76d4de59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d1323be6-2949-44db-92fb-8428f54b1f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#turn the predictions into columns of a new dataframe and save them to a csv file\n",
    "predicted_data_final={\n",
    "'hazard': H_val_prediction,\n",
    "'product': P_val_prediction\n",
    "    \n",
    "}\n",
    "df_final=pd.DataFrame(predicted_data_final)\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_final.to_csv('st2_text_lstm.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ae6eac-8e8f-468d-a3b6-9a78ded60307",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9103509-89d2-4204-926c-0b2ee69db1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
